[
	{
		"title": "Lite",
		"description": "Lightweight, broad evaluation of the capabilities of language models using in-context learning",
		"id": "lite",
		"releases": ["v1.8.0", "v1.7.0", "v1.6.0", "v1.5.0", "v1.4.0", "v1.3.0", "v1.2.0", "v1.1.0", "v1.0.0"]
	},
	{
		"title": "Classic",
		"description": "Thorough language model evaluations based on the scenarios from the original HELM paper",
		"id": "classic",
		"releases": ["v0.4.0", "v0.3.0", "v0.2.4", "v0.2.3", "v0.2.2"]
	},
	{
		"title": "HEIM",
		"description": "Holistic evaluation of text-to-image models",
		"id": "heim",
		"releases": ["v1.1.0", "v1.0.0"]
	},
	{
		"title": "Instruct",
		"description": "Evaluations of instruction following models with absolute ratings",
		"id": "instruct",
		"releases": ["v1.0.0"]
	},
	{
		"title": "MMLU",
		"description": "Massive Multitask Language Understanding (MMLU) evaluations using standardized prompts",
		"id": "mmlu",
		"releases": ["v1.8.0", "v1.7.0", "v1.6.0", "v1.5.0", "v1.4.0", "v1.3.0", "v1.2.0", "v1.1.0", "v1.0.0"]
	},
	{
		"title": "VHELM",
		"description": "Holistic Evaluation of Vision-Language Models",
		"id": "vhelm",
		"releases": ["v1.0.0"]
	},
	{
		"title": "Image2Struct",
		"description": "Evaluations of Vision-Language Models on extracting structured information from images",
		"id": "image2struct",
		"releases": ["v1.0.1", "v1.0.0"]
	},
	{
		"title": "AIR-Bench",
		"description": "Safety benchmark based on emerging government regulations and company policies",
		"id": "air-bench",
		"releases": ["v1.0.0"]
	},
	{
		"title": "CLEVA",
		"description": "Chinese-language benchmark for holistic evaluation of Chinese language models",
		"id": "cleva",
		"releases": ["v1.0.0"]
	},
	{
		"title": "ThaiExam",
		"description": "Thai-language evaluations of language models on standardized examinations in Thailand",
		"id": "thaiexam",
		"releases": ["v1.0.0"]
	},
	{
		"title": "All Leaderboards",
		"description": "Home page for all HELM leaderboards",
		"id": "home",
		"releases": ["v1.0.0"]
	}
]
