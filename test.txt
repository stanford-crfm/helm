\documentclass{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

\begin{algorithm}
\caption{Training of generative MMD flows}
\label{alg:mmd}
\begin{algorithmic}[1]
\State \textbf{Input:} Independent initial samples $x^{(0)}_1, \ldots, x^{(0)}_N$ from $\mu_0$, momentum parameters $m_l \in [0,1)$ for $l = 1, \ldots, L$.
\State Initialize $(v_1, \ldots, v_N) = 0$.
\For{$l = 1, \ldots, L$ \textbf{do}}
\State Set $(x^{(0)}_1, \ldots, x^{(0)}_N) = (x_1^{(l-1)}, \ldots, x_N^{(l-1)})$.
\State Simulate $T_l$ steps of the (momentum) MMD flow:
\For{$t = 1, \ldots, T_l$ \textbf{do}}
\State Update $v$ by
$$(v_1, \ldots, v_N) \leftarrow F_d(x_1^{(t-1)}, \ldots, x_N^{(t-1)}) | y_1, \ldots, y_M) + m_l (v_1, \ldots, v_N)$$
\State Update the flow samples:
$$(x_1^{(t)}, \ldots, x_N^{(t)}) = (x_1^{(t-1)}, \ldots, x_N^{(t-1)}) - \tau_N (v_1, \ldots, v_N)$$
\EndFor
\State Train $\Phi_l$ such that $\Phi_l(x^{(T_l)}) \approx x^{(0)}_i - \Phi_l(x^{(0)}_i)$ by minimizing the loss
$$L(\theta_l) = \frac{1}{N} \sum_{i=1}^N \Vert \Phi_l(x^{(0)}_i) - x_i^{(T_l)} \Vert^2.$$
\State Set $(x_1^{(l)}, \ldots, x_N^{(l)}) = (x_1^{(l-1)}, \ldots, x_N^{(l-1)}) - (\Phi_l(x_1^{(l-1)}), \ldots, \Phi_l(x_N^{(l-1)}))$.
\EndFor
\end{algorithmic}
\end{algorithm}

\end{document}
