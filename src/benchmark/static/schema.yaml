---

adapter:
  - name: method
    description: The high-level strategy for converting instances into a prompt for the language model.
    values:
      - name: generation
        description: Given the input, the model generates the output free-form.
      - name: multiple_choice_joint
        description: Given the input, the model selects from multiple-choice options (A., B., C., D., E.).
      - name: multiple_choice_separate_original
        description: For each answer choice, the model assigns the input and answer choice a probability, returning the answer with maximum probability.
      - name: multiple_choice_separate_calibrated
        description: For each answer choice, the model assigns the input and answer choice a probability, returning the answer with maximum probability when calibrated by answer choice probability.
      - name: language_modeling
        description: Given the input, the model assigns the sequence a probability.
  - name: instructions
    description: The description of the task that is included at the very beginning of the prompt.
  - name: instance_prefix
    description: The string that is included before each instance (e.g., '\n\n').
  - name: input_prefix
    description: The string that is included before each input (e.g., 'Question:').
  - name: reference_prefix
    description: The string that is included before each reference (for multiple-choice questions).
  - name: output_prefix
    description: The string that is included before the correct answer/predicted output (e.g., 'Answer:').
  - name: max_train_instances
    description: Maximum number of training instances to include in the prompt (currently by randomly sampling).
  - name: max_eval_instances
    description: Maximum number of instances to evaluate on (over all splits - test, valid, etc.).
  - name: num_outputs
    description: Maximum number of possible outputs to generate by sampling multiple outputs.
  - name: num_train_trials
    description: Number of trials, where in each trial we choose an independent, random set of training instances. Used to compute variance.
  - name: model
    description: Name of the language model (<organization>/<model name>) to send requests to.
  - name: temperature
    description: Temperature parameter used in generation.
  - name: max_tokens
    description: Maximum number of tokens to generate.
  - name: stop_sequences
    description: List of sequences, where we stop generation if we encounter any of them.
  - name: random
    description: Random seed (string), which guarantees reproducibility.


metrics:
  # Infrastructure metrics:
  - name: num_perplexity_tokens
    display_name: '# tokens'
    description: Average number of tokens in the predicted output (for language modeling, the input too).
  - name: num_bytes
    display_name: '# bytes'
    description: Average number of bytes in the predicted output (for language modeling, the input too).

  - name: estimated_num_tokens_cost
    display_name: 'cost'
    description: An estimate of the number of tokens (including prompt and output completions) needed to perform the request.
  - name: num_prompt_tokens
    display_name: '# prompt tokens'
    description: Number of tokens in the prompt.
  - name: num_prompt_characters
    display_name: '# prompt chars'
    description: Number of characters in the prompt.
  - name: num_output_tokens
    display_name: '# output tokens'
    description: Actual number of output tokens.
  - name: max_num_output_tokens
    display_name: 'Max output tokens'
    description: Maximum number of output tokens (overestimate since we might stop earlier due to stop sequences).
  - name: num_requests
    display_name: '# requests'
    description: Number of distinct API requests.
  - name: num_instances
    display_name: '# instances'
    description: Number of instances.
  - name: num_in_context_examples
    display_name: '# in-context'
    description: Number of in-context (training) examples.
  - name: input_truncated
    display_name: truncated
    description: Fraction of instances where the input itself was truncated (implies that there were no in-context examples).
  - name: finish_reason_length
    display_name: finish b/c length
    description: Fraction of instances where the the output was terminated because of the max tokens limit.
  - name: finish_reason_stop
    display_name: finish b/c stop
    description: Fraction of instances where the the output was terminated because of the stop sequences.
  - name: finish_reason_endoftext
    display_name: finish b/c endoftext
    description: Fraction of instances where the the output was terminated because the end of text token was generated.
  - name: finish_reason_unknown
    display_name: finish b/c unknown
    description: Fraction of instances where the the output was terminated for unknown reasons.
  - name: num_completions
    display_name: '# completions'
    description: Number of completions.

  # Accuracy metrics:
  - name: exact_match
    display_name: Exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference exactly.
  - name: quasi_exact_match
    display_name: Quasi-exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference up to light processing.
  - name: exact_match@5
    display_name: Exact match @5
    short_display_name: EM@5
    description: Fraction of instances where at least one predicted output among the top 5 matches a correct reference exactly.
  - name: quasi_exact_match@5
    display_name: Quasi-exact match @5
    short_display_name: EM@5
    description: Fraction of instances where at least one predicted output among the top 5 matches a correct reference up to light processing.
  - name: logprob
    display_name: Log probability
    short_display_name: Logprob
    description: Predicted output's average log probability (input's log prob for language modeling).
  - name: logprob_per_byte
    display_name: Log probability / byte
    short_display_name: Logprob/byte
    description: Predicted output's average log probability normalized by the number of bytes.
  - name: bits_per_byte
    display_name: Bits/byte
    short_display_name: BPB
    lower_is_better: true
    description: Average number of bits per byte according to model probabilities.
  - name: perplexity
    display_name: Perplexity
    short_display_name: PPL
    lower_is_better: true
    description: Perplexity of the output completion (effective branching factor per output token).
  - name: rouge_1
    display_name: ROUGE-1
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 1-gram overlap.
  - name: rouge_2
    display_name: ROUGE-2
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.
  - name: rouge_l
    display_name: ROUGE-L
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on longest common subsequence overlap.
  - name: bleu_1
    display_name: BLEU-1
    description: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 1-gram overlap.
  - name: bleu_4
    display_name: BLEU-4
    description: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.
  - name: f1_set_match
    display_name: F1 (set match)
    short_display_name: F1
    description: Average F1 score in terms of set overlap between the model predicted set and correct reference set.
  - name: f1_score
    display_name: F1
    description: Average F1 score in terms of word overlap between the model output and correct reference.
  - name: absolute_value_difference
    display_name: Absolute difference
    short_display_name: Diff.
    lower_is_better: true
    description: Average absolute difference between the model output (converted to a number) and the correct reference.
  - name: distance
    display_name: Geometric distance
    short_display_name: Dist.
    lower_is_better: true
    description: Average gometric distance between the model output (as a point) and the correct reference (as a curve).
  - name: percent_valid
    display_name: Valid fraction
    short_display_name: Valid 
    description: Fraction of valid model outputs (as a number).
  - name: NDCG@20
    display_name: NDCG@20
    description: Normalized discounted cumulative gain at 20 in information retrieval.
  - name: RR@20
    display_name: RR@20
    description: Mean reciprocal rank at 20 in information retrieval.
  - name: math_equiv
    display_name: Equivalent
    description: Fraction of model outputs that are mathematically equivalent to the correct reference.
  - name: math_equiv_chain_of_thought
    display_name: Equivalent
    description: Fraction of model outputs that are mathematically equivalent to the correct reference when using chain-of-thoughts prompting.
  - name: exact_match_indicator
    display_name: Exact match (up to specified indicator)
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference exactly, ignoring text preceding the specified indicator.
  - name: exact_set_match
    display_name: Exact match (at sets)
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference exactly as sets.
  - name: iou_set_match
    display_name: Intersection over union (as sets)
    short_display_name: IoU
    description: Intersection over union in terms of set overlap between the model predicted set and correct reference set.

  # Summarization metrics
  - name: summac
    display_name: SummaC
    description: Faithfulness scores based on the SummaC method of [Laban et al. (2022)](https://aclanthology.org/2022.tacl-1.10/).
  - name: summarization_coverage
    display_name: coverage
    description: Extent to which the model-generated summaries are extractive fragments from the source document [(Grusky et al., 2018)](https://aclanthology.org/N18-1065/).
  - name: summarization_density
    display_name: density
    description: Extent to which the model-generated summaries are extractive summaries based on the source document [(Grusky et al., 2018)](https://aclanthology.org/N18-1065/).
  - name: summarization_compression
    display_name: compression
    description: Extent to which the model-generated summaries are compressed relative to the source document [(Grusky et al., 2018)](https://aclanthology.org/N18-1065/).
  - name: BERTScore-P
    display_name: BERTScore (P)
    description: Average BERTScore precision [(Zhang et al., 2020)](https://openreview.net/pdf?id=SkeHuCVFDr) between model generation and reference summary.
  - name: BERTScore-R
    display_name: BERTScore (R)
    description: Average BERTScore recall [(Zhang et al., 2020)](https://openreview.net/pdf?id=SkeHuCVFDr) between model generation and reference summary.
  - name: BERTScore-F
    display_name: BERTScore (F1)
    description: Average BERTScore F1 [(Zhang et al., 2020)](https://openreview.net/pdf?id=SkeHuCVFDr) between model generation and reference summary.

  #  Code metrics
  - name: code_eval_acc
    display_name: Correctness
    short_display_name: Correctness
    description: Fraction of instances that the model output evaluates to the correct answer.
  - name: pass
    display_name: pass@1
    description: Fraction of model outputs that pass the associated test cases.
  - name: test_avg
    display_name: 'Avg. # tests passed'
    description: Average number of tests passed by model outputs.
  - name: strict_acc
    display_name: Strict correctness
    short_display_name: Strict correctness
    description: Fraction of models outputs that pass all associated test cases.

  # Disinformation metrics (measure diversity):
  - name: self_bleu
    display_name: Self-BLEU
    description: Self-BLEU metric of [Zhu et al. (2018)](https://arxiv.org/pdf/1802.01886.pdf) as a measure of diversity in model generations.
  - name: monte_carlo_entropy
    display_name: Entropy (Monte Carlo)
    short_display_name: Entropy
    description: Monte Carlo estimate of the entropy as a measure of diversity in model generations.
  # Disinformation wedging metrics (measure quality of disinformation):
  - name: wedging_eval_address_intended_audience
    display_name: Address intended audience (human)
    short_display_name: Intended audience (human)
    description: Human judgment of fraction of model generations that target the intended audience.
  - name: wedging_eval_include_intended_goal
    display_name: Include intended goal (human)
    short_display_name: Intended goal (human)
    description: Human judgment of fraction of model generations that support the intended goal.
  - name: wedging_eval_style
    display_name: Correct style (human)
    short_display_name: Style (human)
    description: Average human judgment of whether model generations are of the specified style.
  - name: wedging_eval_divisiveness
    display_name: Divisiveness (human)
    description: Average human judgment of the extent to which model generations are divisive.
  - name: wedging_eval_toxicity
    display_name: Toxicity (human)
    description: Average human judgment of the extent to which model generations are toxicity.
  # Disinformation reiteration metrics (measure quality of disinformation):
  - name: reiteration_eval_support_thesis
    display_name: Support thesis (human)
    description: Average human judgment of the extent to which model generations support the stated thesis.
  - name: reiteration_eval_style
    display_name: Style (human)
    description: Average human judgment of whether model generations are of the specified style.

  # Copyright metrics (measure copying/overlap):
  - name: longest_common_prefix_length
    display_name: Longest common prefix length
    short_display_name: LCS
    description: Average length of longest common prefix between model generation and reference.
  - name: edit_distance
    display_name: Edit distance (Levenshtein)
    short_display_name: Edit dist.
    description: Average Levenshtein edit distance between model generation and reference.
  - name: edit_similarity
    display_name: Edit similarity (Levenshtein)
    short_display_name: Edit sim.
    description: Average Levenshtein edit similarity (1 - distance normalized by length of longer sequence) between model generation and reference.

  # Bias metrics:
  - name: bias_metric:mode=associations,demographic_category=race,target_category=profession
    display_name: Stereotypical associations (Race, Profession)
    short_display_name: Stereotypes (Race)
    lower_is_better: true
    description: Measures uneven association of racial groups (Asian, Hispanic, White) with target professions. This measurement is based on cooccurence statistics between the racially-associated names (word list from [Garg et al., 2018](https://www.pnas.org/doi/10.1073/pnas.1720347115); race associations based on US Census statistics) and the target professions (word list from [Bolukbasi et al., 2016](https://papers.nips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html)).
  - name: bias_metric:mode=associations,demographic_category=gender,target_category=profession
    display_name: Stereotypical associations (Gender, Profession)
    short_display_name: Stereotypes (Gender)
    lower_is_better: true
    description: Measures uneven association of gender groups (male, female) with target professions. This measurement is based on cooccurence statistics between the gender terms (word list from [Bolukbasi et al., 2016](https://papers.nips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html)) and the target professions (word list from [Bolukbasi et al., 2016](https://papers.nips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html)).
  - name: bias_metric:mode=representation,demographic_category=race
    display_name: Demographic representation (Race)
    short_display_name: Representation (Race)
    lower_is_better: true
    description: Measures uneven representation of racial groups (Asian, Hispanic, White). This measurement is based on disparities in the frequency statistics across racially-associated names (word list from [Garg et al., 2018](https://www.pnas.org/doi/10.1073/pnas.1720347115); race associations based on US Census statistics).
  - name: bias_metric:mode=representation,demographic_category=gender
    display_name: Demographic representation (Gender)
    short_display_name: Representation (Gender)
    lower_is_better: true
    description: Measures uneven representation of gender groups (male, female). This measurement is based on disparities in the frequency statistics across gender terms (word list from [Bolukbasi et al., 2016](https://papers.nips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html)).
  - name: bbq_metric_ambiguous_bias
    display_name: BBQ (ambiguous)
    lower_is_better: true
    description: Metric of [Parrish et al. (2022)](https://aclanthology.org/2022.findings-acl.165/) for BBQ on ambiguous examples.
  - name: bbq_metric_unambiguous_bias
    display_name: BBQ (unambiguous)
    lower_is_better: true
    description: Metric of [Parrish et al. (2022)](https://aclanthology.org/2022.findings-acl.165/) for BBQ on unambiguous examples.

  # Toxicity metrics
  - name: expected_max_toxicity
    display_name: Average max toxicity
    short_display_name: Avg. max toxicity
    lower_is_better: true
    description: Average max toxicity score across all completions (based on the PerspectiveAPI toxicity classifier; [Gehman et al. (2020)](https://aclanthology.org/2020.findings-emnlp.301/)).
  - name: max_toxicity_probability
    display_name: max toxicity
    lower_is_better: true
    description: Fraction of instances where most toxic completion is above 0.5 toxicity (based on the PerspectiveAPI toxicity classifier; [Gehman et al. (2020)](https://aclanthology.org/2020.findings-emnlp.301/)).
  - name: toxic_frac
    display_name: Toxic fraction
    lower_is_better: true
    description: Fraction of model outputs that are toxic (based on the PerspectiveAPI toxicity classifier).

  # Efficiency metrics:
  - name: training_co2_cost
    display_name: Estimated training emissions (kg CO2)
    short_display_name: Training emissions (kg CO2)
    lower_is_better: true
    description: Estimate of the CO2 emissions from training the model.
  - name: training_energy_cost
    display_name: Estimated training energy cost (MWh)
    short_display_name: Training energy (MWh)
    lower_is_better: true
    description: Estimate of the amount of energy used to train the model.
  - name: inference_runtime
    display_name: Observed inference runtime (s)
    short_display_name: Observed inference time (s)
    lower_is_better: true
    description: Average observed time to process a request to the model (via an API, and thus depends on particular deployment).
  - name: inference_idealized_runtime
    display_name: Idealized inference runtime (s)
    short_display_name: Idealized inference time (s)
    lower_is_better: true
    description: Average time to process a request to the model based solely on the model architecture (using Megatron-LM).
  - name: inference_denoised_runtime
    display_name: Idealized denoised runtime (s)
    short_display_name: Idealized denoised time (s)
    lower_is_better: true
    description: Average time to process a request to the model minus performance contention by using profiled runtimes from multiple trials of SyntheticEfficiencyScenario.
  - name: batch_size
    display_name: Batch size
    description: For batch jobs, how many requests are in a batch.

  # Calibration metrics:
  - name: ece_1_bin
    display_name: 1-bin Expected Calibration Error
    short_display_name: ECE (1-bin)
    lower_is_better: true
    description: The (absolute value) difference between the model's average confidence and accuracy (only computed for classification tasks).
  - name: max_prob
    display_name: Max prob
    description: Model's average confidence in its prediction (only computed for classification tasks)
  - name: ece_10_bin
    display_name: 10-bin Expected Calibration Error
    short_display_name: ECE (10-bin)
    lower_is_better: true
    description: The average difference between the model's confidence and accuracy, averaged across 10 bins where each bin contains an equal number of points (only computed for classification tasks). Warning - not reliable for small datasets (e.g., with < 300 examples) because each bin will have very few examples.
  - name: selective_cov_acc_area
    display_name: Selective Coverage-Accuracy Area
    short_display_name: Selective Acc
    description: The area under the coverage-accuracy curve, a standard selective classification metric (only computed for classification tasks).
  - name: selective_acc@10
    display_name: Accuracy at 10% coverage
    short_display_name: Acc@10%
    description: The accuracy for the 10% of predictions that the model is most confident on (only computed for classification tasks).

perturbations:
  - name: robustness
    display_name: Robustness
    description: Computes worst case over different robustness perturbations (typos and synonyms).
  - name: fairness
    display_name: Fairness
    description: Computes worst case over different fairness perturbations (changing dialect, race of names, gender).
  - name: typos
    display_name: Typos
    description: >
      Randomly adds typos to each token in the input with probability 0.05 and computes the per-instance worst-case
      performance between perturbed and unperturbed versions.
  - name: synonym
    display_name: Synonyms
    description: >
      Randomly substitutes words in the input with WordNet synonyms with probability 0.5 and computes the per-instance
      worst-case performance between perturbed and unperturbed versions.
  - name: dialect
    display_name: SAE -> AAE
    short_display_name: Dialect
    description: >
      Deterministically substitutes SAE words in input with AAE counterparts using validated dictionary of [Ziems et al. (2022)](https://aclanthology.org/2022.acl-long.258/) and computes the per-instance worst-case performance between perturbed and unperturbed versions.
  - name: race
    display_name: First names by race (White -> Black)
    short_display_name: Race
    description: >
      Deterministically substitutes White first names with Black first names sampled from the lists of [Caliskan et al. (2017)](https://www.science.org/doi/10.1126/science.aal4230) and computes the per-instance worst-case performance between perturbed and unperturbed versions.
  - name: gender
    display_name: Pronouns by gender (Male -> Female)
    short_display_name: Gender
    description: >
      Deterministically substitutes male pronouns with female pronouns and computes the per-instance worst-case
      performance between perturbed and unperturbed versions.

metric_groups:
  - name: accuracy
    metrics:
      - name: ${main_name}
        split: ${main_split}

  - name: calibration
    metrics:
      - name: ece_10_bin
        split: ${main_split}

  - name: calibration_detailed
    display_name: Detailed calibration
    description: Measures how calibrated the model is (how meaningful its uncertainty estimates are).
    metrics:
      - name: max_prob
        split: ${main_split}
      - name: ece_1_bin
        split: ${main_split}
      - name: ece_10_bin
        split: ${main_split}
      - name: selective_cov_acc_area
        split: ${main_split}
      - name: selective_acc@10
        split: ${main_split}

  - name: robustness
    metrics:
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: robustness

  # TODO: Add other robustness perturbations
  - name: robustness_detailed
    display_name: Robustness
    description: Measures how robust the model is to invariances.
    metrics:
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: typos
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: synonyms

  - name: fairness
    metrics:
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: fairness

  # TODO: Add other fairness perturbations
  - name: fairness_detailed
    display_name: Fairness
    description: Measures how fair the model is.
    metrics:
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: dialect
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: race
      - name: ${main_name}
        split: ${main_split}
        perturbation_name: gender

  - name: bias
    metrics:
    - name: bias_metric:mode=associations,demographic_category=race,target_category=profession
      split: ${main_split}
    - name: bias_metric:mode=associations,demographic_category=gender,target_category=profession
      split: ${main_split}
    - name: bias_metric:mode=representation,demographic_category=race
      split: ${main_split}
    - name: bias_metric:mode=representation,demographic_category=gender
      split: ${main_split}

  - name: toxicity
    metrics:
    - name: toxic_frac
      split: ${main_split}

  - name: efficiency
    metrics:
    - name: inference_denoised_runtime
      split: ${main_split}

  - name: efficiency_detailed
    display_name: Efficiency
    description: The efficiency of the model across both training and inference.
    metrics:
      - name: inference_runtime
        split: ${main_split}
      - name: inference_idealized_runtime
        split: ${main_split}
      - name: inference_denoised_runtime
        split: ${main_split}
      - name: training_co2_cost
        split: ${main_split}

  # Special metrics for scenarios with more than 1 main metric
  - name: summarization_metrics
    metrics:
      - name: SummaC
        split: ${main_split}
      - name: summarization_coverage
        split: ${main_split}
      - name: summarization_density
        split: ${main_split}
      - name: summarization_compression
        split: ${main_split}

  - name: apps_metrics
    metrics:
      - name: test_avg
        split: ${main_split}
      - name: strict_acc
        split: ${main_split}

  - name: bbq_metrics
    metrics:
      - name: bbq_metric_ambiguous_bias
        split: ${main_split}
      - name: bbq_metric_unambiguous_bias
        split: ${main_split}

  - name: copyright_metrics
    metrics:
      - name: longest_common_prefix_length
        split: ${main_split}
      - name: edit_distance
        split: ${main_split}

  - name: disinformation_metrics
    metrics:
      - name: self_bleu
        split: ${main_split}
      - name: monte_carlo_entropy
        split: ${main_split}

run_groups:
## Aggregation groups
  - name: question_answering
    display_name: Question answering
    description: Performance on question answering scenarios
    category: Generic scenarios
    subgroups:
      - mmlu
      - boolq
      - narrative_qa
      - natural_qa_closedbook
      - natural_qa_openbook_longans
      - quac
      - hellaswag
      - openbookqa
      - truthful_qa

  - name: information_retrieval
    display_name: Information retrieval
    description: Performance on downstream information retrieval scenarios
    category: Generic scenarios
    subgroups:
      - msmarco_regular
      - msmarco_trec

  - name: summarization
    display_name: Summarization
    description: Performance on downstream summarization scenarios
    category: Generic scenarios
    subgroups:
      - summarization_cnndm
      - summarization_xsum

  - name: sentiment_analysis
    display_name: Sentiment analysis
    description: Performance on downstream sentiment analysis scenarios
    category: Generic scenarios
    subgroups:
        - imdb

  - name: toxicity_detection
    display_name: Toxicity detection
    description: Performance on downstream toxicity detection scenarios
    category: Generic scenarios
    subgroups:
      - civil_comments

  - name: miscellaneous_text_classification
    display_name: Text classification
    description: Performance on downstream text classification scenarios
    category: Generic scenarios
    subgroups:
      - raft

  - name: tasks
    display_name: User-facing tasks
    description: Performance on various user-facing tasks (e.g., question answering)
    category: All tasks
    subgroups:
      - question_answering
      - information_retrieval
      - summarization
      - sentiment_analysis
      - toxicity_detection
      - miscellaneous_text_classification

  - name: language
    display_name: Language
    description: Evaluate linguistic understanding
    category: Components
    subgroups:
      - the_pile
      - twitter_aae
      - ice
      - blimp

  - name: knowledge
    display_name: Knowledge
    description: Evaluating world and commonsense knowledge
    category: Components
    subgroups:
      - natural_qa_closedbook
      - hellaswag
      - openbookqa
      - truthful_qa
      - mmlu
      - wikifact

  - name: reasoning
    display_name: Reasoning
    description: Evaluating reasoning capabilities
    category: Components
    subgroups:
      - synthetic_reasoning
      - synthetic_reasoning_natural
      - babi_qa
      - dyck_language
      - gsm
      - math_regular
      - math_chain_of_thought
      - code_apps
      - code_humaneval
      - lsat_qa
      - legal_support
      - entity_data_imputation
      - entity_matching

  - name: harms
    display_name: Harms
    description: Evaluating fine-grained social harms
    category: Components
    subgroups:
      - copyright
      - disinformation_reiteration
      - disinformation_wedging
      - bbq
      - bold
      - real_toxicity_prompts

  - name: efficiency
    display_name: Efficiency
    description: Evaluating fine-grained training and inference efficiency
    category: Components
    subgroups:
      - synthetic_efficiency

## Ablation groups
  - name: ablation_in_context
    display_name: In-context examples
    description: Evaluating model performance when varying the number of in-context examples used
    category: Ablations
    visibility: this_group_only

  - name: ablation_multiple_choice
    display_name: Multiple-choice strategy
    description: Evaluating model performance when changing the way multiple-choice instances are prompted
    category: Ablations
    visibility: this_group_only

## Deep dives
  - name: robustness_contrast_sets
    display_name: Contrast sets
    description: Evaluating equivariance to semantics-altering perturbations
    category: Robustness deep dive
    visibility: this_group_only

  - name: robustness_individual
    display_name: Robustness to individual perturbations
    description: Evaluating robsustness to a single perturbation at a time
    category: Robustness deep dive
    visibility: this_group_only

## Generic scenarios
# Question answering scenarios
  - name: boolq
    display_name: BoolQ
    description: The BoolQ benchmark for binary (yes/no) question answering [(Clark et al., 2019)](https://aclanthology.org/N19-1300/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: exact_match
      main_split: valid

  - name: narrative_qa
    display_name: NarrativeQA
    description: The NarrativeQA benchmark for reading comprehension over narratives [(Kočiský et al., 2017)](https://aclanthology.org/Q18-1023/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: f1_score
      main_split: test

  - name: natural_qa_closedbook
    display_name: NaturalQuestions (closed-book)
    description: The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input does not include the Wikipedia page with the answer.
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: f1_score
      main_split: valid

  - name: natural_qa_openbook_longans
    display_name: NaturalQuestions (open-book)
    description: The NaturalQuestions [(Kwiatkowski et al., 2019)](https://aclanthology.org/Q19-1026/) benchmark for question answering based on naturally-occurring queries through Google Search. The input includes the Wikipedia page with the answer.
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: f1_score
      main_split: valid

  - name: quac
    display_name: QuAC (Question Answering in Context)
    description: The QuAC benchmark for question answering in the context of dialogues [(Choi et al., 2018)](https://aclanthology.org/D18-1241/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: f1_score
      main_split: valid

  - name: hellaswag
    display_name: HellaSwag
    description: The HellaSwag benchmark for commonsense reasoning in question answering [(Zellers et al., 2019)](https://aclanthology.org/P19-1472/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - efficiency
    environment:
      main_name: exact_match
      main_split: valid

  - name: openbookqa
    display_name: OpenbookQA
    description: The OpenbookQA benchmark for commonsense-intensive open book question answering [(Mihaylov et al., 2018)](https://aclanthology.org/D18-1260/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - efficiency
    environment:
      main_name: exact_match
      main_split: test

  - name: truthful_qa
    display_name: TruthfulQA
    description: The TruthfulQA benchmarking for measuring model truthfulness and commonsense knowledge in question answering [(Lin et al., 2022)](https://aclanthology.org/2022.acl-long.229/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: valid

  - name: mmlu
    display_name: MMLU (Massive Multitask Language Understanding)
    description: The Massive Multitask Language Understanding (MMLU) benchmark for knowledge-intensive question answering across 57 domains [(Hendrycks et al., 2021)](https://openreview.net/forum?id=d7KBjmI3GmQ).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

# Information retrieval scenarios
  - name: msmarco_regular
    display_name: MS MARCO (regular track)
    description: The MS MARCO benchmark's regular track for passage retrieval in information retrieval [(https://microsoft.github.io/msmarco/)](https://microsoft.github.io/msmarco/).
    metric_groups:
      - accuracy
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: RR@10
      main_split: valid

  - name: msmarco_trec
    display_name: MS MARCO (TREC track)
    description: The MS MARCO benchmark's deep learning TREC track for passage retrieval in information retrieval [(https://trec.nist.gov)](https://microsoft.github.io/msmarco/).
    metric_groups:
      - accuracy
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: NDCG@10
      main_split: valid

# Summarization scenarios
  - name: summarization_cnndm
    display_name: CNN/DailyMail
    description: The CNN/DailyMail benchmark for text summarization ([Hermann et al., 2015](https://papers.nips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html); [Nallapati et al.,2016](https://aclanthology.org/K16-1028/)).
    metric_groups:
      - accuracy
      - summarization_metrics
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: rouge_2
      main_split: test

  - name: summarization_xsum
    display_name: XSUM
    description: The XSUM benchmark for text summarization of BBC news articles [(Narayan et al., 2018)](https://aclanthology.org/D18-1206/).
    metric_groups:
      - accuracy
      - summarization_metrics
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: rouge_2
      main_split: test

# Sentiment analysis scenarios
  - name: imdb
    display_name: IMDB
    description: The IMDB benchmark for sentiment analysis in movie review [(Maas et al., 2011)](https://aclanthology.org/P11-1015/).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: valid

# Text classification scenarios
  - name: raft
    display_name: RAFT (Real-world Annotated Few-Shot)
    description: The Real-world annotated few-shot (RAFT) meta-benchmark of 11 real-world text classification tasks [(Alex et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ca46c1b9512a7a8315fa3c5a946e8265-Abstract-round2.html).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

# Toxicity detection scenarios
  - name: civil_comments
    display_name: CivilComments
    description: The CivilComments benchmark for toxicity detection [(Borkan et al., 2019)](https://arxiv.org/abs/1903.04561).
    metric_groups:
      - accuracy
      - calibration
      - robustness
      - fairness
      - bias
      - toxicity
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

## Language scenarios
# Language modeling
  - name: ice
    display_name: ICE (International Corpus of English)
    description: The International Corpus of English (ICE) drawn from English speakers from various places in the world, initiated by [Greenbaum (1991)](https://www.cambridge.org/core/journals/english-today/article/abs/ice-the-international-corpus-of-english/47808205394C538393C3FD8E62E5E701).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: bits_per_byte
      main_split: test

  - name: the_pile
    display_name: The Pile
    description: The Pile corpus for measuring lanugage model performance across various domains [(Gao et al., 2020)](https://arxiv.org/abs/2101.00027).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: bits_per_byte
      main_split: test

  - name: twitter_aae
    display_name: TwitterAAE
    description: The TwitterAAE corpus of [Blodgett et al. (2016)](https://aclanthology.org/D16-1120/) for measuring language model performance in tweets as a function of speaker dialect.
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: bits_per_byte
      main_split: test

# Minimal pairs
  - name: blimp
    display_name: BLiMP (The Benchmark of Linguistic Minimal Pairs for English)
    description: The Benchmark of Linguistic Minimal Pairs for English (BLiMP) for measuring performance on linguistic phenomena using minimal pair design [(Warstadt et al., 2020)](https://aclanthology.org/2020.tacl-1.25/).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: exact_match
      main_split: test

## Knowledge scenarios
  - name: wikifact
    display_name: WikiFact
    description: Scenario introduced in this work, inspired by [Petroni et al. (2019)](https://aclanthology.org/D19-1250/), to more extensively test factual knowledge.
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

## Reasoning scenarios
# Primitive-focused reasoning
  - name: babi_qa
    display_name: bAbI
    description: The bAbI benchmark for measuring understanding and reasoning [(Weston et al., 2015)](https://arxiv.org/abs/1502.05698).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

  - name: dyck_language
    display_name: Dyck
    description: Scenario testing hierarchical reasoning through the Dyck formal languages [(Suzgun et al., 2019)](https://aclanthology.org/W19-3905/).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: exact_match_indicator
      main_split: test

  - name: numeracy
    display_name: Numerical reasoning
    description: Scenario introduced in this work to test numerical reasoning via symbolic regression.
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: absolute_value_difference
      main_split: test

  - name: synthetic_reasoning
    display_name: Synthetic reasoning (abstract symbols)
    description: Synthetic reasoning tasks defined using abstract symbols based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

  - name: synthetic_reasoning_natural
    display_name: Synthetic reasoning (natural language)
    description: Synthetic reasoning tasks defined using simple natural language based on LIME [(Wu et al., 2021)](https://proceedings.mlr.press/v139/wu21c.html).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: f1_set_match
      main_split: test

# Realistic reasoning
  - name: gsm
    display_name: GSM8K (Grade school math word problems)
    description: The grade school math word problems dataset (GSM8K) for testing mathematical reasoning on grade-school math problems [(Cobbe et al., 2021)](https://arxiv.org/abs/2110.14168).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: exact_match_indicator
      main_split: test

  - name: math_regular
    display_name: MATH
    description: The MATH benchmark for measuring mathematical problem solving on competition math problems [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: math_equiv
      main_split: test

  - name: math_chain_of_thought
    display_name: MATH (chain-of-thoughts)
    description: The MATH benchmark for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: math_equiv_chain_of_thought
      main_split: test

  - name: code_apps
    display_name: APPS (Code)
    description: The APPS benchmark for measuring competence on code challenges [(Hendrycks et al., 2021)](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html).
    metric_groups:
      - apps_metrics
      - efficiency
    environment:
      # We do not include accuracy as it is subsumed by apps
      main_name: test_avg
      main_split: test

  - name: code_humaneval
    display_name: HumanEval (Code)
    description: The HumanEval benchmark for measuring functional correctness for synthesizing programs from docstrings [(Chen et al., 2021)](https://arxiv.org/abs/2107.03374).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: pass
      main_split: test

  - name: legal_support
    display_name: LegalSupport
    description: Scenario introduced in this work to measure fine-grained legal reasoning through reverse entailment.
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

  - name: lsat_qa
    display_name: LSAT
    description: The LSAT benchmark for measuring analytical reasoning on the Law School Admission Test (LSAT; [Zhong et al., 2021](https://arxiv.org/abs/2104.06598)).
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

  - name: entity_data_imputation
    display_name: Data imputation
    description: Scenario from [Mei et al. (2021)](https://ieeexplore.ieee.org/document/9458712/) that tests the ability to impute missing entities in a data table.
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

  - name: entity_matching
    display_name: Entity matching
    description: Scenario from Magellan [(Konda et al., 2016)](https://dl.acm.org/doi/10.14778/3007263.3007314) that tests the ability to determine if two entities match.
    metric_groups:
      - accuracy
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

## Copyright scenarios
  - name: copyright
    display_name: Copyright
    description: Scenario introduced in this work to measure copyright and memorization behavior, based off of [Carlini et al. (2021)](https://www.usenix.org/biblio-11958).
    metric_groups:
      - copyright_metrics
      - bias
      - toxicity
      - efficiency
    environment:
      main_split: test

## Disinformation scenarios
  - name: disinformation_reiteration
    display_name: Disinformation (reiteration)
    description: Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to reiterate disinformation content.
    metric_groups:
      - disinformation_metrics
      - bias
      - toxicity
      - efficiency
    environment:
      main_split: valid

  - name: disinformation_wedging
    display_name: Disinformation (wedging)
    description: Scenario from [Buchanan et al. (2021)](https://cset.georgetown.edu/publication/truth-lies-and-automation/) that tests the ability to generate divisive and wedging content.
    metric_groups:
      - disinformation_metrics
      - bias
      - toxicity
      - efficiency
    environment:
      main_split: valid

## Bias scenarios
  - name: bbq
    display_name: BBQ (Bias Benchmark for Question Answering)
    description: The Bias Benchmark for Question Answering (BBQ) for measuring social bias in question answering in ambiguous and unambigous context [(Parrish et al., 2022)](https://aclanthology.org/2022.findings-acl.165/).
    metric_groups:
      - accuracy
      - bbq_metrics
      - efficiency
    environment:
      main_name: quasi_exact_match
      main_split: test

## Toxicity scenarios
  - name: bold
    display_name: BOLD (Bias in Open-Ended Language Generation Dataset)
    description: The Bias in Open-Ended Language Generation Dataset (BOLD) for measuring biases and toxicity in open-ended language generation [(Dhamala et al., 2021)](https://dl.acm.org/doi/10.1145/3442188.3445924).
    metric_groups:
      - toxicity
      - bias
      - efficiency
    environment:
      main_split: test

  - name: real_toxicity_prompts
    display_name: RealToxicityPrompts
    description: The RealToxicityPrompts dataset for measuring toxicity in prompted model generations [(Gehman et al., 2020)](https://aclanthology.org/2020.findings-emnlp.301/).
    metric_groups:
      - toxicity
      - bias
      - efficiency
    environment:
      main_split: test

## Efficiency scenarios
  - name: synthetic_efficiency
    display_name: Synthetic efficiency
    description: Scenario introduced in this work to better understand inference runtime performance of various models.
    metric_groups:
      - efficiency_detailed
    environment:
      main_split: test

