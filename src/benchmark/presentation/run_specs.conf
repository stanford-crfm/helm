# List the RunSpecs that we want to display on the website in this file.
# A RunSpec specifies how to do a single run, which gets a scenario, adapts it, and computes a list of metrics.
#
# RunSpecs are unique. Mark the RunSpec with either READY or WIP (work in progress):
#    For READY RunSpecs, we evaluate and generate metrics.
#    For WIP RunSpecs, we just estimate token usage.
#
# Place your new RunSpec description under the appropriate section.

##### Generic #####
"boolq:model=default": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"boolq:model=limited_functionality": {status: "READY"}

"imdb:model=default": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"imdb:model=anthropic/stanford-online-helpful-v4-s3": {status: "READY"}

"narrative_qa:model=default": {status: "READY"}

"natural_qa:model=default,mode=closedbook": {status: "READY"}
# TODO: AI21 tokenizer failing
#       https://github.com/stanford-crfm/benchmarking/issues/346
"natural_qa:mode=openbook-longans": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"natural_qa:model=anthropic/stanford-online-helpful-v4-s3,mode=closedbook": {status: "READY"}
"natural_qa:model=anthropic/stanford-online-helpful-v4-s3,mode=openbook-longans": {status: "READY"}

"news_qa:model=default": {status: "READY"}
"quac:model=default": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"quac:model=anthropic/stanford-online-helpful-v4-s3": {status: "READY"}

"raft:model=default,subset=ade_corpus_v2": {status: "READY"}
"raft:model=default,subset=banking_77": {status: "READY"}
"raft:model=default,subset=neurips_impact_statement_risks": {status: "READY"}
"raft:model=default,subset=one_stop_english": {status: "READY"}
"raft:model=default,subset=overruling": {status: "READY"}
"raft:model=default,subset=semiconductor_org_types": {status: "READY"}
"raft:model=default,subset=tweet_eval_hate": {status: "READY"}
"raft:model=default,subset=twitter_complaints": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=ade_corpus_v2": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=banking_77": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=neurips_impact_statement_risks": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=one_stop_english": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=overruling": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=semiconductor_org_types": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=systematic_review_inclusion": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=tai_safety_research": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=terms_of_service": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=tweet_eval_hate": {status: "READY"}
"raft:model=anthropic/stanford-online-helpful-v4-s3,subset=twitter_complaints": {status: "READY"}

# TODO: @Faisal @Esin - Look at results to decide on the temp setting.
"summarization_cnndm:model=default,temperature=0": {status: "READY"}
"summarization_cnndm:model=default,temperature=0.3": {status: "READY"}
"summarization_xsum:model=default,temperature=0": {status: "READY"}
"summarization_xsum:model=default,temperature=0.3": {status: "READY"}

##### Information Retrieval #####

# 1. Regular passage task, using only topk validation examples
"msmarco:task=passage,track=regular,use_topk_passages=True,valid_topk=50,num_valid_queries=200": {status: "WIP"}

# 2. Regular passage task, using all the validation examples in topk and qrels
"msmarco:task=passage,track=regular,use_qrels_passages=True,use_topk_passages=True,valid_topk=50,num_valid_queries=200": {status: "WIP"}

# 3. TREC passage task, using only topk validation examples
"msmarco:task=passage,track=trec,use_topk_passages=True,valid_topk=100": {status: "WIP"}

# 4. TREC passage task, using only the validation examples in qrels
"msmarco:task=passage,track=trec,use_qrels_passages=True": {status: "WIP"}

# 5. TREC passage task, using all the validation examples in topk and qrels
"msmarco:task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=100": {status: "WIP"}


# @TODO Turn the above run specs to READY, after the following:
#   (1) Completing the runs below.
#   (2) Deciding on hyperparameters we want for the final runs on all the models.
#   (3) Changing the hyperparameters above accordingly.

# 1. Experimental runs: Regular passage task, using only topk validation examples
#    J1-Large
"msmarco:model=ai21/j1-large,task=passage,track=regular,use_topk_passages=True,valid_topk=50,num_valid_queries=200": {status: "READY"}
#    J1-Grande
"msmarco:model=ai21/j1-grande,task=passage,track=regular,use_topk_passages=True,valid_topk=50,num_valid_queries=200": {status: "READY"}

# 2. Experimental runs: Regular passage task, using all the validation examples in topk and qrels
#    J1-Large
"msmarco:model=ai21/j1-large,task=passage,track=regular,use_qrels_passages=True,use_topk_passages=True,valid_topk=50,num_valid_queries=200": {status: "READY"}
#    J1-Grande
"msmarco:model=ai21/j1-grande,task=passage,track=regular,use_qrels_passages=True,use_topk_passages=True,valid_topk=50,num_valid_queries=200": {status: "READY"}

# 3. Experimental runs: TREC passage task, using only topk validation examples
#    J1-Large
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_topk_passages=True,valid_topk=200": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_topk_passages=True,valid_topk=100": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_topk_passages=True,valid_topk=50": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_topk_passages=True,valid_topk=30": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_topk_passages=True,valid_topk=20": {status: "READY"}
#    J1-Grande
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_topk_passages=True,valid_topk=200": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_topk_passages=True,valid_topk=100": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_topk_passages=True,valid_topk=50": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_topk_passages=True,valid_topk=30": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_topk_passages=True,valid_topk=20": {status: "READY"}

# 4. Experimental runs: TREC passage task, using only the validation examples in qrels
#    J1-Large
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_qrels_passages=True": {status: "READY"}
#    J1-Grande
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_qrels_passages=True": {status: "READY"}

# 5. Experimental runs: TREC passage task, using only topk validation examples
#    J1-Large
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=200": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=100": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=50": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=30": {status: "READY"}
"msmarco:model=ai21/j1-large,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=20": {status: "READY"}
#    J1-Grande
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=200": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=100": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=50": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=30": {status: "READY"}
"msmarco:model=ai21/j1-grande,task=passage,track=trec,use_qrels_passages=True,use_topk_passages=True,valid_topk=20": {status: "READY"}


##### Language #####
# TODO: Support AI21 tokenizer for language modeling:
#       https://github.com/stanford-crfm/benchmarking/issues/189

# TODO: @Yian @Rishi - Look at results and decide what we want.
"blimp:phenomenon=island_effects": {status: "READY"}
"blimp:phenomenon=anaphor_agreement": {status: "READY"}
"blimp:phenomenon=argument_structure": {status: "READY"}
"blimp:phenomenon=determiner_noun_agreement": {status: "READY"}
"blimp:phenomenon=subject_verb_agreement": {status: "READY"}
"blimp:phenomenon=ellipsis": {status: "READY"}
"blimp:phenomenon=control_raising": {status: "READY"}
"blimp:phenomenon=quantifiers": {status: "READY"}
"blimp:phenomenon=irregular_forms": {status: "READY"}
"blimp:phenomenon=npi_licensing": {status: "READY"}
"blimp:phenomenon=binding": {status: "READY"}
"blimp:phenomenon=filler_gap_dependency": {status: "READY"}

# TODO: @Yian @Rishi - Decide what subsets we want of The Pile.
"the_pile:subset=OpenSubtitles": {status: "READY"}

# TODO: @Yian - Update with improved info once we get from Brendan et al.
"twitter_aae:demographic=aa": {status: "READY"}
"twitter_aae:demographic=white": {status: "READY"}

# TODO: @Nathan @Rishi @Percy - Decide how to handle intersectional groups for ICE.
# TODO: @Nathan @Rishi @Percy - Decide on which regional subsets to include for ICE.
"ice:subset=CAN": {status: "READY"}
# TODO: @Nathan - Add East Africa.
"ice:subset=HK": {status: "READY"}
"ice:subset=IND": {status: "READY"}
"ice:subset=JA": {status: "READY"}
"ice:subset=PHI": {status: "READY"}
"ice:subset=SIN": {status: "READY"}
"ice:subset=USA": {status: "READY"}

"ice:split=spoken": {status: "READY"}
"ice:split=written": {status: "READY"}

# TODO: FileNotFoundError: [Errno 2] No such file or directory: 'benchmark_output/scenarios/ice/ICE-CAN/Corpus/W2B-037.txt'
#       https://github.com/stanford-crfm/benchmarking/issues/406
# "ice:gender=F": {status: "READY"}
# "ice:gender=M": {status: "READY"}

# TODO: Support AI21 Tokenizer for language modeling
# "wikitext_103:model=default": {status: "READY"}
"wikitext_103:model=openai/davinci": {status: "READY"}
"wikitext_103:model=openai/curie": {status: "READY"}
"wikitext_103:model=openai/text-davinci-001": {status: "READY"}
"wikitext_103:model=openai/text-curie-001": {status: "READY"}


##### Knowledge #####

"commonsense_qa:model=default,dataset=hellaswag,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=openbookqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=commonsenseqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=piqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=default,dataset=siqa,method=mcqa": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"commonsense_qa:model=anthropic/stanford-online-helpful-v4-s3,dataset=hellaswag,method=mcqa": {status: "READY"}
"commonsense_qa:model=anthropic/stanford-online-helpful-v4-s3,dataset=openbookqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=anthropic/stanford-online-helpful-v4-s3,dataset=commonsenseqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=anthropic/stanford-online-helpful-v4-s3,dataset=piqa,method=mcqa": {status: "READY"}
"commonsense_qa:model=anthropic/stanford-online-helpful-v4-s3,dataset=siqa,method=mcqa": {status: "READY"}


# TODO: @Michi @Percy @Dimitris @Rishi - Decide if we are doing clm strategy in main benchmark.
# Note: We can always include clm as an ablation.
# "commonsense_qa:dataset=hellaswag,method=clm": {status: "READY"}
# "commonsense_qa:dataset=openbookqa,method=clm": {status: "READY"}
# "commonsense_qa:dataset=commonsenseqa,method=clm": {status: "READY"}
# TODO: Currently produces an error - see issue 223
# "commonsense_qa:dataset=piqa,method=clm": {status: "READY"}
# "commonsense_qa:dataset=siqa,method=clm": {status: "READY"}

# For MMLU, we sampled the following 10 subjects, which cover diverse topics across humanities, social sciences and STEM.
"mmlu:model=default,subject=abstract_algebra": {status: "READY"}
"mmlu:model=default,subject=anatomy": {status: "READY"}
"mmlu:model=default,subject=college_chemistry": {status: "READY"}
"mmlu:model=default,subject=computer_security": {status: "READY"}
"mmlu:model=default,subject=econometrics": {status: "READY"}
"mmlu:model=default,subject=global_facts": {status: "READY"}
"mmlu:model=default,subject=jurisprudence": {status: "READY"}
"mmlu:model=default,subject=professional_medicine": {status: "READY"}  # It will take about ~335,221 tokens to evaluate.
"mmlu:model=default,subject=philosophy": {status: "READY"}
"mmlu:model=default,subject=us_foreign_policy": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"mmlu:model=limited_functionality,subject=abstract_algebra": {status: "READY"}
"mmlu:model=limited_functionality,subject=anatomy": {status: "READY"}
"mmlu:model=limited_functionality,subject=college_chemistry": {status: "READY"}
"mmlu:model=limited_functionality,subject=computer_security": {status: "READY"}
"mmlu:model=limited_functionality,subject=econometrics": {status: "READY"}
"mmlu:model=limited_functionality,subject=global_facts": {status: "READY"}
"mmlu:model=limited_functionality,subject=jurisprudence": {status: "READY"}
"mmlu:model=limited_functionality,subject=medical_genetics": {status: "READY"}
"mmlu:model=limited_functionality,subject=philosophy": {status: "READY"}
"mmlu:model=limited_functionality,subject=us_foreign_policy": {status: "READY"}

# For WikiFact, we sampled the following 10 relation types, which cover diverse topics across general facts, humanities, social sciences and STEM.
"wikifact:model=default,k=5,subject=P19": {status: "READY"}
"wikifact:model=default,k=5,subject=P31": {status: "READY"}
"wikifact:model=default,k=5,subject=P50": {status: "READY"}
"wikifact:model=default,k=5,subject=P1620": {status: "READY"}
"wikifact:model=default,k=5,subject=P39": {status: "READY"}
"wikifact:model=default,k=5,subject=P36": {status: "READY"}
"wikifact:model=default,k=5,subject=P38": {status: "READY"}
"wikifact:model=default,k=5,subject=P61": {status: "READY"}
"wikifact:model=default,k=5,subject=P2175": {status: "READY"}
"wikifact:model=default,k=5,subject=P780": {status: "READY"}


##### Reasoning #####

# TODO: @Tony W. - None of HumanEval, APPS, LSAT, Dyck-n, number relationship are included.
# TODO: @Tony W. - Please add whichever ones should be run at this stage.

# TODO: Evaluate with "use_official_prompt=False" once the following issue is resolved:
#		https://github.com/stanford-crfm/benchmarking/issues/317
"math:model=default,subject=number_theory,level=1": {status: "READY"}
"math:model=default,subject=intermediate_algebra,level=1": {status: "READY"}
"math:model=default,subject=algebra,level=1": {status: "READY"}
"math:model=default,subject=prealgebra,level=1": {status: "READY"}
"math:model=default,subject=geometry,level=1": {status: "READY"}
"math:model=default,subject=counting_and_probability,level=1": {status: "READY"}
"math:model=default,subject=precalculus,level=1": {status: "READY"}

"math:model=default,subject=number_theory,level=2": {status: "READY"}
"math:model=default,subject=intermediate_algebra,level=2": {status: "READY"}
"math:model=default,subject=algebra,level=2": {status: "READY"}
"math:model=default,subject=prealgebra,level=2": {status: "READY"}
"math:model=default,subject=geometry,level=2": {status: "READY"}
"math:model=default,subject=counting_and_probability,level=2": {status: "READY"}
"math:model=default,subject=precalculus,level=2": {status: "READY"}

"math:model=default,subject=number_theory,level=3": {status: "READY"}
"math:model=default,subject=intermediate_algebra,level=3": {status: "READY"}
"math:model=default,subject=algebra,level=3": {status: "READY"}
"math:model=default,subject=prealgebra,level=3": {status: "READY"}
"math:model=default,subject=geometry,level=3": {status: "READY"}
"math:model=default,subject=counting_and_probability,level=3": {status: "READY"}
"math:model=default,subject=precalculus,level=3": {status: "READY"}

"math:model=default,subject=number_theory,level=4": {status: "READY"}
"math:model=default,subject=intermediate_algebra,level=4": {status: "READY"}
"math:model=default,subject=algebra,level=4": {status: "READY"}
"math:model=default,subject=prealgebra,level=4": {status: "READY"}
"math:model=default,subject=geometry,level=4": {status: "READY"}
"math:model=default,subject=counting_and_probability,level=4": {status: "READY"}
"math:model=default,subject=precalculus,level=4": {status: "READY"}

"math:model=default,subject=number_theory,level=5": {status: "READY"}
"math:model=default,subject=intermediate_algebra,level=5": {status: "READY"}
"math:model=default,subject=algebra,level=5": {status: "READY"}
"math:model=default,subject=prealgebra,level=5": {status: "READY"}
"math:model=default,subject=geometry,level=5": {status: "READY"}
"math:model=default,subject=counting_and_probability,level=5": {status: "READY"}
"math:model=default,subject=precalculus,level=5": {status: "READY"}

"numeracy:model=default,run_solver=True,relation_type=linear,mode=function": {status: "READY"}
"numeracy:model=default,run_solver=True,relation_type=plane,mode=function": {status: "READY"}
# The DistanceMetric is slow to compute for relation_type 'parabola' and 'paraboloid', so set run_solver=False
"numeracy:model=default,run_solver=False,relation_type=parabola,mode=function": {status: "READY"}
# TODO: gets stuck at: trial 0: evaluate 54 (total 200)
#       https://github.com/stanford-crfm/benchmarking/issues/348
# "numeracy:model=default,run_solver=False,relation_type=paraboloid,mode=function": {status: "READY"}

"synthetic_reasoning:model=default,mode=pattern_match": {status: "READY"}
"synthetic_reasoning:model=default,mode=variable_substitution": {status: "READY"}
"synthetic_reasoning:model=default,mode=induction": {status: "READY"}

"synthetic_reasoning_natural:model=default,difficulty=easy": {status: "READY"}
"synthetic_reasoning_natural:model=default,difficulty=hard": {status: "READY"}


"gsm:model=default,num_train_trials=default": {status: "READY"}

# TODO: @Dor @Tony W. @Percy - Decide on BABI tasks.
"babi_qa:model=default,task=1": {status: "READY"}
"babi_qa:model=default,task=2": {status: "READY"}
"babi_qa:model=default,task=3": {status: "READY"}
"babi_qa:model=default,task=4": {status: "READY"}
"babi_qa:model=default,task=5": {status: "READY"}
"babi_qa:model=default,task=6": {status: "READY"}
"babi_qa:model=default,task=7": {status: "READY"}
"babi_qa:model=default,task=8": {status: "READY"}
"babi_qa:model=default,task=9": {status: "READY"}
"babi_qa:model=default,task=10": {status: "READY"}
"babi_qa:model=default,task=11": {status: "READY"}
"babi_qa:model=default,task=12": {status: "READY"}
"babi_qa:model=default,task=13": {status: "READY"}
"babi_qa:model=default,task=14": {status: "READY"}
"babi_qa:model=default,task=15": {status: "READY"}
"babi_qa:model=default,task=16": {status: "READY"}
"babi_qa:model=default,task=17": {status: "READY"}
"babi_qa:model=default,task=18": {status: "READY"}
"babi_qa:model=default,task=19": {status: "READY"}
"babi_qa:model=default,task=20": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=1": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=2": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=3": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=4": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=5": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=6": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=7": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=8": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=9": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=10": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=11": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=12": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=13": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=14": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=15": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=16": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=17": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=18": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=19": {status: "READY"}
"babi_qa:model=anthropic/stanford-online-helpful-v4-s3,task=20": {status: "READY"}

"legal_support:model=default": {status: "READY"}

"entity_matching:model=default,dataset=Beer": {status: "READY"}
"entity_matching:model=default,dataset=Abt_Buy": {status: "READY"}
"entity_matching:model=default,dataset=Dirty_iTunes_Amazon": {status: "READY"}

"entity_data_imputation:model=default,dataset=Buy": {status: "READY"}
"entity_data_imputation:model=default,dataset=Restaurant": {status: "READY"}

"code:model=code,dataset=HumanEval": {status: "READY"}
"code:model=code,dataset=APPS": {status: "READY"}

"lsat_qa:model=default,num_train_trials=default,task=all": {status: "READY"}
"lsat_qa:model=default,num_train_trials=default,task=grouping": {status: "READY"}
"lsat_qa:model=default,num_train_trials=default,task=ordering": {status: "READY"}
"lsat_qa:model=default,num_train_trials=default,task=assignment": {status: "READY"}
"lsat_qa:model=default,num_train_trials=default,task=miscellaneous": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"lsat_qa:model=limited_functionality,num_train_trials=default,task=all": {status: "READY"}
"lsat_qa:model=limited_functionality,num_train_trials=default,task=grouping": {status: "READY"}
"lsat_qa:model=limited_functionality,num_train_trials=default,task=ordering": {status: "READY"}
"lsat_qa:model=limited_functionality,num_train_trials=default,task=assignment": {status: "READY"}
"lsat_qa:model=limited_functionality,num_train_trials=default,task=miscellaneous": {status: "READY"}

"dyck_language:model=default,num_parenthesis_pairs=2": {status: "READY"}
"dyck_language:model=default,num_parenthesis_pairs=3": {status: "READY"}
"dyck_language:model=default,num_parenthesis_pairs=4": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"dyck_language:model=anthropic/stanford-online-helpful-v4-s3,num_parenthesis_pairs=2": {status: "READY"}
"dyck_language:model=anthropic/stanford-online-helpful-v4-s3,num_parenthesis_pairs=3": {status: "READY"}
"dyck_language:model=anthropic/stanford-online-helpful-v4-s3,num_parenthesis_pairs=4": {status: "READY"}

##### Individual fairness

"imdb:data_augmentation=gender_pronouns_deterministic": {status: "READY"}
"imdb:data_augmentation=gender_terms_deterministic": {status: "READY"}
"imdb:data_augmentation=person_name_first_deterministic": {status: "READY"}
"imdb:data_augmentation=person_name_last_deterministic": {status: "READY"}
"imdb:data_augmentation=dialect_easy": {status: "READY"}
"imdb:data_augmentation=dialect_medium": {status: "READY"}
"imdb:data_augmentation=dialect_hard": {status: "READY"}
"imdb:data_augmentation=dialect_deterministic": {status: "READY"}

##### Harms #####

# Large and small GPT-3 on randomly sampled instances from the original BooksCorpus.
# We expect data here to be less repeated in the pretraining corpus. This approximates the average case.
"copyright:model=openai/ada,datatag=n_books_1000-extractions_per_book_1-prefix_length_125": {status: "READY"}
"copyright:model=openai/davinci,datatag=n_books_1000-extractions_per_book_1-prefix_length_125": {status: "READY"}

# Large and small GPT-3 on popular books.
# We expect data here to be repeated more in the pretraining corpus. This approximates the worst case.
"copyright:model=openai/ada,datatag=popular_books-prefix_length_125.json": {status: "READY"}
"copyright:model=openai/davinci,datatag=popular_books-prefix_length_125.json": {status: "READY"}

# Small codex model.
"copyright:model=openai/code-cushman-001,datatag=prompt_num_line_1-min_lines_20.json": {status: "READY"}
"copyright:model=openai/code-cushman-001,datatag=prompt_num_line_5-min_lines_20.json": {status: "READY"}
"copyright:model=openai/code-cushman-001,datatag=prompt_num_line_10-min_lines_20.json": {status: "READY"}

# Large codex model.
"copyright:model=openai/code-davinci-001,datatag=prompt_num_line_1-min_lines_20.json": {status: "READY"}
"copyright:model=openai/code-davinci-001,datatag=prompt_num_line_5-min_lines_20.json": {status: "READY"}
"copyright:model=openai/code-davinci-001,datatag=prompt_num_line_10-min_lines_20.json": {status: "READY"}

"disinformation:model=default,capability=reiteration,topic=covid": {status: "READY"}
"disinformation:model=default,capability=reiteration,topic=climate": {status: "READY"}
"disinformation:model=default,capability=wedging": {status: "READY"}

# TODO: @Ryan @Rishi - Decide which of the below runs to keep.
"bbq:model=default,subject=all": {status: "READY"}
"bbq:model=default,subject=age": {status: "READY"}
"bbq:model=default,subject=disability_status": {status: "READY"}
"bbq:model=default,subject=gender_identity": {status: "READY"}
"bbq:model=default,subject=nationality": {status: "READY"}
"bbq:model=default,subject=physical_appearance": {status: "READY"}
"bbq:model=default,subject=race_ethnicity": {status: "READY"}
"bbq:model=default,subject=race_x_SES": {status: "READY"}
"bbq:model=default,subject=race_x_gender": {status: "READY"}
"bbq:model=default,subject=religion": {status: "READY"}
"bbq:model=default,subject=SES": {status: "READY"}
"bbq:model=default,subject=sexual_orientation": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"bbq:model=limited_functionality,subject=all": {status: "READY"}
"bbq:model=limited_functionality,subject=age": {status: "READY"}
"bbq:model=limited_functionality,subject=disability_status": {status: "READY"}
"bbq:model=limited_functionality,subject=gender_identity": {status: "READY"}
"bbq:model=limited_functionality,subject=nationality": {status: "READY"}
"bbq:model=limited_functionality,subject=physical_appearance": {status: "READY"}
"bbq:model=limited_functionality,subject=race_ethnicity": {status: "READY"}
"bbq:model=limited_functionality,subject=race_x_SES": {status: "READY"}
"bbq:model=limited_functionality,subject=race_x_gender": {status: "READY"}
"bbq:model=limited_functionality,subject=religion": {status: "READY"}
"bbq:model=limited_functionality,subject=SES": {status: "READY"}
"bbq:model=limited_functionality,subject=sexual_orientation": {status: "READY"}

"bold:model=default,subject=all": {status: "READY"}
"bold:model=default,subject=gender": {status: "READY"}
"bold:model=default,subject=political_ideology": {status: "READY"}
"bold:model=default,subject=profession": {status: "READY"}
"bold:model=default,subject=race": {status: "READY"}
"bold:model=default,subject=religious_ideology": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"bold:model=anthropic/stanford-online-helpful-v4-s3,subject=all": {status: "READY"}
"bold:model=anthropic/stanford-online-helpful-v4-s3,subject=gender": {status: "READY"}
"bold:model=anthropic/stanford-online-helpful-v4-s3,subject=political_ideology": {status: "READY"}
"bold:model=anthropic/stanford-online-helpful-v4-s3,subject=profession": {status: "READY"}
"bold:model=anthropic/stanford-online-helpful-v4-s3,subject=race": {status: "READY"}
"bold:model=anthropic/stanford-online-helpful-v4-s3,subject=religious_ideology": {status: "READY"}

"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=all": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=asian": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=atheist": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=bisexual": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=black": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=buddhist": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=christian": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=female": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=heterosexual": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=hindu": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=homosexual_gay_or_lesbian": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=intellectual_or_learning_disability": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=jewish": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=latino": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=male": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=muslim": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=other_disability": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=other_gender": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=other_race_or_ethnicity": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=other_religion": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=other_sexual_orientation": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=physical_disability": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=psychiatric_or_mental_illness": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=transgender": {status: "READY"}
"civil_comments:model=default,data_path=/u/scr/nlp/crfm/benchmarking/civil_comments/,subject=white": {status: "READY"}

# TODO: run on all default models once the following issue is resolved:
#       https://github.com/stanford-crfm/benchmarking/issues/286
"real_toxicity_prompts:model=default": {status: "READY"}


"truthful_qa:model=default,task=mc_single": {status: "READY"}

# `num_outputs` has to be 1 for the Anthropic and MT-NLG models
"truthful_qa:model=limited_functionality,task=mc_single": {status: "READY"}


# TODO: @Dilara - Implement generative version based on classification results.


##### Robustness #####
"boolq:model=default,data_augmentation=all": {status: "READY"}
# "boolq:model=limited_functionality,data_augmentation=all": {status: "READY"}
"boolq:model=default,only_contrast=True,data_augmentation=contrast_sets": {status: "READY"}
# "boolq:model=limited_functionality,only_contrast=True,data_augmentation=contrast_sets": {status: "READY"}
"natural_qa:model=default,mode=closedbook,data_augmentation=all": {status: "READY"}
# "natural_qa:model=limited_functionality,mode=closedbook,data_augmentation=all": {status: "READY"}
"news_qa:model=openai/davinci,data_augmentation=all": {status: "READY"}
"news_qa:model=openai/curie,data_augmentation=all": {status: "READY"}
"news_qa:model=openai/text-davinci-001,data_augmentation=all": {status: "READY"}
"news_qa:model=openai/text-curie-001,data_augmentation=all": {status: "READY"}
# TODO: Fix AI21 tokenizer step and change this back to 'default'
#       https://github.com/stanford-crfm/benchmarking/issues/349
# "news_qa:model=ai21/j1-jumbo,data_augmentation=all": {status: "READY"}
# "news_qa:model=limited_functionality,data_augmentation=all": {status: "READY"}
"raft:model=default,subset=one_stop_english,data_augmentation=all": {status: "READY"}
"imdb:model=default,only_contrast=True,data_augmentation=contrast_sets": {status: "READY"}
# "imdb:model=limited_functionality,only_contrast=True,data_augmentation=contrast_sets": {status: "READY"}


##### Interaction #####
