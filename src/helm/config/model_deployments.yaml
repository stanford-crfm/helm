# This file defines all the model deployments that are supported by the Helm API.
# Some models have several deployments, each with different parameters.

# If you want to add a new deployment, you can technically do it here but we recommend
# you to do it in prod_env/model_deployments.yaml instead.

# Follow the template of this file to add a new deployment. You can copy paste this to get started:
#    # This file defines all the model deployments that you do not want to be public.
#    model_deployments: [] # Leave empty to disable private model deployments


model_deployments:

  - name: simple/model1
    model_name: simple/model1
    tokenizer_name: simple/model1
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.simple_client.SimpleClient"
      args: {}

  # AI21 Labs

  # J1 models are Deprecated by AI21 Labs
  # API returns: Detail: Jurassic J1 models are deprecated
  - name: ai21/j1-jumbo
    deprecated: true
    model_name: ai21/j1-jumbo
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j1-large
    deprecated: true
    model_name: ai21/j1-large
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j1-grande
    deprecated: true
    model_name: ai21/j1-grande
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j1-grande-v2-beta
    deprecated: true
    model_name: ai21/j1-grande-v2-beta
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j2-jumbo
    model_name: ai21/j2-jumbo
    tokenizer_name: ai21/j1
    max_sequence_length: 6000
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_ai21_window_service.AI21Jurassic2JumboWindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j2-large
    model_name: ai21/j2-large
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j2-grande
    model_name: ai21/j2-grande
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}



  # Aleph Alpha
  - name: AlephAlpha/luminous-base
    model_name: AlephAlpha/luminous-base
    tokenizer_name: AlephAlpha/luminous-base
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.aleph_alpha_client.AlephAlphaClient"
      args: {}

  - name: AlephAlpha/luminous-extended
    model_name: AlephAlpha/luminous-extended
    tokenizer_name: AlephAlpha/luminous-extended
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.aleph_alpha_client.AlephAlphaClient"
      args: {}

  - name: AlephAlpha/luminous-supreme
    model_name: AlephAlpha/luminous-supreme
    tokenizer_name: AlephAlpha/luminous-supreme
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.aleph_alpha_client.AlephAlphaClient"
      args: {}

  # TODO: Add luminous-world once it is released.


 
  # Anthropic
  - name: anthropic/claude-v1.3
    model_name: anthropic/claude-v1.3
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}

  - name: anthropic/claude-instant-v1
    model_name: anthropic/claude-instant-v1
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}

  - name: anthropic/claude-instant-1.2
    model_name: anthropic/claude-instant-1.2
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}

  - name: anthropic/claude-2.0
    model_name: anthropic/claude-2.0
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}

  - name: anthropic/claude-2.1
    model_name: anthropic/claude-2.1
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}

  - name: anthropic/stanford-online-all-v4-s3
    deprecated: true # Closed model, not accessible via API
    model_name: anthropic/stanford-online-all-v4-s3
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicLegacyClient"
      args: {}

  # Cohere
  - name: cohere/xlarge-20220609
    model_name: cohere/xlarge-20220609
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/large-20220720
    model_name: cohere/large-20220720
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/medium-20220720
    model_name: cohere/medium-20220720
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/small-20220720
    model_name: cohere/small-20220720
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/xlarge-20221108
    model_name: cohere/xlarge-20221108
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/medium-20221108
    model_name: cohere/medium-20221108
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/command-medium-beta
    model_name: cohere/command-medium-beta
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019
    max_request_length: 2020
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}

  - name: cohere/command-xlarge-beta
    model_name: cohere/command-xlarge-beta
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019
    max_request_length: 2020
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}

  - name: cohere/command
    model_name: cohere/command
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019 # TODO: verify this
    max_request_length: 2020 # TODO: verify this
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}

  - name: cohere/command-light
    model_name: cohere/command-light
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019 # TODO: verify this
    max_request_length: 2020 # TODO: verify this
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}



  # Gooseai

  ## EleutherAI
  - name: gooseai/gpt-neo-20b
    model_name: eleutherai/gpt-neox-20b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.goose_ai_client.GooseAIClient"
      args: {}

  - name: gooseai/gpt-j-6b
    model_name: eleutherai/gpt-j-6b
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.goose_ai_client.GooseAIClient"
      args: {}



  # Google

  ## PaLM 2
  - name: google/text-bison@001
    model_name: google/text-bison@001
    tokenizer_name: google/text-bison@001
    max_sequence_length: 6000 # Officially 8192
    max_sequence_and_generated_tokens_length: 7000 # Officially 9216
    client_spec:
      class_name: "helm.proxy.clients.vertexai_client.VertexAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.no_decoding_window_service.NoDecodingWindowService"


  - name: google/text-bison-32k
    model_name: google/text-bison-32k
    tokenizer_name: google/mt5-base
    max_sequence_length: 32000
    max_sequence_and_generated_tokens_length: 32000
    client_spec:
      class_name: "helm.proxy.clients.vertexai_client.VertexAIClient"
      args: {}

  - name: google/text-unicorn@001
    model_name: google/text-unicorn@001
    tokenizer_name: google/text-unicorn@001
    max_sequence_length: 6000 # Officially 8192
    max_sequence_and_generated_tokens_length: 7000 # Officially 9216
    client_spec:
      class_name: "helm.proxy.clients.vertexai_client.VertexAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.no_decoding_window_service.NoDecodingWindowService"

  - name: google/code-bison@001
    model_name: google/code-bison@001
    tokenizer_name: google/mt5-base
    max_sequence_length: 6000 # Officially 6144
    max_sequence_and_generated_tokens_length: 7000 # Officially 7168
    client_spec:
      class_name: "helm.proxy.clients.vertexai_client.VertexAIClient"
      args: {}

  - name: google/code-bison-32k
    model_name: google/code-bison-32k
    tokenizer_name: google/mt5-base
    max_sequence_length: 32000
    max_sequence_and_generated_tokens_length: 32000
    client_spec:
      class_name: "helm.proxy.clients.vertexai_client.VertexAIClient"
      args: {}



  # HuggingFace

  ## Bigcode
  - name: huggingface/santacoder
    model_name: bigcode/santacoder
    tokenizer_name: bigcode/santacoder
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}

  - name: huggingface/starcoder
    model_name: bigcode/starcoder
    tokenizer_name: bigcode/starcoder
    max_sequence_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}

  ## EleutherAI
  - name: huggingface/gpt-j-6b
    model_name: eleutherai/gpt-j-6b
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}

  ## OpenAI
  - name: huggingface/gpt2
    model_name: openai/gpt2
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 1024
    max_request_length: 1025
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}

  # HuggingFaceM4
  - name: HuggingFaceM4/idefics-9b
    model_name: HuggingFaceM4/idefics-9b
    tokenizer_name: HuggingFaceM4/idefics-9b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}

  - name: HuggingFaceM4/idefics-9b-instruct
    model_name: HuggingFaceM4/idefics-9b-instruct
    tokenizer_name: HuggingFaceM4/idefics-9b-instruct
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}

  - name: HuggingFaceM4/idefics-80b
    model_name: HuggingFaceM4/idefics-80b
    tokenizer_name: HuggingFaceM4/idefics-80b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}

  - name: HuggingFaceM4/idefics-80b-instruct
    model_name: HuggingFaceM4/idefics-80b-instruct
    tokenizer_name: HuggingFaceM4/idefics-80b-instruct
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}



  # Lighting AI
  - name: lightningai/lit-gpt
    model_name: lightningai/lit-gpt
    tokenizer_name: lightningai/lit-gpt
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.lit_gpt_client.LitGPTClient"
      args:
        checkpoint_dir: "" # Path to the checkpoint directory
        precision: bf16-true



  # Microsoft
  - name: microsoft/TNLGv2_530B
    model_name: microsoft/TNLGv2_530B
    tokenizer_name: microsoft/gpt2
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.microsoft_client.MicrosoftClient"
      args: {}

  - name: microsoft/TNLGv2_7B
    model_name: microsoft/TNLGv2_7B
    tokenizer_name: microsoft/gpt2
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.microsoft_client.MicrosoftClient"
      args: {}



  # Neurips
  - name: neurips/local
    model_name: neurips/local
    tokenizer_name: neurips/local
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.http_model_client.HTTPModelClient"
      args: {}



  # Nvidia
  - name: nvidia/megatron-gpt2
    model_name: nvidia/megatron-gpt2
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 1024
    client_spec:
      class_name: "helm.proxy.clients.megatron_client.MegatronClient"
      args: {}



  # OpenAI

  ## GPT 3 Models
  # The list of models can be found here: https://beta.openai.com/docs/engines/gpt-3
  # DEPRECATED: Announced on July 06 2023 that these models will be shut down on January 04 2024.
 
  - name: openai/davinci
    deprecated: true
    model_name: openai/davinci
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/curie
    deprecated: true
    model_name: openai/curie
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/babbage
    deprecated: true
    model_name: openai/babbage
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/ada
    deprecated: true
    model_name: openai/ada
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-davinci-003
    deprecated: true
    model_name: openai/text-davinci-003
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-davinci-002
    deprecated: true
    model_name: openai/text-davinci-002
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-davinci-001
    deprecated: true
    model_name: openai/text-davinci-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-curie-001
    deprecated: true
    model_name: openai/text-curie-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-babbage-001
    deprecated: true
    model_name: openai/text-babbage-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-ada-001
    deprecated: true
    model_name: openai/text-ada-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}


  ## GPT 3.5 Turbo Models
  # ChatGPT: https://openai.com/blog/chatgpt

  # The claimed sequence length is 4096, but as of 2023-03-07, the empirical usable
  # sequence length is smaller at 4087 with one user input message and one assistant
  # output message because ChatGPT uses special tokens for message roles and boundaries.
  # We use a rounded-down sequence length of 4000 to account for these special tokens.
  - name: openai/gpt-3.5-turbo-0301
    model_name: openai/gpt-3.5-turbo-0301
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  # The claimed sequence length is 4096, but as of 2023-03-07, the empirical usable
  # sequence length is smaller at 4087 with one user input message and one assistant
  # output message because ChatGPT uses special tokens for message roles and boundaries.
  # We use a rounded-down sequence length of 4000 to account for these special tokens.
  - name: openai/gpt-3.5-turbo-0613
    model_name: openai/gpt-3.5-turbo-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  # Claimed length is 16,384; we round down to 16,000 for the same reasons as explained
  # in the openai/gpt-3.5-turbo-0613 comment
  - name: openai/gpt-3.5-turbo-16k-0613
    model_name: openai/gpt-3.5-turbo-16k-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 16000
    max_request_length: 16001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}


  ## GPT 4 Models

  - name: openai/gpt-4-1106-preview
    model_name: openai/gpt-4-1106-preview
    tokenizer_name: openai/cl100k_base
    # According to https://help.openai.com/en/articles/8555510-gpt-4-turbo,
    # the maximum number of output tokens for this model is 4096
    # TODO: add max_generated_tokens_length of 4096 https://github.com/stanford-crfm/helm/issues/2098
    max_sequence_length: 128000
    max_request_length: 128001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/gpt-4-0314
    model_name: openai/gpt-4-0314
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 8192
    max_request_length: 8193
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/gpt-4-32k-0314
    model_name: openai/gpt-4-32k-0314
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 32768
    max_request_length: 32769
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/gpt-4-0613
    model_name: openai/gpt-4-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 8192
    max_request_length: 8193
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/gpt-4-32k-0613
    model_name: openai/gpt-4-32k-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 32768
    max_request_length: 32769
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}


  ## Codex Models
  # DEPRECATED: Codex models have been shut down on March 23 2023.

  - name: openai/code-davinci-002
    deprecated: true
    model_name: openai/code-davinci-002
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/code-davinci-001
    deprecated: true
    model_name: openai/code-davinci-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/code-cushman-001
    deprecated: true
    model_name: openai/code-cushman-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

 
  ## Text Similarity Models
  # OpenAI similarity embedding models: https://beta.openai.com/docs/guides/embeddings
  # The number of parameters is guessed based on the number of parameters of the
  # corresponding GPT-3 model.
  # DEPRECATED: Announced on July 06 2023 that first generation embeddings models
  #  will be shut down on January 04 2024.

  - name: openai/text-similarity-davinci-001
    deprecated: true
    model_name: openai/text-similarity-davinci-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-similarity-curie-001
    deprecated: true
    model_name: openai/text-similarity-curie-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-similarity-babbage-001
    deprecated: true
    model_name: openai/text-similarity-babbage-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  - name: openai/text-similarity-ada-001
    deprecated: true
    model_name: openai/text-similarity-ada-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}

  # As of 2023-11-07, text-embedding-ada-002 is not deprecated:
  # "We recommend using text-embedding-ada-002 for nearly all use cases."
  # Source: https://platform.openai.com/docs/guides/embeddings/what-are-embeddings
  - name: openai/text-embedding-ada-002
    model_name: openai/text-embedding-ada-002
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}



  # Together
  # The list of models served by Together changes often, to check the latest list, visit:
  # https://docs.together.ai/docs/inference-models
  # You can also check the playground to check that the live models are working:
  # https://api.together.xyz/playground

  ## BigScience
  - name: together/bloom
    deprecated: true # Removed from together
    model_name: bigscience/bloom
    tokenizer_name: bigscience/bloom
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/t0pp
    deprecated: true # Removed from together
    model_name: bigscience/t0pp
    tokenizer_name: bigscience/T0pp
    max_sequence_length: 1024
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.t0pp_window_service.T0ppWindowService"
      args: {}

  ## Databricks
  - name: together/dolly-v2-3b
    model_name: databricks/dolly-v2-3b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/dolly-v2-7b
    model_name: databricks/dolly-v2-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/dolly-v2-12b
    model_name: databricks/dolly-v2-12b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## EleutherAI
  - name: together/gpt-j-6b
    deprecated: true # Removed from together
    model_name: eleutherai/gpt-j-6b
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/gpt-neox-20b
    deprecated: true # Removed from together
    model_name: eleutherai/gpt-neox-20b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/pythia-1b-v0
    model_name: eleutherai/pythia-1b-v0
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/pythia-2.8b-v0
    model_name: eleutherai/pythia-2.8b-v0
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/pythia-6.9b
    model_name: eleutherai/pythia-6.9b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/pythia-12b-v0
    model_name: eleutherai/pythia-12b-v0
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## Google
  - name: together/t5-11b
    deprecated: true # Removed from together
    model_name: google/t5-11b
    tokenizer_name: google/t5-11b
    max_sequence_length: 511
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.t511b_window_service.T511bWindowService"
      args: {}

  - name: together/flan-t5-xxl
    deprecated: true # Removed from together
    model_name: google/flan-t5-xxl
    tokenizer_name: google/flan-t5-xxl
    max_sequence_length: 511
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.flan_t5_window_service.FlanT5WindowService"
      args: {}

  - name: together/ul2
    deprecated: true # Removed from together
    model_name: google/ul2
    tokenizer_name: google/ul2
    max_sequence_length: 511
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ul2_window_service.UL2WindowService"
      args: {}

  ## HazyResearch
  - name: together/h3-2.7b
    deprecated: true # Not available on Together yet
    model_name: hazyresearch/h3-2.7b
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 1024
    max_request_length: 1025
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## LMSYS
  # TODO: might be deprecated. Needs to be checked.
  # Together officialy supports vicuna 1.5, not sure if 1.3 is still supported.
  - name: together/vicuna-7b-v1.3
    model_name: lmsys/vicuna-7b-v1.3
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/vicuna-13b-v1.3
    model_name: lmsys/vicuna-13b-v1.3
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## Meta
  - name: together/llama-7b
    model_name: meta/llama-7b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/llama-13b
    model_name: meta/llama-13b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/llama-30b
    model_name: meta/llama-30b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/llama-65b
    model_name: meta/llama-65b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2047  # Subtract 1 tokens to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/llama-2-7b
    model_name: meta/llama-2-7b
    tokenizer_name: meta-llama/Llama-2-7b-hf
    max_sequence_length: 4094  # Subtract 2 tokens to work around a off-by-two bug in Together's token counting (#2080 and #2094)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/llama-2-13b
    model_name: meta/llama-2-13b
    tokenizer_name: meta-llama/Llama-2-7b-hf
    max_sequence_length: 4094  # Subtract 2 tokens to work around a off-by-two bug in Together's token counting (#2080 and #2094)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/llama-2-70b
    model_name: meta/llama-2-70b
    tokenizer_name: meta-llama/Llama-2-7b-hf
    max_sequence_length: 4094  # Subtract 2 tokens to work around a off-by-two bug in Together's token counting (#2080 and #2094)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/opt-175b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-175b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/opt-66b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-66b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/opt-6.7b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-6.7b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/opt-1.3b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-1.3b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  # 01.AI
  - name: together/yi-6b
    model_name: 01-ai/yi-6b
    tokenizer_name: 01-ai/Yi-6B
    max_sequence_length: 4095
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/yi-34b
    model_name: 01-ai/yi-34b
    tokenizer_name: 01-ai/Yi-6B
    max_sequence_length: 4095
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## MistralAI
  - name: together/mistral-7b-v0.1
    model_name: mistralai/mistral-7b-v0.1
    tokenizer_name: mistralai/Mistral-7B-v0.1
    max_sequence_length: 4095  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/mixtral-8x7b-32kseqlen
    model_name: mistralai/mixtral-8x7b-32kseqlen
    tokenizer_name: mistralai/Mistral-7B-v0.1
    max_sequence_length: 4095  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## MosaicML
  - name: together/mpt-7b
    deprecated: true # Not available on Together yet
    model_name: mosaicml/mpt-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/mpt-instruct-7b
    deprecated: true # Not available on Together yet
    model_name: mosaicml/mpt-instruct-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/mpt-30b
    model_name: mosaicml/mpt-30b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/mpt-instruct-30b
    model_name: mosaicml/mpt-instruct-30b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## StabilityAI
  - name: together/stablelm-base-alpha-3b
    deprecated: true # Removed from together
    model_name: stabilityai/stablelm-base-alpha-3b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 4096
    max_request_length: 4097
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/stablelm-base-alpha-7b
    deprecated: true # Removed from together
    model_name: stabilityai/stablelm-base-alpha-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 4096
    max_request_length: 4097
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## Stanford
  - name: together/alpaca-7b
    model_name: stanford/alpaca-7b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## Tiiuae
  - name: together/falcon-7b
    model_name: tiiuae/falcon-7b
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/falcon-7b-instruct
    model_name: tiiuae/falcon-7b-instruct
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/falcon-40b
    model_name: tiiuae/falcon-40b
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/falcon-40b-instruct
    model_name: tiiuae/falcon-40b-instruct
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2047  # Subtract 1 token to work around a off-by-one bug in Together's input validation token counting (#2080)
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## Together
  # These are models fine-tuned by Together (and not simply hosted by Together).
  - name: together/gpt-jt-6b-v1
    model_name: together/gpt-jt-6b-v1
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/gpt-neoxt-chat-base-20b
    model_name: together/gpt-neoxt-chat-base-20b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/redpajama-incite-base-3b-v1
    model_name: together/redpajama-incite-base-3b-v1
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/redpajama-incite-instruct-3b-v1
    model_name: together/redpajama-incite-instruct-3b-v1
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/redpajama-incite-base-7b
    model_name: together/redpajama-incite-base-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  - name: together/redpajama-incite-instruct-7b
    model_name: together/redpajama-incite-instruct-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}

  ## Tsinghua
  - name: together/glm
    deprecated: true # Not available on Together yet
    model_name: tsinghua/glm
    tokenizer_name: TsinghuaKEG/ice
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ice_window_service.ICEWindowService"
      args: {}

  ## Yandex
  - name: together/yalm
    deprecated: true # Not available on Together yet
    model_name: yandex/yalm
    tokenizer_name: Yandex/yalm
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.yalm_window_service.YaLMWindowService"
      args: {}



  # Writer
  - name: writer/palmyra-base
    model_name: writer/palmyra-base
    tokenizer_name: writer/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-large
    model_name: writer/palmyra-large
    tokenizer_name: writer/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-instruct-30
    model_name: writer/palmyra-instruct-30
    tokenizer_name: writer/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-e
    model_name: writer/palmyra-e
    tokenizer_name: writer/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/silk-road
    model_name: writer/silk-road
    tokenizer_name: writer/gpt2
    max_sequence_length: 8192
    max_sequence_and_generated_tokens_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-x
    model_name: writer/palmyra-x
    tokenizer_name: writer/gpt2
    max_sequence_length: 8192
    max_sequence_and_generated_tokens_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-x-v2
    model_name: writer/palmyra-x-v2
    tokenizer_name: writer/gpt2
    max_sequence_length: 6000
    max_sequence_and_generated_tokens_length: 7024
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-x-v3
    model_name: writer/palmyra-x-v3
    tokenizer_name: writer/gpt2
    max_sequence_length: 6000
    max_sequence_and_generated_tokens_length: 7024
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}

  - name: writer/palmyra-x-32k
    model_name: writer/palmyra-x-32k
    tokenizer_name: writer/gpt2
    max_sequence_length: 28000
    max_sequence_and_generated_tokens_length: 30048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
