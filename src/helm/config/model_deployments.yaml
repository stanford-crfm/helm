# This file defines all the model deployments that are supported by the Helm API.
# Some models have several deployments, each with different parameters.

# If you want to add a new deployment, you can technically do it here but we recommend
# you to do it in private/model_deployments.yaml instead.

model_deployments:

  - name: simple/model1
    model_name: simple/model1
    tokenizer_name: simple/model1
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.simple_client.SimpleClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  # ========== AI21 Labs ========== #

  # J1 models are Deprecated by AI21 Labs
  # API returns: Detail: Jurassic J1 models are deprecated
  - name: ai21/j1-jumbo
    deprecated: true
    model_name: ai21/j1-jumbo
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j1-large
    deprecated: true
    model_name: ai21/j1-large
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j1-grande 
    deprecated: true
    model_name: ai21/j1-grande
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j1-grande-v2-beta 
    deprecated: true
    model_name: ai21/j1-grande-v2-beta
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j2-jumbo
    model_name: ai21/j2-jumbo
    tokenizer_name: ai21/j1
    max_sequence_length: 6000
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_ai21_window_service.AI21Jurassic2JumboWindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j2-large
    model_name: ai21/j2-large
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}

  - name: ai21/j2-grande
    model_name: ai21/j2-grande
    tokenizer_name: ai21/j1
    max_sequence_length: 2047
    client_spec:
      class_name: "helm.proxy.clients.ai21_client.AI21Client"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ai21_window_service.AI21WindowService"
      args:
        gpt2_window_service:
          class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
          args: {}
  # =============================== #



  # ========== Aleph Alpha ========== #
  - name: AlephAlpha/luminous-base
    model_name: AlephAlpha/luminous-base
    tokenizer_name: AlephAlpha/luminous-base
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.aleph_alpha_client.AlephAlphaClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.luminous_window_service.LuminousBaseWindowService"
      args: {}

  - name: AlephAlpha/luminous-extended
    model_name: AlephAlpha/luminous-extended
    tokenizer_name: AlephAlpha/luminous-extended
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.aleph_alpha_client.AlephAlphaClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.luminous_window_service.LuminousExtendedWindowService"
      args: {}

  - name: AlephAlpha/luminous-supreme
    model_name: AlephAlpha/luminous-supreme
    tokenizer_name: AlephAlpha/luminous-supreme
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.aleph_alpha_client.AlephAlphaClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.luminous_window_service.LuminousSupremeWindowService"
      args: {}

  # TODO: Add luminous-world once it is released.
  # ================================= #


  
  # =========== Anthropic =========== #
  - name: anthropic/claude-v1.3
    model_name: anthropic/claude-v1.3
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.anthropic_window_service.AnthropicWindowService"
      args: {}

  - name: anthropic/claude-instant-v1
    model_name: anthropic/claude-instant-v1
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.anthropic_window_service.AnthropicWindowService"
      args: {}

  - name: anthropic/claude-2.0
    model_name: anthropic/claude-2.0
    tokenizer_name: anthropic/claude
    max_sequence_length: 8000
    max_sequence_and_generated_tokens_length: 9016
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.anthropic_window_service.AnthropicWindowService"
      args: {}

  - name: anthropic/stanford-online-all-v4-s3
    deprecated: true # Not longer served: "Connexion to remote was lost."
    model_name: anthropic/stanford-online-all-v4-s3
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.anthropic_client.AnthropicLegacyClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.anthropic_window_service.LegacyAnthropicWindowService"
      args: {}
  # ================================= #



  # =========== Cohere =========== #
  - name: cohere/xlarge-20220609
    model_name: cohere/xlarge-20220609
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/large-20220720
    model_name: cohere/large-20220720
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/medium-20220720
    model_name: cohere/medium-20220720
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/small-20220720
    model_name: cohere/small-20220720
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/xlarge-20221108
    model_name: cohere/xlarge-20221108
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/medium-20221108
    model_name: cohere/medium-20221108
    tokenizer_name: cohere/cohere
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereWindowService"
      args: {}

  - name: cohere/command-medium-beta
    model_name: cohere/command-medium-beta
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019
    max_request_length: 2020
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}

  - name: cohere/command-xlarge-beta
    model_name: cohere/command-xlarge-beta
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019
    max_request_length: 2020
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}

  - name: cohere/command
    model_name: cohere/command
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019 # TODO: verify this
    max_request_length: 2020 # TODO: verify this
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}

  - name: cohere/command-light
    model_name: cohere/command-light
    tokenizer_name: cohere/cohere
    max_sequence_length: 2019 # TODO: verify this
    max_request_length: 2020 # TODO: verify this
    client_spec:
      class_name: "helm.proxy.clients.cohere_client.CohereClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.cohere_window_service.CohereCommandWindowService"
      args: {}
  # ============================== #



  # =========== Gooseai =========== #

  # ---------- EleutherAI ---------- #
  - name: gooseai/gpt-neo-20b
    model_name: eleutherai/gpt-neox-20b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.goose_ai_client.GooseAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: gooseai/gpt-j-6b
    model_name: eleutherai/gpt-j-6b
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.goose_ai_client.GooseAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptj_window_service.GPTJWindowService"
      args: {}
  # =============================== #



  # =========== HuggingFace =========== #

  # ---------- Bigcode ---------- #
  - name: huggingface/santacoder
    model_name: bigcode/santacoder
    tokenizer_name: bigcode/santacoder
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.santacoder_window_service.SantaCoderWindowService"
      args: {}

  - name: huggingface/starcoder
    model_name: bigcode/starcoder
    tokenizer_name: bigcode/starcoder
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.starcoder_window_service.StarCoderWindowService"
      args: {}

  # ---------- EleutherAI ---------- #
  - name: huggingface/gpt-j-6b
    model_name: eleutherai/gpt-j-6b
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptj_window_service.GPTJWindowService"
      args: {}

  # ---------- OpenAI  ---------- #
  - name: huggingface/gpt2
    model_name: openai/gpt2
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 1024
    max_request_length: 1025
    client_spec:
      class_name: "helm.proxy.clients.huggingface_client.HuggingFaceClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
      args: {}
  # =================================== #



  # ========== HuggingFaceM4 ========== #
  - name: HuggingFaceM4/idefics-9b
    model_name: huggingface/idefics-9b
    tokenizer_name: HuggingFaceM4/idefics-9b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  - name: HuggingFaceM4/idefics-9b-instruct
    model_name: huggingface/idefics-9b-instruct
    tokenizer_name: HuggingFaceM4/idefics-9b-instruct
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  - name: HuggingFaceM4/idefics-80b
    model_name: huggingface/idefics-80b
    tokenizer_name: HuggingFaceM4/idefics-80b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  - name: HuggingFaceM4/idefics-80b-instruct
    model_name: huggingface/idefics-80b-instruct
    tokenizer_name: HuggingFaceM4/idefics-80b-instruct
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.vision_language.idefics_client.IDEFICSClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}
  # =================================== #



  # ========== Microsoft ========== #
  - name: microsoft/TNLGv2_530B
    model_name: microsoft/TNLGv2_530B
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.microsoft_client.MicrosoftClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.mt_nlg_window_service.MTNLGWindowService"
      args: {}

  - name: microsoft/TNLGv2_7B
    model_name: microsoft/TNLGv2_7B
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2047
    max_request_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.microsoft_client.MicrosoftClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.mt_nlg_window_service.MTNLGWindowService"
      args: {}
  # =============================== #



  # ========== Nvidia ========== #
  - name: nvidia/megatron-gpt2
    model_name: nvidia/megatron-gpt2
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 1024
    client_spec:
      class_name: "helm.proxy.clients.megatron_client.MegatronClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.megatron_window_service.MegatronWindowService"
      args: {}
  # ============================ #



  # ========== OpenAI ========== #

  # ----- GPT 3 Models ----- #
  # The list of models can be found here: https://beta.openai.com/docs/engines/gpt-3
  # DEPRECATED: Announced on July 06 2023 that these models will be shut down on January 04 2024.
  
  - name: openai/davinci 
    deprecated: true
    model_name: openai/davinci
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/curie 
    deprecated: true
    model_name: openai/curie
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/babbage 
    deprecated: true
    model_name: openai/babbage
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/ada 
    deprecated: true
    model_name: openai/ada
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-davinci-003 
    deprecated: true
    model_name: openai/text-davinci-003
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.WiderOpenAIWindowService"
      args: {}

  - name: openai/text-davinci-002 
    deprecated: true
    model_name: openai/text-davinci-002
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.WiderOpenAIWindowService"
      args: {}

  - name: openai/text-davinci-001 
    deprecated: true
    model_name: openai/text-davinci-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-curie-001 
    deprecated: true
    model_name: openai/text-curie-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-babbage-001 
    deprecated: true
    model_name: openai/text-babbage-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-ada-001 
    deprecated: true
    model_name: openai/text-ada-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}


  # ----- GPT 3.5 Turbo Models ----- #
  # ChatGPT: https://openai.com/blog/chatgpt

  # The claimed sequence length is 4096, but as of 2023-03-07, the empirical usable
  # sequence length is smaller at 4087 with one user input message and one assistant
  # output message because ChatGPT uses special tokens for message roles and boundaries.
  # We use a rounded-down sequence length of 4000 to account for these special tokens.
  - name: openai/gpt-3.5-turbo-0301
    model_name: openai/gpt-3.5-turbo-0301
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPTTurboWindowService"
      args: {}

  # The claimed sequence length is 4096, but as of 2023-03-07, the empirical usable
  # sequence length is smaller at 4087 with one user input message and one assistant
  # output message because ChatGPT uses special tokens for message roles and boundaries.
  # We use a rounded-down sequence length of 4000 to account for these special tokens.
  - name: openai/gpt-3.5-turbo-0613
    model_name: openai/gpt-3.5-turbo-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPTTurboWindowService"
      args: {}

  # Claimed length is 16,384; we round down to 16,000 for the same reasons as explained
  # in the openai/gpt-3.5-turbo-0613 comment
  - name: openai/gpt-3.5-turbo-16k-0613
    model_name: openai/gpt-3.5-turbo-16k-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 16000
    max_request_length: 16001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPTTurbo16KWindowService"
      args: {}


  # ----- GPT 4 Models ----- #

  - name: openai/gpt-4-0314
    model_name: openai/gpt-4-0314
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 8192
    max_request_length: 8193
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPT4WindowService"
      args: {}

  - name: openai/gpt-4-32k-0314
    model_name: openai/gpt-4-32k-0314
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 32768
    max_request_length: 32769
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPT432KWindowService"
      args: {}

  - name: openai/gpt-4-0613
    model_name: openai/gpt-4-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 8192
    max_request_length: 8193
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPT4WindowService"
      args: {}

  - name: openai/gpt-4-32k-0613
    model_name: openai/gpt-4-32k-0613
    tokenizer_name: openai/cl100k_base
    max_sequence_length: 32768
    max_request_length: 32769
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.GPT432KWindowService"
      args: {}


  # ----- Codex Models ----- #
  # DEPRECATED: Codex models have been shut down on March 23 2023.

  - name: openai/code-davinci-002 
    deprecated: true
    model_name: openai/code-davinci-002
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 4000
    max_request_length: 4001
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.wider_openai_window_service.WiderOpenAIWindowService"
      args: {}

  - name: openai/code-davinci-001 
    deprecated: true
    model_name: openai/code-davinci-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/code-cushman-001 
    deprecated: true
    model_name: openai/code-cushman-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  
  # ----- Text Similarity Models ----- #
  # OpenAI similarity embedding models: https://beta.openai.com/docs/guides/embeddings
  # The number of parameters is guessed based on the number of parameters of the
  # corresponding GPT-3 model.
  # DEPRECATED: Announced on July 06 2023 that first generation embeddings models
  #  will be shut down on January 04 2024.

  - name: openai/text-similarity-davinci-001 
    deprecated: true
    model_name: openai/text-similarity-davinci-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-similarity-curie-001 
    deprecated: true
    model_name: openai/text-similarity-curie-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-similarity-babbage-001 
    deprecated: true
    model_name: openai/text-similarity-babbage-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-similarity-ada-001 
    deprecated: true
    model_name: openai/text-similarity-ada-001
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}

  - name: openai/text-embedding-ada-002 
    deprecated: true
    model_name: openai/text-embedding-ada-002
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.openai_client.OpenAIClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.openai_window_service.OpenAIWindowService"
      args: {}
  # ============================ #



  # =========== Together =========== #
  # The list of models served by Together changes often, to check the latest list, visit:
  # https://docs.together.ai/docs/inference-models
  # You can also check the playground to check that the live models are working:
  # https://api.together.xyz/playground

  # ---------- BigScience ---------- #
  - name: together/bloom 
    deprecated: true # Removed from together
    model_name: bigscience/bloom
    tokenizer_name: bigscience/bloom
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.bloom_window_service.BloomWindowService"
      args: {}

  - name: together/t0pp 
    deprecated: true # Removed from together
    model_name: bigscience/t0pp
    tokenizer_name: bigscience/T0pp
    max_sequence_length: 1024
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.t0pp_window_service.T0ppWindowService"
      args: {}

  # ---------- Databricks ---------- #
  - name: together/dolly-v2-3b
    model_name: databricks/dolly-v2-3b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/dolly-v2-7b
    model_name: databricks/dolly-v2-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/dolly-v2-12b
    model_name: databricks/dolly-v2-12b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  # ---------- EleutherAI ---------- #
  - name: together/gpt-j-6b
    deprecated: true # Removed from together
    model_name: eleutherai/gpt-j-6b
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptj_window_service.GPTJWindowService"
      args: {}

  - name: together/gpt-neox-20b
    deprecated: true # Removed from together
    model_name: eleutherai/gpt-neox-20b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/pythia-1b-v0
    model_name: eleutherai/pythia-1b-v0
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/pythia-2.8b-v0
    model_name: eleutherai/pythia-2.8b-v0
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/pythia-6.9b
    model_name: eleutherai/pythia-6.9b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/pythia-12b-v0
    model_name: eleutherai/pythia-12b-v0
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  # ---------- Google ---------- #
  - name: together/t5-11b
    deprecated: true # Removed from together
    model_name: google/t5-11b
    tokenizer_name: google/t5-11b
    max_sequence_length: 511
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.t511b_window_service.T511bWindowService"
      args: {}

  - name: together/flan-t5-xxl
    deprecated: true # Removed from together
    model_name: google/flan-t5-xxl
    tokenizer_name: google/flan-t5-xxl
    max_sequence_length: 511
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.flan_t5_window_service.FlanT5WindowService"
      args: {}

  - name: together/ul2
    deprecated: true # Removed from together
    model_name: google/ul2
    tokenizer_name: google/ul2
    max_sequence_length: 511
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ul2_window_service.UL2WindowService"
      args: {}

  # ---------- HazyResearch ---------- #
  - name: together/h3-2.7b
    deprecated: true# Not available on Together yet
    model_name: hazyresearch/h3-2.7b
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 1024
    max_request_length: 1025
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gpt2_window_service.GPT2WindowService"
      args: {}

  # ---------- Lmsys ---------- #
  # TODO: might be deprecated. Needs to be checked.
  # Together officialy supports vicuna 1.5, not sure if 1.3 is still supported.
  - name: together/vicuna-7b-v1.3
    model_name: lmsys/vicuna-7b-v1.3
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  - name: together/vicuna-13b-v1.3
    model_name: lmsys/vicuna-13b-v1.3
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  # ---------- Meta ---------- #
  - name: together/llama-7b
    model_name: meta/llama-7b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  - name: together/llama-13b
    model_name: meta/llama-13b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  - name: together/llama-30b
    model_name: meta/llama-30b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  - name: together/llama-65b
    deprecated: true # Not available on Together yet
    model_name: meta/llama-65b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  - name: together/llama-2-7b
    model_name: meta/llama-2-7b
    tokenizer_name: meta-llama/Llama-2-7b-hf
    max_sequence_length: 4096
    max_request_length: 1000000000000000019884624838656
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.Llama2WindowService"
      args: {}

  - name: together/llama-2-13b
    model_name: meta/llama-2-13b
    tokenizer_name: meta-llama/Llama-2-7b-hf
    max_sequence_length: 4096
    max_request_length: 1000000000000000019884624838656
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.Llama2WindowService"
      args: {}

  - name: together/llama-2-70b
    model_name: meta/llama-2-70b
    tokenizer_name: meta-llama/Llama-2-7b-hf
    max_sequence_length: 4096
    max_request_length: 1000000000000000019884624838656
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.Llama2WindowService"
      args: {}

  - name: together/opt-175b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-175b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.opt_window_service.OPTWindowService"
      args: {}

  - name: together/opt-66b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-66b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.opt_window_service.OPTWindowService"
      args: {}

  - name: together/opt-6.7b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-6.7b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.opt_window_service.OPTWindowService"
      args: {}

  - name: together/opt-1.3b
    deprecated: true # Not available on Together yet
    model_name: meta/opt-1.3b
    tokenizer_name: facebook/opt-66b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.opt_window_service.OPTWindowService"
      args: {}

  # ----------- MistralAI ----------- #
  - name: together/mistral-7b-v0.1
    model_name: mistralai/mistral-7b-v0.1
    tokenizer_name: mistralai/Mistral-7B-v0.1
    max_sequence_length: 1000000000000000019884624838656
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  # ----------- MosaicML ----------- #
  - name: together/mpt-7b
    deprecated: true # Not available on Together yet
    model_name: mosaicml/mpt-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/mpt-instruct-7b
    deprecated: true # Not available on Together yet
    model_name: mosaicml/mpt-instruct-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/mpt-30b
    model_name: mosaicml/mpt-30b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/mpt-instruct-30b
    model_name: mosaicml/mpt-instruct-30b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  # ----------- StabilityAI ----------- #
  - name: together/stablelm-base-alpha-3b
    deprecated: true # Removed from together
    model_name: stabilityai/stablelm-base-alpha-3b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 4096
    max_request_length: 4097
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.StableLMAlphaWindowService"
      args: {}

  - name: together/stablelm-base-alpha-7b
    deprecated: true # Removed from together
    model_name: stabilityai/stablelm-base-alpha-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 4096
    max_request_length: 4097
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.StableLMAlphaWindowService"
      args: {}

  # ----------- Stanford ----------- #
  - name: together/alpaca-7b
    model_name: stanford/alpaca-7b
    tokenizer_name: hf-internal-testing/llama-tokenizer
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.llama_window_service.LlamaWindowService"
      args: {}

  # ----------- Tiiuae ----------- #
  - name: together/falcon-7b
    model_name: tiiuae/falcon-7b
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  - name: together/falcon-7b-instruct
    model_name: tiiuae/falcon-7b-instruct
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  - name: together/falcon-40b
    model_name: tiiuae/falcon-40b
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  - name: together/falcon-40b-instruct
    model_name: tiiuae/falcon-40b-instruct
    tokenizer_name: tiiuae/falcon-7b
    max_sequence_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.huggingface_window_service.HuggingFaceWindowService"
      args: {}

  # ---------- Together ---------- #
  # These are models fine-tuned by Together (and not simply hosted by Together).
  - name: together/gpt-jt-6b-v1
    model_name: together/gpt-jt-6b-v1
    tokenizer_name: EleutherAI/gpt-j-6B
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptj_window_service.GPTJWindowService"
      args: {}

  - name: together/gpt-neoxt-chat-base-20b
    model_name: together/gpt-neoxt-chat-base-20b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/redpajama-incite-base-3b-v1
    model_name: together/redpajama-incite-base-3b-v1
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/redpajama-incite-instruct-3b-v1
    model_name: together/redpajama-incite-instruct-3b-v1
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/redpajama-incite-base-7b
    model_name: together/redpajama-incite-base-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  - name: together/redpajama-incite-instruct-7b
    model_name: together/redpajama-incite-instruct-7b
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.gptneox_window_service.GPTNeoXWindowService"
      args: {}

  # ---------- Tsinghua ---------- #
  - name: together/glm
    deprecated: true # Not available on Together yet
    model_name: tsinghua/glm
    tokenizer_name: TsinghuaKEG/ice
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.ice_window_service.ICEWindowService"
      args: {}

  # ---------- Yandex ---------- #
  - name: together/yalm
    deprecated: true # Not available on Together yet
    model_name: yandex/yalm
    tokenizer_name: Yandex/yalm
    max_sequence_length: 2048
    max_request_length: 2049
    client_spec:
      class_name: "helm.proxy.clients.together_client.TogetherClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.yalm_window_service.YaLMWindowService"
      args: {}
  # ================================ #



  # ========== Writer ========== #
  - name: writer/palmyra-base
    model_name: writer/palmyra-base
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.palmyra_window_service.PalmyraWindowService"
      args: {}

  - name: writer/palmyra-large
    model_name: writer/palmyra-large
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.palmyra_window_service.PalmyraWindowService"
      args: {}

  - name: writer/palmyra-instruct-30
    model_name: writer/palmyra-instruct-30
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.palmyra_window_service.PalmyraWindowService"
      args: {}

  - name: writer/palmyra-e
    model_name: writer/palmyra-e
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 2048
    max_sequence_and_generated_tokens_length: 2048
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.palmyra_window_service.PalmyraWindowService"
      args: {}

  - name: writer/silk-road
    model_name: writer/silk-road
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 8192
    max_sequence_and_generated_tokens_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.palmyra_window_service.LongerPalmyraWindowService"
      args: {}

  - name: writer/palmyra-x
    model_name: writer/palmyra-x
    tokenizer_name: huggingface/gpt2
    max_sequence_length: 8192
    max_sequence_and_generated_tokens_length: 8192
    client_spec:
      class_name: "helm.proxy.clients.palmyra_client.PalmyraClient"
      args: {}
    window_service_spec:
      class_name: "helm.benchmark.window_services.palmyra_window_service.LongerPalmyraWindowService"
      args: {}
  # ============================ #