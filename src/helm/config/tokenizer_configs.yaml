# This file defines all the tokenizers that are supported by the Helm API.

# If you want to add a new tokenizer, you can technically do it here but we recommend
# you to do it in prod_env/tokenizer_configs.yaml instead.

# Follow the template of this file to add a new tokenizer. You can copy paste this to get started:
#    # This file contains the tokenizer configs for the private tokenizers
#    tokenizer_configs: [] # Leave empty to disable private tokenizers


tokenizer_configs:

  - name: simple/tokenizer1
    tokenizer_spec:
      class_name: "helm.tokenizers.simple_tokenizer.SimpleTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # AI21
  - name: ai21/j1
    tokenizer_spec:
      class_name: "helm.tokenizers.ai21_tokenizer.AI21Tokenizer"
    end_of_text_token: " "
    prefix_token: ""

  # AlephAlpha
  - name: AlephAlpha/luminous-base
    tokenizer_spec:
      class_name: "helm.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""
  - name: AlephAlpha/luminous-extended
    tokenizer_spec:
      class_name: "helm.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""
  - name: AlephAlpha/luminous-supreme
    tokenizer_spec:
      class_name: "helm.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""
  - name: AlephAlpha/luminous-world
    tokenizer_spec:
      class_name: "helm.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""

  # Anthropic
  - name: anthropic/claude
    tokenizer_spec:
      class_name: "helm.tokenizers.anthropic_tokenizer.AnthropicTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Bigcode
  - name: bigcode/santacoder
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"
  - name: bigcode/starcoder
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Bigscience
  - name: bigscience/bloom
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "</s>"
  - name: bigscience/T0pp
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""

  # Cohere
  - name: cohere/cohere
    tokenizer_spec:
      class_name: "helm.tokenizers.cohere_tokenizer.CohereTokenizer"
    end_of_text_token: ""
    prefix_token: ":"

  # Databricks
  - name: databricks/dbrx-instruct
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # DeepSeek
  - name: deepseek-ai/deepseek-llm-67b-chat
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<｜end▁of▁sentence｜>"
    prefix_token: "<｜begin▁of▁sentence｜>"

  # EleutherAI
  - name: EleutherAI/gpt-j-6B
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"
  - name: EleutherAI/gpt-neox-20b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Facebook
  - name: facebook/opt-66b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "</s>"

  # Google
  - name: google/t5-11b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: google-t5/t5-11b
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/flan-t5-xxl
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/ul2
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/mt5-base
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/text-bison@001
    tokenizer_spec:
      class_name: "helm.tokenizers.vertexai_tokenizer.VertexAITokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/text-bison@002
    tokenizer_spec:
      class_name: "helm.tokenizers.vertexai_tokenizer.VertexAITokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/text-unicorn@001
    tokenizer_spec:
      class_name: "helm.tokenizers.vertexai_tokenizer.VertexAITokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/gemma-2b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<eos>"
    prefix_token: "<bos>"

  # Hf-internal-testing

  # Tokenizer name hf-internal-testing/llama-tokenizer is taken from:
  # https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaTokenizerFast.example
  - name: hf-internal-testing/llama-tokenizer
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # HuggingFaceM4
  - name: HuggingFaceM4/idefics-9b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
  - name: HuggingFaceM4/idefics-9b-instruct
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
  - name: HuggingFaceM4/idefics-80b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
  - name: HuggingFaceM4/idefics-80b-instruct
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
    
  - name: anas-awadalla/mpt-7b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  # Huggingface
  - name: huggingface/gpt2
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: openai-community/gpt2
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Lighting AI
  - name: lightningai/lit-gpt
    tokenizer_spec:
      class_name: "helm.tokenizers.lit_gpt_tokenizer.LitGPTTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Meta-llama

  # To use the Llama-2 tokenizer:
    #
    # 1. Accept the license agreement: https://ai.meta.com/resources/models-and-libraries/llama-downloads/
    # 2. Request to access the Hugging Face repository: https://huggingface.co/meta-llama/Llama-2-7b
    # 3. Run `huggingface-cli login`
    #
    # If you encounter the following error, complete the above steps and try again:
    #
    #     meta-llama/Llama-2-70b-hf is not a local folder and is not a valid model identifier listed on
    #     'https://huggingface.co/models'
  - name: meta-llama/Llama-2-7b-hf
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  - name: meta/llama-3-8b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: meta-llama/Meta-Llama-3-8B
    prefix_token: "<|begin_of_text|>"
    end_of_text_token: "<|end_of_text|>"

  # 01-ai
  - name: 01-ai/Yi-6B
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"


  # Allen Institute for AI
  # The allenai/olmo-7b requires Python 3.9 or newer.
  # To use the allenai/olmo-7b tokenizer, run `pip install crfm-helm[allenai]` first.
  - name: allenai/olmo-7b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        trust_remote_code: true
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""


  # Microsoft
  - name: microsoft/phi-2  
    tokenizer_spec:  
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"  
    end_of_text_token: "<|endoftext|>"  
    prefix_token: "<|endoftext|>"

  # Mistralai
  - name: mistralai/Mistral-7B-v0.1
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # Neurips
  - name: neurips/local
    tokenizer_spec:
      class_name: "helm.tokenizers.http_model_tokenizer.HTTPModelTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Openai
  - name: openai/cl100k_base
    tokenizer_spec:
      class_name: "helm.tokenizers.tiktoken_tokenizer.TiktokenTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  - name: openai/clip-vit-large-patch14
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: ""
    prefix_token: ""

  - name: qwen/qwen-7b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: Qwen/Qwen-7B
        trust_remote_code: true
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  - name: qwen/qwen1.5-7b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: Qwen/Qwen1.5-7B
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  - name: qwen/qwen-vl
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: Qwen/Qwen-VL
        trust_remote_code: true
    # Source: https://github.com/QwenLM/Qwen-VL
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  - name: qwen/qwen-vl-chat
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: Qwen/Qwen-VL-Chat
        trust_remote_code: true
    # Source: https://github.com/QwenLM/Qwen-VL
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  # Snowflake
  - name: snowflake/snowflake-arctic-instruct
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|im_end|>"
    prefix_token: "<|im_start|>"

  # Tiiuae
  - name: tiiuae/falcon-7b
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  # TsinghuaKEG
  - name: TsinghuaKEG/ice
    tokenizer_spec:
      class_name: "helm.tokenizers.ice_tokenizer.ICETokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""

  # Writer
  - name: writer/gpt2
    tokenizer_spec:
      class_name: "helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
      args:
        pretrained_model_name_or_path: openai-community/gpt2
    end_of_text_token: ""
    prefix_token: ""

  # Yandex
  - name: Yandex/yalm
    tokenizer_spec:
      class_name: "helm.tokenizers.yalm_tokenizer.YaLMTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "</s>"
