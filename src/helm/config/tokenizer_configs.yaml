# This file defines all the tokenizers that are supported by the Helm API.

# If you want to add a new tokenizer, you can technically do it here but we recommend
# you to do it in prod_env/tokenizer_configs.yaml instead.

# Follow the template of this file to add a new tokenizer. You can copy paste this to get started:
#    # This file contains the tokenizer configs for the private tokenizers
#    tokenizer_configs: [] # Leave empty to disable private tokenizers


tokenizer_configs:

  - name: simple/model1
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.simple_tokenizer.SimpleTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # AI21
  - name: ai21/j1
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.ai21_tokenizer.AI21Tokenizer"
    end_of_text_token: " "
    prefix_token: ""

  # AlephAlpha
  - name: AlephAlpha/luminous-base
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""
  - name: AlephAlpha/luminous-extended
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""
  - name: AlephAlpha/luminous-supreme
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""
  - name: AlephAlpha/luminous-world
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.aleph_alpha_tokenizer.AlephAlphaTokenizer"
    end_of_text_token: ""
    prefix_token: ""

  # Anthropic
  - name: anthropic/claude
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.anthropic_tokenizer.AnthropicTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Bigcode
  - name: bigcode/santacoder
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"
  - name: bigcode/starcoder
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Bigscience
  - name: bigscience/bloom
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "</s>"
  - name: bigscience/T0pp
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""

  # Cohere
  - name: cohere/cohere
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.cohere_tokenizer.CohereTokenizer"
    end_of_text_token: ""
    prefix_token: ":"

  # EleutherAI
  - name: EleutherAI/gpt-j-6B
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"
  - name: EleutherAI/gpt-neox-20b
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Facebook
  - name: facebook/opt-66b
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "</s>"

  # Google
  - name: google/t5-11b
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/flan-t5-xxl
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/ul2
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/mt5-base
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/text-bison@001
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.vertexai_tokenizer.VertexAITokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""
  - name: google/text-unicorn@001
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.vertexai_tokenizer.VertexAITokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""

  # Hf-internal-testing

  # Tokenizer name hf-internal-testing/llama-tokenizer is taken from:
  # https://huggingface.co/docs/transformers/main/en/model_doc/llama#transformers.LlamaTokenizerFast.example
  - name: hf-internal-testing/llama-tokenizer
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # HuggingFaceM4
  - name: HuggingFaceM4/idefics-9b
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
  - name: HuggingFaceM4/idefics-9b-instruct
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
  - name: HuggingFaceM4/idefics-80b
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"
  - name: HuggingFaceM4/idefics-80b-instruct
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # Huggingface
  - name: huggingface/gpt2
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Lighting AI
  - name: lightningai/lit-gpt
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.lit_gpt_tokenizer.LitGPTTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Meta-llama

  # To use the Llama-2 tokenizer:
    #
    # 1. Accept the license agreement: https://ai.meta.com/resources/models-and-libraries/llama-downloads/
    # 2. Request to access the Hugging Face repository: https://huggingface.co/meta-llama/Llama-2-7b
    # 3. Run `huggingface-cli login`
    #
    # If you encounter the following error, complete the above steps and try again:
    #
    #     meta-llama/Llama-2-70b-hf is not a local folder and is not a valid model identifier listed on
    #     'https://huggingface.co/models'
  - name: meta-llama/Llama-2-7b-hf
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"


  # 01-ai
  - name: 01-ai/Yi-6B
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # Microsoft
  - name: microsoft/gpt2
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<<"

  # Mistralai
  - name: mistralai/Mistral-7B-v0.1
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "<s>"

  # Neurips
  - name: neurips/local
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.http_model_tokenizer.HTTPModelTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Openai
  - name: openai/cl100k_base
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.tiktoken_tokenizer.TiktokenTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: "<|endoftext|>"

  # Tiiuae
  - name: tiiuae/falcon-7b
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: "<|endoftext|>"
    prefix_token: ""

  # TsinghuaKEG
  - name: TsinghuaKEG/ice
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.ice_tokenizer.ICETokenizer"
    end_of_text_token: "</s>"
    prefix_token: ""

  # Writer
  - name: writer/gpt2
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer"
    end_of_text_token: ""
    prefix_token: ""

  # Yandex
  - name: Yandex/yalm
    tokenizer_spec:
      class_name: "helm.proxy.tokenizers.yalm_tokenizer.YaLMTokenizer"
    end_of_text_token: "</s>"
    prefix_token: "</s>"
