# This file defines all the models officially supported by the Helm API.
# The model names here should match the model names in model_deployments.yaml.

# If you want to add a new model, you can technically do it here but we recommend
# you to do it in prod_env/model_metadata.yaml instead.

# Follow the template of this file to add a new model. You can copy paste this to get started:
#    # This file contains the metadata for private models
#    models: [] # Leave empty to disable private models


models:

  - name: simple/model1
    display_name: Simple Model 1
    description: This is a test model.
    creator_organization_name: Helm
    access: open
    release_date: 2023-01-01
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]
    
  # Adobe
  - name: adobe/giga-gan
    display_name: GigaGAN (1B)
    description: GigaGAN is a GAN model that produces high-quality images extremely quickly. The model was trained on text and image pairs from LAION2B-en and COYO-700M. ([paper](https://arxiv.org/abs/2303.05511)).
    creator_organization_name: Adobe
    access: limited
    num_parameters: 1000000000
    release_date: 2023-06-22
    tags: [TEXT_TO_IMAGE_MODEL_TAG]


  # AI21 Labs
  - name: ai21/j1-jumbo
    display_name: J1-Jumbo v1 (178B)
    description: Jurassic-1 Jumbo (178B parameters) ([docs](https://studio.ai21.com/docs/jurassic1-language-models/), [tech report](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)).
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 178000000000
    release_date: 2021-08-11
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: ai21/j1-large
    display_name: J1-Large v1 (7.5B)
    description: Jurassic-1 Large (7.5B parameters) ([docs](https://studio.ai21.com/docs/jurassic1-language-models/), [tech report](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)).
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 7500000000
    release_date: 2021-08-11
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: ai21/j1-grande
    display_name: J1-Grande v1 (17B)
    description: Jurassic-1 Grande (17B parameters) with a "few tweaks" to the training process ([docs](https://studio.ai21.com/docs/jurassic1-language-models/), [tech report](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)).
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 17000000000
    release_date: 2022-05-03
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: ai21/j1-grande-v2-beta
    display_name: J1-Grande v2 beta (17B)
    description: Jurassic-1 Grande v2 beta (17B parameters)
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 17000000000
    release_date: 2022-10-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: ai21/j2-large
    display_name: Jurassic-2 Large (7.5B)
    description: Jurassic-2 Large (7.5B parameters) ([docs](https://www.ai21.com/blog/introducing-j2))
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 7500000000
    release_date: 2023-03-09
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ai21/j2-grande
    display_name: Jurassic-2 Grande (17B)
    description: Jurassic-2 Grande (17B parameters) ([docs](https://www.ai21.com/blog/introducing-j2))
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 17000000000
    release_date: 2023-03-09
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ai21/j2-jumbo
    display_name: Jurassic-2 Jumbo (178B)
    description: Jurassic-2 Jumbo (178B parameters) ([docs](https://www.ai21.com/blog/introducing-j2))
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 178000000000
    release_date: 2023-03-09
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # TODO(1524): Change AI21 model names
  # - j2-jumbo -> j2-ultra
  # - j2-grande -> j2-mid
  # - j2-large -> j2-light

  - name: ai21/jamba-instruct
    display_name: Jamba Instruct
    description: Jamba Instruct is an instruction tuned version of Jamba, which uses a hybrid Transformer-Mamba mixture-of-experts (MoE) architecture that interleaves blocks of Transformer and Mamba layers. ([blog](https://www.ai21.com/blog/announcing-jamba-instruct))
    creator_organization_name: AI21 Labs
    access: limited
    num_parameters: 52000000000
    release_date: 2024-05-02
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ai21/jamba-1.5-mini
    display_name: Jamba 1.5 Mini
    description: Jamba 1.5 Mini is a long-context, hybrid SSM-Transformer instruction following foundation model that is optimized for function calling, structured output, and grounded generation. ([blog](https://www.ai21.com/blog/announcing-jamba-model-family))
    creator_organization_name: AI21 Labs
    access: open
    num_parameters: 51600000000
    release_date: 2024-08-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ai21/jamba-1.5-large
    display_name: Jamba 1.5 Large
    description: Jamba 1.5 Large is a long-context, hybrid SSM-Transformer instruction following foundation model that is optimized for function calling, structured output, and grounded generation. ([blog](https://www.ai21.com/blog/announcing-jamba-model-family))
    creator_organization_name: AI21 Labs
    access: open
    num_parameters: 399000000000
    release_date: 2024-08-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # AI Singapore
  - name: aisingapore/sea-lion-7b
    display_name: SEA-LION 7B
    description: SEA-LION is a collection of language models which has been pretrained and instruct-tuned on languages from the Southeast Asia region. It utilizes the MPT architecture and a custom SEABPETokenizer for tokenization.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 7000000000
    release_date: 2023-02-24
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: aisingapore/sea-lion-7b-instruct
    display_name: SEA-LION 7B Instruct
    description: SEA-LION is a collection of language models which has been pretrained and instruct-tuned on languages from the Southeast Asia region. It utilizes the MPT architecture and a custom SEABPETokenizer for tokenization.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 7000000000
    release_date: 2023-02-24
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: aisingapore/llama3-8b-cpt-sea-lionv2-base
    display_name: Llama3 8B CPT SEA-LIONv2
    description: Llama3 8B CPT SEA-LIONv2 is a multilingual model which was continued pre-trained on 48B additional tokens, including tokens in Southeast Asian languages.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 8030000000
    release_date: 2024-07-31
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: aisingapore/llama3-8b-cpt-sea-lionv2.1-instruct
    display_name: Llama3 8B CPT SEA-LIONv2.1 Instruct
    description: Llama3 8B CPT SEA-LIONv2.1 Instruct is a multilingual model which has been fine-tuned with around 100,000 English instruction-completion pairs alongside a smaller pool of around 50,000 instruction-completion pairs from other Southeast Asian languages, such as Indonesian, Thai and Vietnamese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 8030000000
    release_date: 2024-08-21
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: aisingapore/gemma2-9b-cpt-sea-lionv3-base
    display_name: Gemma2 9B CPT SEA-LIONv3
    description: Gemma2 9B CPT SEA-LIONv3 Base is a multilingual model which has undergone continued pre-training on approximately 200B tokens across the 11 official Southeast Asian languages, such as English, Chinese, Vietnamese, Indonesian, Thai, Tamil, Filipino, Malay, Khmer, Lao, Burmese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 9240000000
    release_date: 2024-10-30
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: aisingapore/gemma2-9b-cpt-sea-lionv3-instruct
    display_name: Gemma2 9B CPT SEA-LIONv3 Instruct
    description: Gemma2 9B CPT SEA-LIONv3 Instruct is a multilingual model which has been fine-tuned with around 500,000 English instruction-completion pairs alongside a larger pool of around 1,000,000 instruction-completion pairs from other ASEAN languages, such as Indonesian, Thai and Vietnamese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 9240000000
    release_date: 2024-10-30
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: aisingapore/llama3.1-8b-cpt-sea-lionv3-base
    display_name: Llama3.1 8B CPT SEA-LIONv3
    description: Llama3.1 8B CPT SEA-LIONv3 Base is a multilingual model which has undergone continued pre-training on approximately 200B tokens across 11 SEA languages, such as Burmese, Chinese, English, Filipino, Indonesia, Khmer, Lao, Malay, Tamil, Thai and Vietnamese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 9240000000
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct
    display_name: Llama3.1 8B CPT SEA-LIONv3 Instruct
    description: Llama3.1 8B CPT SEA-LIONv3 Instruct is a multilingual model that has been fine-tuned in two stages on approximately 12.3M English instruction-completion pairs alongside a pool of 4.5M Southeast Asian instruction-completion pairs from SEA languages such as Indonesian, Javanese, Sundanese, Tamil, Thai and Vietnamese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 9240000000
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: aisingapore/llama3.1-70b-cpt-sea-lionv3-base
    display_name: Llama3.1 70B CPT SEA-LIONv3
    description: Llama3.1 70B CPT SEA-LIONv3 Base is a multilingual model which has undergone continued pre-training on approximately 200B tokens across 11 SEA languages, such as Burmese, Chinese, English, Filipino, Indonesia, Khmer, Lao, Malay, Tamil, Thai and Vietnamese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 70600000000
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: aisingapore/llama3.1-70b-cpt-sea-lionv3-instruct
    display_name: Llama3.1 70B CPT SEA-LIONv3 Instruct
    description: Llama3.1 70B CPT SEA-LIONv3 Instruct is a multilingual model that has been fine-tuned in two stages on approximately 12.3M English instruction-completion pairs alongside a pool of 4.5M Southeast Asian instruction-completion pairs from SEA languages such as Indonesian, Javanese, Sundanese, Tamil, Thai, and Vietnamese.
    creator_organization_name: AI Singapore
    access: open
    num_parameters: 70600000000
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Aleph Alpha
  # Aleph Alpha's Luminous models: https://docs.aleph-alpha.com/docs/introduction/luminous
  # TODO: add Luminous World when it's released
  - name: AlephAlpha/luminous-base
    display_name: Luminous Base (13B)
    description: Luminous Base (13B parameters) ([docs](https://docs.aleph-alpha.com/docs/introduction/luminous/))
    creator_organization_name: Aleph Alpha
    access: limited
    num_parameters: 13000000000
    # TODO: get exact release date
    release_date: 2022-01-01
    # Does not support echo
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: AlephAlpha/luminous-extended
    display_name: Luminous Extended (30B)
    description: Luminous Extended (30B parameters) ([docs](https://docs.aleph-alpha.com/docs/introduction/luminous/))
    creator_organization_name: Aleph Alpha
    access: limited
    num_parameters: 30000000000
    release_date: 2022-01-01
    # Does not support echo
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: AlephAlpha/luminous-supreme
    display_name: Luminous Supreme (70B)
    description: Luminous Supreme (70B parameters) ([docs](https://docs.aleph-alpha.com/docs/introduction/luminous/))
    creator_organization_name: Aleph Alpha
    access: limited
    num_parameters: 70000000000
    release_date: 2022-01-01
    # Does not support echo.
    # Currently, only Luminous-extended and Luminous-base support multimodal inputs
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]
  
  # TODO: Uncomment when luminous-world is released.
  # - name: AlephAlpha/luminous-world # Not released yet.
  #   display_name: Luminous World (178B)
  #   description: Luminous World (178B parameters) ([docs](https://docs.aleph-alpha.com/docs/introduction/luminous/))
  #   creator_organization_name: Aleph Alpha
  #   access: limited
  #   num_parameters: TBD
  #   release_date: TBD
  #   # Does not support echo.
  #   tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]
  
  - name: AlephAlpha/m-vader
    display_name: MultiFusion (13B)
    description: MultiFusion is a multimodal, multilingual diffusion model that extend the capabilities of Stable Diffusion v1.4 by integrating different pre-trained modules, which transfers capabilities to the downstream model ([paper](https://arxiv.org/abs/2305.15296))
    creator_organization_name: Aleph Alpha
    access: limited
    num_parameters: 13000000000
    release_date: 2023-05-24
    tags: [TEXT_TO_IMAGE_MODEL_TAG]


  # Amazon Nova models
  # References for Amazon Nova models:
  # https://aws.amazon.com/ai/generative-ai/nova/
  - name: amazon/nova-premier-v1:0
    display_name: Amazon Nova Premier
    description: Amazon Nova Premier is the most capable model in the Nova family of foundation models. ([blog](https://aws.amazon.com/blogs/aws/amazon-nova-premier-our-most-capable-model-for-complex-tasks-and-teacher-for-model-distillation/))
    creator_organization_name: Amazon
    access: limited
    release_date: 2025-04-30
    tags: [NOVA_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: amazon/nova-pro-v1:0
    display_name: Amazon Nova Pro
    description: Amazon Nova Pro Model
    creator_organization_name: Amazon
    access: limited
    release_date: 2024-12-03
    tags: [NOVA_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: amazon/nova-lite-v1:0
    display_name: Amazon Nova Lite
    description: Amazon Nova Lite Model
    creator_organization_name: Amazon
    access: limited
    release_date: 2024-12-03
    tags: [NOVA_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: amazon/nova-micro-v1:0
    display_name: Amazon Nova Micro
    description: Amazon Nova Micro Model
    creator_organization_name: Amazon
    access: limited
    release_date: 2024-12-03
    tags: [NOVA_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  # Titan Models
  # References for Amazon Titan models:
  # - https://aws.amazon.com/bedrock/titan/
  # - https://community.aws/content/2ZUVD3fkNtqEOYIa2iUJAFArS7c/family-of-titan-text-models---cli-demo
  # - https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-titan-models-express-lite-bedrock/
  - name: amazon/titan-text-lite-v1
    display_name: Amazon Titan Text Lite
    description: Amazon Titan Text Lite is a lightweight, efficient model perfect for fine-tuning English-language tasks like summarization and copywriting. It caters to customers seeking a smaller, cost-effective, and highly customizable model. It supports various formats, including text generation, code generation, rich text formatting, and orchestration (agents). Key model attributes encompass fine-tuning, text generation, code generation, and rich text formatting.
    creator_organization_name: Amazon
    access: limited
    release_date: 2023-11-29
    tags: [BEDROCK_MODEL_TAG,TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]
 
  - name: amazon/titan-text-express-v1
    display_name: Amazon Titan Text Express
    description: Amazon Titan Text Express, with a context length of up to 8,000 tokens, excels in advanced language tasks like open-ended text generation and conversational chat. It's also optimized for Retrieval Augmented Generation (RAG). Initially designed for English, the model offers preview multilingual support for over 100 additional languages.
    creator_organization_name: Amazon
    access: limited
    release_date: 2023-11-29
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

# Mistral Models on Bedrock
# References for Mistral on Amazon Bedrock
# https://aws.amazon.com/bedrock/mistral/

  - name: mistralai/amazon-mistral-7b-instruct-v0:2
    display_name:  Mistral 7B Instruct on Amazon Bedrock
    description: A 7B dense Transformer, fast-deployed and easily customisable. Small, yet powerful for a variety of use cases. Supports English and code, and a 32k context window.
    creator_organization_name: Mistral
    access: limited
    release_date: 2024-03-23
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/amazon-mixtral-8x7b-instruct-v0:1
    display_name: Mixtral 8x7B Instruct on Amazon Bedrock
    description: A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Uses 12B active parameters out of 45B total. Supports multiple languages, code and 32k context window.
    creator_organization_name: Mistral
    access: limited
    release_date: 2023-12-11
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/amazon-mistral-large-2402-v1:0
    display_name: Mistral Large(2402) on Amazon Bedrock
    description: The most advanced Mistral AI Large Language model capable of handling any language task including complex multilingual reasoning, text understanding, transformation, and code generation.
    creator_organization_name: Mistral
    access: limited
    release_date: 2023-07-26
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/amazon-mistral-small-2402-v1:0
    display_name: Mistral Small on Amazon Bedrock
    description: Mistral Small is perfectly suited for straightforward tasks that can be performed in bulk, such as classification, customer support, or text generation. It provides outstanding performance at a cost-effective price point.
    creator_organization_name: Mistral
    access: limited
    release_date: 2023-02-26
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/amazon-mistral-large-2407-v1:0
    display_name: Mistral Large(2407) on Amazon Bedrock
    description: Mistral Large 2407 is an advanced Large Language Model (LLM) that supports dozens of languages and is trained on 80+ coding languages. It has best-in-class agentic capabilities with native function calling JSON outputting and reasoning capabilities.
    creator_organization_name: Mistral
    access: limited
    release_date: 2024-07-24
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

# Llama3 on Amazon Bedrock
# References for Llama3 on Amazon Bedrock
# https://aws.amazon.com/bedrock/llama/

  - name: meta/amazon-llama3-8b-instruct-v1:0
    display_name: Llama 3 8B Instruct on Amazon Bedrock
    description: Meta Llama 3 is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. Ideal for limited computational power and resources, edge devices, and faster training times.
    creator_organization_name: Meta
    access: limited
    release_date: 2024-04-23
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/amazon-llama3-70b-instruct-v1:0
    display_name: Llama 3 70B Instruct on Amazon Bedrock
    description: Meta Llama 3 is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. Ideal for content creation, conversational AI, language understanding, R&D, and Enterprise applications.
    creator_organization_name: Meta
    access: limited
    release_date: 2024-04-23
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/amazon-llama3-1-405b-instruct-v1:0
    display_name: Llama 3.1 405b Instruct on Amazon Bedrock.
    description: Meta's Llama 3.1 offers multilingual models (8B, 70B, 405B) with 128K context, improved reasoning, and optimization for dialogue. It outperforms many open-source chat models and is designed for commercial and research use in multiple languages.
    creator_organization_name: Meta
    access: limited
    release_date: 2024-07-26
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/amazon-llama3-1-70b-instruct-v1:0
    display_name: Llama 3.1 70b Instruct on Amazon Bedrock.
    description: Meta's Llama 3.1 offers multilingual models (8B, 70B, 405B) with 128K context, improved reasoning, and optimization for dialogue. It outperforms many open-source chat models and is designed for commercial and research use in multiple languages.
    creator_organization_name: Meta
    access: limited
    release_date: 2024-07-26
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/amazon-llama3-1-8b-instruct-v1:0
    display_name: Llama 3.1 8b Instruct on Amazon Bedrock.
    description: Meta's Llama 3.1 offers multilingual models (8B, 70B, 405B) with 128K context, improved reasoning, and optimization for dialogue. It outperforms many open-source chat models and is designed for commercial and research use in multiple languages.
    creator_organization_name: Meta
    access: limited
    release_date: 2024-07-26
    tags: [BEDROCK_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Anthropic
  - name: anthropic/claude-v1.3
    display_name: Claude v1.3
    description: A 52B parameter language model, trained using reinforcement learning from human feedback [paper](https://arxiv.org/pdf/2204.05862.pdf).
    creator_organization_name: Anthropic
    access: limited
    num_parameters: 52000000000
    release_date: 2023-03-17
    tags: [ANTHROPIC_CLAUDE_1_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
 
  - name: anthropic/claude-instant-v1
    display_name: Claude Instant V1
    description: A lightweight version of Claude, a model trained using reinforcement learning from human feedback ([docs](https://www.anthropic.com/index/introducing-claude)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2023-03-17
    tags: [ANTHROPIC_CLAUDE_1_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-instant-1.2
    display_name: Claude Instant 1.2
    description: A lightweight version of Claude, a model trained using reinforcement learning from human feedback ([docs](https://www.anthropic.com/index/introducing-claude)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2023-08-09
    tags: [ANTHROPIC_CLAUDE_1_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-2.0
    display_name: Claude 2.0
    description: Claude 2.0 is a general purpose large language model developed by Anthropic. It uses a transformer architecture and is trained via unsupervised learning, RLHF, and Constitutional AI (including both a supervised and Reinforcement Learning (RL) phase). ([model card](https://efficient-manatee.files.svdcdn.com/production/images/Model-Card-Claude-2.pdf))
    creator_organization_name: Anthropic
    access: limited
    release_date: 2023-07-11
    tags: [ANTHROPIC_CLAUDE_2_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-2.1
    display_name: Claude 2.1
    description: Claude 2.1 is a general purpose large language model developed by Anthropic. It uses a transformer architecture and is trained via unsupervised learning, RLHF, and Constitutional AI (including both a supervised and Reinforcement Learning (RL) phase). ([model card](https://efficient-manatee.files.svdcdn.com/production/images/Model-Card-Claude-2.pdf))
    creator_organization_name: Anthropic
    access: limited
    release_date: 2023-11-21
    tags: [ANTHROPIC_CLAUDE_2_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-haiku-20240307
    display_name: Claude 3 Haiku (20240307)
    description: Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI ([blog](https://www.anthropic.com/news/claude-3-family)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2024-03-13  # https://www.anthropic.com/news/claude-3-haiku
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-sonnet-20240229
    display_name: Claude 3 Sonnet (20240229)
    description: Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI ([blog](https://www.anthropic.com/news/claude-3-family)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2024-03-04  # https://www.anthropic.com/news/claude-3-family
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-opus-20240229
    display_name: Claude 3 Opus (20240229)
    description: Claude 3 is a a family of models that possess vision and multilingual capabilities. They were trained with various methods such as unsupervised learning and Constitutional AI ([blog](https://www.anthropic.com/news/claude-3-family)).
    access: limited
    creator_organization_name: Anthropic
    release_date: 2024-03-04  # https://www.anthropic.com/news/claude-3-family
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-5-haiku-20241022
    display_name: Claude 3.5 Haiku (20241022)
    description: Claude 3.5 Haiku is a Claude 3 family model which matches the performance of Claude 3 Opus at a similar speed to the previous generation of Haiku ([blog](https://www.anthropic.com/news/3-5-models-and-computer-use)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2024-11-04  # Released after the blog post
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-5-sonnet-20240620
    display_name: Claude 3.5 Sonnet (20240620)
    description: Claude 3.5 Sonnet is a Claude 3 family model which outperforms Claude 3 Opus while operating faster and at a lower cost. ([blog](https://www.anthropic.com/news/claude-3-5-sonnet))
    creator_organization_name: Anthropic
    access: limited
    release_date: 2024-06-20
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-5-sonnet-20241022
    display_name: Claude 3.5 Sonnet (20241022)
    description: Claude 3.5 Sonnet is a Claude 3 family model which outperforms Claude 3 Opus while operating faster and at a lower cost ([blog](https://www.anthropic.com/news/claude-3-5-sonnet)). This is an upgraded snapshot released on 2024-10-22 ([blog](https://www.anthropic.com/news/3-5-models-and-computer-use)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2024-10-22
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-7-sonnet-20250219
    display_name: Claude 3.7 Sonnet (20250219)
    description: Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user ([blog](https://www.anthropic.com/news/claude-3-7-sonnet)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2025-02-24
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-3-7-sonnet-20250219-thinking-10k
    display_name: Claude 3.7 Sonnet (20250219, extended thinking)
    description: Claude 3.7 Sonnet is a Claude 3 family hybrid reasoning model that can produce near-instant responses or extended, step-by-step thinking that is made visible to the user ([blog](https://www.anthropic.com/news/claude-3-7-sonnet)). Extended thinking is enabled with 10k budget tokens.
    creator_organization_name: Anthropic
    access: limited
    release_date: 2025-02-24
    tags: [ANTHROPIC_CLAUDE_3_MODEL_TAG, TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-sonnet-4-20250514
    display_name: Claude 4 Sonnet (20250514)
    description: Claude 4 Sonnet is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning ([blog](https://www.anthropic.com/news/claude-4)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2025-05-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-sonnet-4-20250514-thinking-10k
    display_name: Claude 4 Sonnet (20250514, extended thinking)
    description: Claude 4 Sonnet is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning ([blog](https://www.anthropic.com/news/claude-4)). Extended thinking is enabled with 10k budget tokens.
    creator_organization_name: Anthropic
    access: limited
    release_date: 2025-05-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-opus-4-20250514
    display_name: Claude 4 Opus (20250514)
    description: Claude 4 Opus is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning ([blog](https://www.anthropic.com/news/claude-4)).
    creator_organization_name: Anthropic
    access: limited
    release_date: 2025-05-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/claude-opus-4-20250514-thinking-10k
    display_name: Claude 4 Opus (20250514, extended thinking)
    description: Claude 4 Opus is a hybrid model offering two modes - near-instant responses and extended thinking for deeper reasoning ([blog](https://www.anthropic.com/news/claude-4)). Extended thinking is enabled with 10k budget tokens.
    creator_organization_name: Anthropic
    access: limited
    release_date: 2025-05-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: anthropic/stanford-online-all-v4-s3
    display_name: Anthropic-LM v4-s3 (52B)
    description: A 52B parameter language model, trained using reinforcement learning from human feedback [paper](https://arxiv.org/pdf/2204.05862.pdf).
    creator_organization_name: Anthropic
    access: closed
    num_parameters: 52000000000
    release_date: 2021-12-01
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG]



  # Berkeley
  - name: berkeley/koala-13b # NOT SUPPORTED
    display_name: Koala (13B)
    description: Koala (13B) is a chatbot fine-tuned from Llama (13B) on dialogue data gathered from the web. ([blog post](https://bair.berkeley.edu/blog/2023/04/03/koala/))
    creator_organization_name: UC Berkeley
    access: open
    num_parameters: 13000000000
    release_date: 2022-04-03
    tags: [DEPRECATED_MODEL_TAG] # TODO: add tags



  # BigScience
  - name: bigscience/bloom
    display_name: BLOOM (176B)
    description: BLOOM (176B parameters) is an autoregressive model trained on 46 natural languages and 13 programming languages ([paper](https://arxiv.org/pdf/2211.05100.pdf)).
    creator_organization_name: BigScience
    access: open
    num_parameters: 176000000000
    release_date: 2022-06-28
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG]

  - name: bigscience/bloomz # NOT SUPPORTED
    display_name: BLOOMZ (176B)
    description: BLOOMZ (176B parameters) is BLOOM that has been fine-tuned on natural language instructions ([details](https://huggingface.co/bigscience/bloomz)).
    creator_organization_name: BigScience
    access: open
    num_parameters: 176000000000
    release_date: 2022-11-03
    tags: [DEPRECATED_MODEL_TAG] # TODO: add tags

  - name: bigscience/t0pp
    display_name: T0pp (11B)
    description: T0pp (11B parameters) is an encoder-decoder model trained on a large set of different tasks specified in natural language prompts ([paper](https://arxiv.org/pdf/2110.08207.pdf)).
    creator_organization_name: BigScience
    access: open
    num_parameters: 11000000000
    release_date: 2021-10-15
    # Does not support echo.
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, NO_NEWLINES_TAG]



  # BigCode
  - name: bigcode/santacoder
    display_name: SantaCoder (1.1B)
    description: SantaCoder (1.1B parameters) model trained on the Python, Java, and JavaScript subset of The Stack (v1.1) ([model card](https://huggingface.co/bigcode/santacoder)).
    creator_organization_name: BigCode
    access: open
    num_parameters: 1100000000
    release_date: 2023-01-09 # ArXiv submission date
    tags: [CODE_MODEL_TAG]

  - name: bigcode/starcoder
    display_name: StarCoder (15.5B)
    description: The StarCoder (15.5B parameter) model trained on 80+ programming languages from The Stack (v1.2) ([model card](https://huggingface.co/bigcode/starcoder)).
    creator_organization_name: BigCode
    access: open
    num_parameters: 15500000000
    release_date: 2023-05-09 # ArXiv submission date
    tags: [CODE_MODEL_TAG]

  # BioMistral

  - name: biomistral/biomistral-7b
    display_name: BioMistral (7B)
    description: BioMistral 7B is an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central.
    creator_organization_name: BioMistral
    access: open
    num_parameters: 7300000000
    release_date: 2024-02-15
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]




  # Cerebras Systems
  - name: cerebras/cerebras-gpt-6.7b # NOT SUPPORTED
    display_name: Cerebras GPT (6.7B)
    description: Cerebras GPT is a family of open compute-optimal language models scaled from 111M to 13B parameters trained on the Eleuther Pile. ([paper](https://arxiv.org/pdf/2304.03208.pdf))
    creator_organization_name: Cerebras
    access: limited
    num_parameters: 6700000000
    release_date: 2023-04-06
    tags: [DEPRECATED_MODEL_TAG] # TODO: add tags

  - name: cerebras/cerebras-gpt-13b # NOT SUPPORTED
    display_name: Cerebras GPT (13B)
    description: Cerebras GPT is a family of open compute-optimal language models scaled from 111M to 13B parameters trained on the Eleuther Pile. ([paper](https://arxiv.org/pdf/2304.03208.pdf))
    creator_organization_name: Cerebras
    access: limited
    num_parameters: 13000000000
    release_date: 2023-04-06
    tags: [DEPRECATED_MODEL_TAG] # TODO: add tags



  # Cohere
  # Model versioning and the possible versions are not documented here:
  # https://docs.cohere.ai/generate-reference#model-optional.
  # So, instead, we got the names of the models from the Cohere Playground.
  #
  # Note that their tokenizer and model were trained on English text and
  # they do not have a dedicated decode API endpoint, so the adaptation
  # step for language modeling fails for certain Scenarios:
  # the_pile:subset=ArXiv
  # the_pile:subset=Github
  # the_pile:subset=PubMed Central

  # TODO: Consider renaming to new model names.
  - name: cohere/xlarge-20220609
    display_name: Cohere xlarge v20220609 (52.4B)
    description: Cohere xlarge v20220609 (52.4B parameters)
    creator_organization_name: Cohere
    access: limited
    num_parameters: 52400000000
    release_date: 2022-06-09
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: cohere/large-20220720
    display_name: Cohere large v20220720 (13.1B)
    description: Cohere large v20220720 (13.1B parameters), which is deprecated by Cohere as of December 2, 2022.
    creator_organization_name: Cohere
    access: limited
    num_parameters: 13100000000
    release_date: 2022-07-20
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: cohere/medium-20220720
    display_name: Cohere medium v20220720 (6.1B)
    description: Cohere medium v20220720 (6.1B parameters)
    creator_organization_name: Cohere
    access: limited
    num_parameters: 6100000000
    release_date: 2022-07-20
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: cohere/small-20220720
    display_name: Cohere small v20220720 (410M)
    description: Cohere small v20220720 (410M parameters), which is deprecated by Cohere as of December 2, 2022.
    creator_organization_name: Cohere
    access: limited
    num_parameters: 410000000
    release_date: 2022-07-20
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: cohere/xlarge-20221108
    display_name: Cohere xlarge v20221108 (52.4B)
    description: Cohere xlarge v20221108 (52.4B parameters)
    creator_organization_name: Cohere
    access: limited
    num_parameters: 52400000000
    release_date: 2022-11-08
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: cohere/medium-20221108
    display_name: Cohere medium v20221108 (6.1B)
    description: Cohere medium v20221108 (6.1B parameters)
    creator_organization_name: Cohere
    access: limited
    num_parameters: 6100000000
    release_date: 2022-11-08
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: cohere/command-medium-beta
    display_name: Command beta (6.1B)
    description: Command beta (6.1B parameters) is fine-tuned from the medium model to respond well with instruction-like prompts ([details](https://docs.cohere.ai/docs/command-beta)).
    creator_organization_name: Cohere
    access: limited
    num_parameters: 6100000000
    release_date: 2022-11-08
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: cohere/command-xlarge-beta
    display_name: Command beta (52.4B)
    description: Command beta (52.4B parameters) is fine-tuned from the XL model to respond well with instruction-like prompts ([details](https://docs.cohere.ai/docs/command-beta)).
    creator_organization_name: Cohere
    access: limited
    num_parameters: 52400000000
    release_date: 2022-11-08
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: cohere/command
    display_name: Command
    description: Command is Cohere’s flagship text generation model. It is trained to follow user commands and to be instantly useful in practical business applications. [docs](https://docs.cohere.com/reference/generate) and [changelog](https://docs.cohere.com/changelog)
    creator_organization_name: Cohere
    access: limited
    release_date: 2023-09-29
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: cohere/command-light
    display_name: Command Light
    description: Command is Cohere’s flagship text generation model. It is trained to follow user commands and to be instantly useful in practical business applications. [docs](https://docs.cohere.com/reference/generate) and [changelog](https://docs.cohere.com/changelog)
    creator_organization_name: Cohere
    access: limited
    release_date: 2023-09-29
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: cohere/command-r
    display_name: Command R
    description: Command R is a multilingual 35B parameter model with a context length of 128K that has been trained with conversational tool use capabilities.
    creator_organization_name: Cohere
    access: open
    num_parameters: 35000000000
    release_date: 2024-03-11
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: cohere/command-r-plus
    display_name: Command R Plus
    description: Command R+ is a multilingual 104B parameter model with a context length of 128K that has been trained with conversational tool use capabilities.
    creator_organization_name: Cohere
    access: open
    num_parameters: 104000000000
    release_date: 2024-04-04
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Craiyon
  - name: craiyon/dalle-mini
    display_name: DALL-E mini (0.4B)
    description: DALL-E mini is an open-source text-to-image model that attempt to reproduce OpenAI's DALL-E 1 ([code](https://github.com/borisdayma/dalle-mini)).
    creator_organization_name: Craiyon
    access: open
    num_parameters: 400000000
    release_date: 2022-04-21
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: craiyon/dalle-mega
    display_name: DALL-E mega (2.6B)
    description: DALL-E mega is an open-source text-to-image model that attempt to reproduce OpenAI's DALL-E 1 ([code](https://github.com/borisdayma/dalle-mini)).
    creator_organization_name: Craiyon
    access: open
    num_parameters: 2600000000
    release_date: 2022-04-21
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  # DeepFloyd
  - name: DeepFloyd/IF-I-M-v1.0
    display_name: DeepFloyd IF Medium (0.4B)
    description: DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding (paper coming soon).
    creator_organization_name: DeepFloyd
    access: open
    num_parameters: 400000000
    release_date: 2023-04-28
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: DeepFloyd/IF-I-L-v1.0
    display_name: DeepFloyd IF Large (0.9B)
    description: DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding (paper coming soon).
    creator_organization_name: DeepFloyd
    access: open
    num_parameters: 900000000
    release_date: 2023-04-28
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: DeepFloyd/IF-I-XL-v1.0
    display_name: DeepFloyd IF X-Large (4.3B)
    description: DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model with state-of-the-art photorealism and language understanding (paper coming soon).
    creator_organization_name: DeepFloyd
    access: open
    num_parameters: 4300000000
    release_date: 2023-04-28
    tags: [TEXT_TO_IMAGE_MODEL_TAG]


  # Databricks
  - name: databricks/dolly-v2-3b
    display_name: Dolly V2 (3B)
    description: Dolly V2 (3B) is an instruction-following large language model trained on the Databricks machine learning platform. It is based on pythia-12b.
    creator_organization_name: Databricks
    access: open
    num_parameters: 2517652480
    release_date: 2023-04-12
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: databricks/dolly-v2-7b
    display_name: Dolly V2 (7B)
    description: Dolly V2 (7B) is an instruction-following large language model trained on the Databricks machine learning platform. It is based on pythia-12b.
    creator_organization_name: Databricks
    access: open
    num_parameters: 6444163072
    release_date: 2023-04-12
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: databricks/dolly-v2-12b
    display_name: Dolly V2 (12B)
    description: Dolly V2 (12B) is an instruction-following large language model trained on the Databricks machine learning platform. It is based on pythia-12b.
    creator_organization_name: Databricks
    access: open
    num_parameters: 11327027200
    release_date: 2023-04-12
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: databricks/dbrx-instruct
    display_name: DBRX Instruct
    description: DBRX is a large language model with a fine-grained mixture-of-experts (MoE) architecture that uses 16 experts and chooses 4. It has 132B total parameters, of which 36B parameters are active on any input. ([blog post](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm))
    creator_organization_name: Databricks
    access: open
    num_parameters: 132000000000
    release_date: 2024-03-27
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]


  # DeepMind
  - name: deepmind/gopher # NOT SUPPORTED
    display_name: Gopher (280B)
    description: Gopher (280B parameters) ([paper](https://arxiv.org/pdf/2112.11446.pdf)).
    creator_organization_name: DeepMind
    access: closed
    num_parameters: 280000000000
    release_date: 2021-12-08
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: deepmind/chinchilla # NOT SUPPORTED
    display_name: Chinchilla (70B)
    description: Chinchilla (70B parameters) ([paper](https://arxiv.org/pdf/2203.15556.pdf)).
    creator_organization_name: DeepMind
    access: closed
    num_parameters: 70000000000
    release_date: 2022-03-31
    tags: [UNSUPPORTED_MODEL_TAG]


  # Deepseek
  - name: deepseek-ai/deepseek-llm-67b-chat
    display_name: DeepSeek LLM Chat (67B)
    description: DeepSeek LLM Chat is a open-source language model trained on 2 trillion tokens in both English and Chinese, and fine-tuned supervised fine-tuning (SFT) and Direct Preference Optimization (DPO). ([paper](https://arxiv.org/abs/2401.02954))
    creator_organization_name: DeepSeek
    access: open
    num_parameters: 67000000000
    release_date: 2024-01-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: deepseek-ai/deepseek-v3
    display_name: DeepSeek v3
    description: DeepSeek v3 a Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. It adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures. ([paper](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf))
    creator_organization_name: DeepSeek
    access: open
    # NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.
    num_parameters: 685000000000
    release_date: 2024-12-24
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: deepseek-ai/deepseek-r1
    display_name: DeepSeek R1
    description: DeepSeek R1 is DeepSeek's first-generation reasoning model which incoporates which incorporates multi-stage training and cold-start data before RL. ([paper](https://arxiv.org/abs/2501.12948))
    creator_organization_name: DeepSeek
    access: open
    # NOTE: The total size of DeepSeek-R3 model1 on HuggingFace is 685B
    num_parameters: 685000000000
    release_date: 2025-01-20
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: deepseek-ai/deepseek-r1-hide-reasoning
    display_name: DeepSeek R1 (hide reasoning)
    description: DeepSeek R1 is DeepSeek's first-generation reasoning model which incoporates which incorporates multi-stage training and cold-start data before RL. ([paper](https://arxiv.org/abs/2501.12948)) The reasoning tokens are hidden from the output of the model.
    creator_organization_name: DeepSeek
    access: open
    # NOTE: The total size of DeepSeek-R3 model1 on HuggingFace is 685B
    num_parameters: 685000000000
    release_date: 2025-01-20
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
 
  - name: deepseek-ai/deepseek-r1-0528
    display_name: DeepSeek-R1-0528
    description: DeepSeek-R1-0528 is a minor version upgrade from DeepSeek R1 that has improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. ([paper](https://arxiv.org/abs/2501.12948))
    creator_organization_name: DeepSeek
    access: open
    num_parameters: 685000000000
    release_date: 2025-05-28
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
    display_name: DeepSeek-R1-Distill-Llama-8b
    description: DeepSeek-R1-Distill-Llama-8b is a model that is distilled from LLaMA 8B model for the DeepSeek-R1 task.
    creator_organization_name: DeepSeek
    access: open
    num_parameters: 8000000000
    release_date: 2025-01-20
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: deepseek-ai/deepseek-coder-6.7b-instruct
    display_name: DeepSeek-Coder-6.7b-Instruct
    description: DeepSeek-Coder-6.7b-Instruct is a model that is fine-tuned from the LLaMA 6.7B model for the DeepSeek-Coder task.
    creator_organization_name: DeepSeek
    access: open
    num_parameters: 6740000000
    release_date: 2025-01-20
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # EleutherAI
  - name: eleutherai/gpt-j-6b # Served by GooseAi, HuggingFace and Together.
    display_name: GPT-J (6B)
    description: GPT-J (6B parameters) autoregressive language model trained on The Pile ([details](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)).
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 6000000000
    release_date: 2021-06-04
    # TODO: The BUGGY_TEMP_0_TAG is a deployment related tag (Together).
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, BUGGY_TEMP_0_TAG]

  - name: eleutherai/gpt-neox-20b # Served by GooseAi and Together.
    display_name: GPT-NeoX (20B)
    description: GPT-NeoX (20B parameters) autoregressive language model trained on The Pile ([paper](https://arxiv.org/pdf/2204.06745.pdf)).
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 20000000000
    release_date: 2022-02-02
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG]

  - name: eleutherai/pythia-1b-v0
    display_name: Pythia (1B)
    description: Pythia (1B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 805736448
    release_date: 2023-02-13
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: eleutherai/pythia-2.8b-v0
    display_name: Pythia (2.8B)
    description: Pythia (2.8B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 2517652480
    release_date: 2023-02-13
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: eleutherai/pythia-6.9b
    display_name: Pythia (6.9B)
    description: Pythia (6.9B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 6444163072
    release_date: 2023-02-13
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: eleutherai/pythia-12b-v0
    display_name: Pythia (12B)
    description: Pythia (12B parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 11327027200
    release_date: 2023-02-13
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # EPFL LLM

  - name: epfl-llm/meditron-7b
    display_name: Meditron (7B)
    description: Meditron-7B is a 7 billion parameter model adapted to the medical domain from Llama-2-7B through continued pretraining on a comprehensively curated medical corpus.
    creator_organization_name: EPFL LLM
    access: open
    num_parameters: 7000000000
    release_date: 2023-11-27
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # Google
  - name: google/t5-11b
    display_name: T5 (11B)
    description: T5 (11B parameters) is an encoder-decoder model trained on a multi-task mixture, where each task is converted into a text-to-text format ([paper](https://arxiv.org/pdf/1910.10683.pdf)).
    creator_organization_name: Google
    access: open
    num_parameters: 11000000000
    release_date: 2019-10-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, NO_NEWLINES_TAG]

  - name: google/ul2
    display_name: UL2 (20B)
    description: UL2 (20B parameters) is an encoder-decoder model trained on the C4 corpus. It's similar to T5 but trained with a different objective and slightly different scaling knobs ([paper](https://arxiv.org/pdf/2205.05131.pdf)).
    creator_organization_name: Google
    access: open
    num_parameters: 20000000000
    release_date: 2022-05-10
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, NO_NEWLINES_TAG, NLG_PREFIX_TAG]

  - name: google/flan-t5-xxl
    display_name: Flan-T5 (11B)
    description: Flan-T5 (11B parameters) is T5 fine-tuned on 1.8K tasks ([paper](https://arxiv.org/pdf/2210.11416.pdf)).
    creator_organization_name: Google
    access: open
    num_parameters: 11000000000
    release_date: 2022-12-06 # Paper date
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, NO_NEWLINES_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/palm # NOT SUPPORTED
    display_name: PaLM (540B)
    description: Pathways Language Model (540B parameters) is trained using 6144 TPU v4 chips ([paper](https://arxiv.org/pdf/2204.02311.pdf)).
    creator_organization_name: Google
    access: closed
    num_parameters: 540000000000
    release_date: 2023-03-01 # was first announced on 2022-04 but remained private.
    tags: [UNSUPPORTED_MODEL_TAG]

    # Note: This is aliased to a snapshot of gemini-pro. When possible, please use a versioned snapshot instead.
  - name: google/gemini-pro
    display_name: Gemini Pro
    description: Gemini Pro is a multimodal model able to reason across text, images, video, audio and code. ([paper](https://arxiv.org/abs/2312.11805))
    creator_organization_name: Google
    access: limited
    release_date: 2023-12-13
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.0-pro-001
    display_name: Gemini 1.0 Pro (001)
    description: Gemini 1.0 Pro is a multimodal model able to reason across text, images, video, audio and code. ([paper](https://arxiv.org/abs/2312.11805))
    creator_organization_name: Google
    access: limited
    release_date: 2023-12-13
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.0-pro-002
    display_name: Gemini 1.0 Pro (002)
    description: Gemini 1.0 Pro is a multimodal model able to reason across text, images, video, audio and code. ([paper](https://arxiv.org/abs/2312.11805))
    creator_organization_name: Google
    access: limited
    release_date: 2024-04-09
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

    # Note: This is aliased to a snapshot of gemini-pro-vision. When possible, please use a versioned snapshot instead.
  - name: google/gemini-pro-vision
    display_name: Gemini Pro Vision
    description: Gemini Pro Vision is a multimodal model able to reason across text, images, video, audio and code. ([paper](https://arxiv.org/abs/2312.11805))
    creator_organization_name: Google
    access: limited
    release_date: 2023-12-13
    tags: [VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG]

  - name: google/gemini-1.0-pro-vision-001
    display_name: Gemini 1.0 Pro Vision
    description: Gemini 1.0 Pro Vision is a multimodal model able to reason across text, images, video, audio and code. ([paper](https://arxiv.org/abs/2312.11805))
    creator_organization_name: Google
    access: limited
    release_date: 2023-12-13
    tags: [VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, GOOGLE_GEMINI_PRO_VISION_V1_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/gemini-1.5-pro-001
    display_name: Gemini 1.5 Pro (001)
    description: Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-flash-001
    display_name: Gemini 1.5 Flash (001)
    description: Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-pro-preview-0409
    display_name: Gemini 1.5 Pro (0409 preview)
    description: Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-04-10
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-pro-preview-0514
    display_name: Gemini 1.5 Pro (0514 preview)
    description: Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-flash-preview-0514
    display_name: Gemini 1.5 Flash (0514 preview)
    description: Gemini 1.5 Flash is a smaller Gemini model. It has a 1 million token context window and allows interleaving text, images, audio and video as inputs. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([blog](https://blog.google/technology/developers/gemini-gemma-developer-updates-may-2024/))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-14  
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-pro-001-safety-default
    display_name: Gemini 1.5 Pro (001, default safety)
    description: Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and uses default safety settings. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-pro-001-safety-block-none
    display_name: Gemini 1.5 Pro (001, BLOCK_NONE safety)
    description: Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-flash-001-safety-default
    display_name: Gemini 1.5 Flash (001, default safety)
    description: Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and uses default safety settings. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-flash-001-safety-block-none
    display_name: Gemini 1.5 Flash (001, BLOCK_NONE safety)
    description: Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-05-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-pro-002
    display_name: Gemini 1.5 Pro (002)
    description: Gemini 1.5 Pro is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-09-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-flash-002
    display_name: Gemini 1.5 Flash (002)
    description: Gemini 1.5 Flash is a multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from long contexts. This model is accessed through Vertex AI and has all safety thresholds set to `BLOCK_NONE`. ([paper](https://arxiv.org/abs/2403.05530))
    creator_organization_name: Google
    access: limited
    release_date: 2024-09-24
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.0-flash-exp
    display_name: Gemini 2.0 Flash (Experimental)
    description: Gemini 2.0 Flash (Experimental) is a Gemini model that supports multimodal inputs like images, video and audio, as well as multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. ([blog](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash))
    creator_organization_name: Google
    access: limited
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-1.5-flash-8b-001
    display_name: Gemini 1.5 Flash 8B
    description: Gemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2024-10-01
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.0-flash-001
    display_name: Gemini 2.0 Flash
    description: Gemini 2.0 Flash ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-02-01
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.0-flash-lite-preview-02-05
    display_name: Gemini 2.0 Flash Lite (02-05 preview)
    description: Gemini 2.0 Flash Lite (02-05 preview) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-02-05
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.0-flash-lite-001
    display_name: Gemini 2.0 Flash Lite
    description: Gemini 2.0 Flash Lite ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-03-25
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.0-flash-thinking-exp-01-21
    display_name: Gemini 2.0 Flash Thinking (01-21 preview)
    description: Gemini 2.0 Flash Thinking (01-21 preview) ([documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/thinking))
    creator_organization_name: Google
    access: limited
    release_date: 2025-01-21
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.0-pro-exp-02-05
    display_name: Gemini 2.0 Pro (02-05 preview)
    description: Gemini 2.0 Pro (02-05 preview) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-02-05
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-flash-lite-preview-06-17
    display_name: Gemini 2.5 Flash-Lite (06-17 preview)
    description: Gemini 2.5 Flash-Lite (06-17 preview) ([blog](https://blog.google/products/gemini/gemini-2-5-model-family-expands/))
    creator_organization_name: Google
    access: limited
    release_date: 2025-06-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-flash-lite
    display_name: Gemini 2.5 Flash-Lite
    description: Gemini 2.5 Flash-Lite ([blog](https://blog.google/products/gemini/gemini-2-5-model-family-expands/))
    creator_organization_name: Google
    access: limited
    release_date: 2025-07-22
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-flash-preview-04-17
    display_name: Gemini 2.5 Flash (04-17 preview)
    description: Gemini 2.5 Flash (04-17 preview) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-04-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-flash-preview-05-20
    display_name: Gemini 2.5 Flash (05-20 preview)
    description: Gemini 2.5 Flash (05-20 preview) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-04-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-flash
    display_name: Gemini 2.5 Flash
    description: Gemini 2.5 Flash ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-06-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-pro-exp-03-25
    display_name: Gemini 2.5 Pro (03-25 experimental)
    description: Gemini 2.5 Pro (03-25 experimental) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-03-25
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-pro-preview-03-25
    display_name: Gemini 2.5 Pro (03-25 preview)
    description: Gemini 2.5 Pro (03-25 preview) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-04-09  # source: https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-pro-preview-05-06
    display_name: Gemini 2.5 Pro (05-06 preview)
    description: Gemini 2.5 Pro (05-06 preview) ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-05-06  # source: https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemini-2.5-pro
    display_name: Gemini 2.5 Pro
    description: Gemini 2.5 Pro ([documentation](https://ai.google.dev/gemini-api/docs/models/gemini))
    creator_organization_name: Google
    access: limited
    release_date: 2025-06-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, AUDIO_LANGUAGE_MODEL_TAG, GOOGLE_GEMINI_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemma-2b
    display_name: Gemma (2B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/gemma-open-models/))
    creator_organization_name: Google
    access: open
    release_date: 2024-02-21
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/gemma-2b-it
    display_name: Gemma Instruct (2B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/gemma-open-models/))
    creator_organization_name: Google
    access: open
    release_date: 2024-02-21
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMMA_INSTRUCT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemma-7b
    display_name: Gemma (7B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/gemma-open-models/))
    creator_organization_name: Google
    access: open
    release_date: 2024-02-21
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/gemma-7b-it
    display_name: Gemma Instruct (7B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/gemma-open-models/))
    creator_organization_name: Google
    access: open
    release_date: 2024-02-21
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMMA_INSTRUCT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemma-2-9b
    display_name: Gemma 2 (9B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/google-gemma-2/))
    creator_organization_name: Google
    access: open
    release_date: 2024-06-27
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/gemma-2-9b-it
    display_name: Gemma 2 Instruct (9B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/google-gemma-2/))
    creator_organization_name: Google
    access: open
    release_date: 2024-06-27
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMMA_INSTRUCT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/gemma-2-27b
    display_name: Gemma 2 (27B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/google-gemma-2/))
    creator_organization_name: Google
    access: open
    release_date: 2024-06-27
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/gemma-2-27b-it
    display_name: Gemma 2 Instruct (27B)
    description: Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models. ([model card](https://www.kaggle.com/models/google/gemma), [blog post](https://blog.google/technology/developers/google-gemma-2/))
    creator_organization_name: Google
    access: open
    release_date: 2024-06-27
    tags: [TEXT_MODEL_TAG, GOOGLE_GEMMA_INSTRUCT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: google/paligemma-3b-mix-224
    display_name: PaliGemma (3B) Mix 224
    description: PaliGemma is a versatile and lightweight vision-language model (VLM) inspired by PaLI-3 and based on open components such as the SigLIP vision model and the Gemma language model. Pre-trained with 224x224 input images and 128 token input/output text sequences. Finetuned on a mixture of downstream academic datasets. ([blog](https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/))
    creator_organization_name: Google
    access: open
    release_date: 2024-05-12
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/paligemma-3b-mix-448
    display_name: PaliGemma (3B) Mix 448
    description: PaliGemma is a versatile and lightweight vision-language model (VLM) inspired by PaLI-3 and based on open components such as the SigLIP vision model and the Gemma language model. Pre-trained with 448x448 input images and 512 token input/output text sequences. Finetuned on a mixture of downstream academic datasets. ([blog](https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/))
    creator_organization_name: Google
    access: open
    release_date: 2024-05-12
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/text-bison@001
    display_name: PaLM-2 (Bison)
    description: The best value PaLM model. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-06-07 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#model_versions
    tags: [TEXT_MODEL_TAG, GOOGLE_PALM_2_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/text-bison@002
    display_name: PaLM-2 (Bison)
    description: The best value PaLM model. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-06-07 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#model_versions
    tags: [TEXT_MODEL_TAG, GOOGLE_PALM_2_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/text-bison-32k
    display_name: PaLM-2 (Bison)
    description: The best value PaLM model with a 32K context. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-06-07 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#model_versions
    tags: [TEXT_MODEL_TAG, GOOGLE_PALM_2_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/text-unicorn@001
    display_name: PaLM-2 (Unicorn)
    description: The largest model in PaLM family. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-11-30 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#model_versions
    tags: [TEXT_MODEL_TAG, GOOGLE_PALM_2_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/code-bison@001
    display_name: Codey PaLM-2 (Bison)
    description: A model fine-tuned to generate code based on a natural language description of the desired code. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-06-29 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-generation#model_versions
    tags: [CODE_MODEL_TAG]

  - name: google/code-bison@002
    display_name: Codey PaLM-2 (Bison)
    description: A model fine-tuned to generate code based on a natural language description of the desired code. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-06-29 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-generation#model_versions
    tags: [CODE_MODEL_TAG]

  - name: google/code-bison-32k
    display_name: Codey PaLM-2 (Bison)
    description: Codey with a 32K context. PaLM 2 (Pathways Language Model) is a Transformer-based model trained using a mixture of objectives that was evaluated on English and multilingual language, and reasoning tasks. ([report](https://arxiv.org/pdf/2305.10403.pdf))
    creator_organization_name: Google
    access: limited
    release_date: 2023-06-29 # Source: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-generation#model_versions
    tags: [CODE_MODEL_TAG]

  - name: google/medlm-medium
    display_name: MedLM (Medium)
    description: MedLM is a family of foundation models fine-tuned for the healthcare industry based on Google Research's medically-tuned large language model, Med-PaLM 2. ([documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/medlm/overview))
    creator_organization_name: Google
    access: limited
    release_date: 2023-12-13
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: google/medlm-large
    display_name: MedLM (Large)
    description: MedLM is a family of foundation models fine-tuned for the healthcare industry based on Google Research's medically-tuned large language model, Med-PaLM 2. ([documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/medlm/overview))
    creator_organization_name: Google
    access: limited
    release_date: 2023-12-13
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # HuggingFace
  - name: HuggingFaceM4/idefics2-8b
    display_name: IDEFICS 2 (8B)
    description: IDEFICS 2 (8B parameters) is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text outputs. ([blog](https://huggingface.co/blog/idefics2)).
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 8000000000
    release_date: 2024-04-15
    tags: [VISION_LANGUAGE_MODEL_TAG, IDEFICS_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: HuggingFaceM4/idefics-9b
    display_name: IDEFICS (9B)
    description: IDEFICS (9B parameters) is an open-source model based on DeepMind's Flamingo ([blog](https://huggingface.co/blog/idefics)).
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 9000000000
    release_date: 2023-08-22
    tags: [VISION_LANGUAGE_MODEL_TAG, IDEFICS_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: HuggingFaceM4/idefics-9b-instruct
    display_name: IDEFICS-instruct (9B)
    description: IDEFICS-instruct (9B parameters) is the instruction-tuned version of IDEFICS 9B ([blog](https://huggingface.co/blog/idefics)).
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 9000000000
    release_date: 2023-08-22
    tags: [VISION_LANGUAGE_MODEL_TAG, IDEFICS_MODEL_TAG, IDEFICS_INSTRUCT_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: HuggingFaceM4/idefics-80b
    display_name: IDEFICS (80B)
    description: IDEFICS (80B parameters) is an open-source model based on DeepMind's Flamingo ([blog](https://huggingface.co/blog/idefics)).
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 80000000000
    release_date: 2023-08-22
    tags: [VISION_LANGUAGE_MODEL_TAG, IDEFICS_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: HuggingFaceM4/idefics-80b-instruct
    display_name: IDEFICS-instruct (80B)
    description: IDEFICS-instruct (80B parameters) is the instruction-tuned version of IDEFICS 80B ([blog](https://huggingface.co/blog/idefics)).
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 80000000000
    release_date: 2023-08-22
    tags: [VISION_LANGUAGE_MODEL_TAG, IDEFICS_MODEL_TAG, IDEFICS_INSTRUCT_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: huggingface/smollm2-135m
    display_name: SmolLM2 (135M)
    description: SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. ([paper](https://arxiv.org/abs/2502.02737v1))
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 135000000
    release_date: 2024-10-31
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: huggingface/smollm2-360m
    display_name: SmolLM2 (360M)
    description: SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. ([paper](https://arxiv.org/abs/2502.02737v1))
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 362000000
    release_date: 2024-10-31
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: huggingface/smollm2-1.7b
    display_name: SmolLM2 (1.7B)
    description: SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. ([paper](https://arxiv.org/abs/2502.02737v1))
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 1710000000
    release_date: 2024-10-31
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: huggingface/smollm2-135m-instruct
    display_name: SmolLM2 Instruct (135M)
    description: SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. ([paper](https://arxiv.org/abs/2502.02737v1))
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 135000000
    release_date: 2024-10-31
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: huggingface/smollm2-360m-instruct
    display_name: SmolLM2 Instruct (360M)
    description: SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. ([paper](https://arxiv.org/abs/2502.02737v1))
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 362000000
    release_date: 2024-10-31
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: huggingface/smollm2-1.7b-instruct
    display_name: SmolLM2 Instruct (1.7B)
    description: SmolLM2 is a family of compact language models that are capable of solving a wide range of tasks while being lightweight enough to run on-device. ([paper](https://arxiv.org/abs/2502.02737v1))
    creator_organization_name: HuggingFace
    access: open
    num_parameters: 1710000000
    release_date: 2024-10-31
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  ## Text-to-Image Diffusion Models
  - name: huggingface/dreamlike-diffusion-v1-0
    display_name: Dreamlike Diffusion v1.0 (1B)
    description: Dreamlike Diffusion v1.0 is Stable Diffusion v1.5 fine tuned on high quality art ([HuggingFace model card](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0))
    creator_organization_name: dreamlike.art
    access: open
    num_parameters: 1000000000
    release_date: 2023-03-08
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/dreamlike-photoreal-v2-0
    display_name: Dreamlike Photoreal v2.0 (1B)
    description: Dreamlike Photoreal v2.0 is a photorealistic model based on Stable Diffusion v1.5 ([HuggingFace model card](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0))
    creator_organization_name: dreamlike.art
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-23
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/openjourney-v1-0
    display_name: Openjourney (1B)
    description: Openjourney is an open source Stable Diffusion fine tuned model on Midjourney images ([HuggingFace model card](https://huggingface.co/prompthero/openjourney))
    creator_organization_name: PromptHero
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-01  # TODO: get the exact date
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/openjourney-v2-0
    display_name: Openjourney v2 (1B)
    description: Openjourney v2 is an open source Stable Diffusion fine tuned model on Midjourney images. Openjourney v2 is now referred to as Openjourney v4 in Hugging Face ([HuggingFace model card](https://huggingface.co/prompthero/openjourney-v4)).
    creator_organization_name: PromptHero
    access: open
    num_parameters: 1000000000
    release_date: 2023-01-01  # TODO: get the exact date
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/promptist-stable-diffusion-v1-4
    display_name: Promptist + Stable Diffusion v1.4 (1B)
    description: Trained with human preferences, Promptist optimizes user input into model-preferred prompts for Stable Diffusion v1.4 ([paper](https://arxiv.org/abs/2212.09611))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 1000000000
    release_date: 2022-12-19
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/redshift-diffusion
    display_name: Redshift Diffusion (1B)
    description: Redshift Diffusion is an open source Stable Diffusion model fine tuned on high resolution 3D artworks ([HuggingFace model card](https://huggingface.co/nitrosocke/redshift-diffusion))
    creator_organization_name: nitrosocke
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-29
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-safe-weak
    display_name: Safe Stable Diffusion weak (1B)
    description: Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content ([paper](https://arxiv.org/abs/2211.05105)).
    creator_organization_name: TU Darmstadt
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-09
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-safe-medium
    display_name: Safe Stable Diffusion medium (1B)
    description: Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content ([paper](https://arxiv.org/abs/2211.05105))
    creator_organization_name: TU Darmstadt
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-09
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-safe-strong
    display_name: Safe Stable Diffusion strong (1B)
    description: Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content ([paper](https://arxiv.org/abs/2211.05105))
    creator_organization_name: TU Darmstadt
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-09
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-safe-max
    display_name: Safe Stable Diffusion max (1B)
    description: Safe Stable Diffusion is an extension to the Stable Diffusion that drastically reduces inappropriate content ([paper](https://arxiv.org/abs/2211.05105))
    creator_organization_name: TU Darmstadt
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-09
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-v1-4
    display_name: Stable Diffusion v1.4 (1B)
    description: Stable Diffusion v1.4 is a latent text-to-image diffusion model capable of generating photorealistic images given any text input ([paper](https://arxiv.org/abs/2112.10752))
    creator_organization_name: Ludwig Maximilian University of Munich CompVis
    access: open
    num_parameters: 1000000000
    release_date: 2022-08-01
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-v1-5
    display_name: Stable Diffusion v1.5 (1B)
    description: The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling ([paper](https://arxiv.org/abs/2112.10752))
    creator_organization_name: Runway
    access: open
    num_parameters: 1000000000
    release_date: 2022-10-20
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-v2-base
    display_name: Stable Diffusion v2 base (1B)
    description: The model is trained from scratch 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score greater than 4.5. Then it is further trained for 850k steps at resolution 512x512 on the same dataset on images with resolution greater than 512x512 ([paper](https://arxiv.org/abs/2112.10752))
    creator_organization_name: Stability AI
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-23
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/stable-diffusion-v2-1-base
    display_name: Stable Diffusion v2.1 base (1B)
    description: This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base with 220k extra steps taken, with punsafe=0.98 on the same dataset ([paper](https://arxiv.org/abs/2112.10752))
    creator_organization_name: Stability AI
    access: open
    num_parameters: 1000000000
    release_date: 2022-11-23
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: huggingface/vintedois-diffusion-v0-1
    display_name: Vintedois (22h) Diffusion model v0.1 (1B)
    description: Vintedois (22h) Diffusion model v0.1 is Stable Diffusion v1.5 that was finetuned on a large amount of high quality images with simple prompts to generate beautiful images without a lot of prompt engineering ([HuggingFace model card](https://huggingface.co/22h/vintedois-diffusion-v0-1))
    creator_organization_name: 22 Hours
    access: open
    num_parameters: 1000000000
    release_date: 2022-12-27
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: segmind/Segmind-Vega
    display_name: Segmind Stable Diffusion (0.74B)
    description: The Segmind-Vega Model is a distilled version of the Stable Diffusion XL (SDXL), offering a remarkable 70% reduction in size and an impressive 100% speedup while retaining high-quality text-to-image generation capabilities. Trained on diverse datasets, including Grit and Midjourney scrape data, it excels at creating a wide range of visual content based on textual prompts. ([HuggingFace model card](https://huggingface.co/segmind/Segmind-Vega))
    creator_organization_name: Segmind
    access: open
    num_parameters: 740000000
    release_date: 2023-12-01
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: segmind/SSD-1B
    display_name: Segmind Stable Diffusion (1B)
    description: The Segmind Stable Diffusion Model (SSD-1B) is a distilled 50% smaller version of the Stable Diffusion XL (SDXL), offering a 60% speedup while maintaining high-quality text-to-image generation capabilities. It has been trained on diverse datasets, including Grit and Midjourney scrape data, to enhance its ability to create a wide range of visual content based on textual prompts. ([HuggingFace model card](https://huggingface.co/segmind/SSD-1B))
    creator_organization_name: Segmind
    access: open
    num_parameters: 1000000000
    release_date: 2023-10-20
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: stabilityai/stable-diffusion-xl-base-1.0
    display_name: Stable Diffusion XL
    description: Stable Diffusion XL (SDXL) consists of an ensemble of experts pipeline for latent diffusion. ([HuggingFace model card](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0))
    creator_organization_name: Stability AI
    access: open
    num_parameters: 6600000000
    release_date: 2023-07-26
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  # Kakao
  - name: kakaobrain/mindall-e
    display_name: minDALL-E (1.3B)
    description: minDALL-E, named after minGPT, is an autoregressive text-to-image generation model trained on 14 million image-text pairs ([code](https://github.com/kakaobrain/minDALL-E))
    creator_organization_name: Kakao
    access: open
    num_parameters: 1300000000
    release_date: 2021-12-13
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  # Lexica
  - name: lexica/search-stable-diffusion-1.5
    display_name: Lexica Search with Stable Diffusion v1.5 (1B)
    description: Retrieves Stable Diffusion v1.5 images Lexica users generated ([docs](https://lexica.art/docs)).
    creator_organization_name: Lexica
    access: open
    release_date: 2023-01-01
    tags: [TEXT_TO_IMAGE_MODEL_TAG]


  # Lightning AI
  - name: lightningai/lit-gpt
    display_name: Lit-GPT
    description: Lit-GPT is an optimized collection of open-source LLMs for finetuning and inference. It supports – Falcon, Llama 2, Vicuna, LongChat, and other top-performing open-source large language models.
    creator_organization_name: Lightning AI
    access: open
    release_date: 2023-04-04
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]



  # LMSYS
  - name: lmsys/vicuna-7b-v1.3
    display_name: Vicuna v1.3 (7B)
    description: Vicuna v1.3 (7B) is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
    creator_organization_name: LMSYS
    access: open
    num_parameters: 7000000000
    release_date: 2023-06-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: lmsys/vicuna-13b-v1.3
    display_name: Vicuna v1.3 (13B)
    description: Vicuna v1.3 (13B) is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
    creator_organization_name: LMSYS
    access: open
    num_parameters: 13000000000
    release_date: 2023-06-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Marin Community
  - name: marin-community/marin-8b-instruct
    display_name: Marin 8B Instruct
    description: Marin 8B Instruct is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
    creator_organization_name: Marin Community
    access: open
    num_parameters: 8030000000
    release_date: 2025-05-15
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Meta
  - name: meta/opt-iml-175b # NOT SUPPORTED
    display_name: OPT-IML (175B)
    description: OPT-IML (175B parameters) is a suite of decoder-only transformer LMs that are multi-task fine-tuned on 2000 datasets ([paper](https://arxiv.org/pdf/2212.12017.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 175000000000
    release_date: 2022-12-22
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: meta/opt-iml-30b # NOT SUPPORTED
    display_name: OPT-IML (30B)
    description: OPT-IML (30B parameters) is a suite of decoder-only transformer LMs that are multi-task fine-tuned on 2000 datasets ([paper](https://arxiv.org/pdf/2212.12017.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 30000000000
    release_date: 2022-12-22
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: meta/opt-175b
    display_name: OPT (175B)
    description: Open Pre-trained Transformers (175B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers ([paper](https://arxiv.org/pdf/2205.01068.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 175000000000
    release_date: 2022-05-02
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG]

  - name: meta/opt-66b
    display_name: OPT (66B)
    description: Open Pre-trained Transformers (66B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers ([paper](https://arxiv.org/pdf/2205.01068.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 66000000000
    release_date: 2022-05-02
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG]

  - name: meta/opt-6.7b
    display_name: OPT (6.7B)
    description: Open Pre-trained Transformers (6.7B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers ([paper](https://arxiv.org/pdf/2205.01068.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 6700000000
    release_date: 2022-05-02
    # TODO: The BUGGY_TEMP_0_TAG is a deployment related tag (Together).
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, BUGGY_TEMP_0_TAG]

  - name: meta/opt-1.3b
    display_name: OPT (1.3B)
    description: Open Pre-trained Transformers (1.3B parameters) is a suite of decoder-only pre-trained transformers that are fully and responsibly shared with interested researchers ([paper](https://arxiv.org/pdf/2205.01068.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 1300000000
    release_date: 2022-05-02
    # TODO: The BUGGY_TEMP_0_TAG is a deployment related tag (Together).
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, BUGGY_TEMP_0_TAG]

  - name: meta/galactica-120b # NOT SUPPORTED
    display_name: Galactica (120B)
    description: Galactica (120B parameters) is trained on 48 million papers, textbooks, lectures notes, compounds and proteins, scientific websites, etc. ([paper](https://galactica.org/static/paper.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 120000000000
    release_date: 2022-11-15
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: meta/galactica-30b # NOT SUPPORTED
    display_name: Galactica (30B)
    description: Galactica (30B parameters) is trained on 48 million papers, textbooks, lectures notes, compounds and proteins, scientific websites, etc. ([paper](https://galactica.org/static/paper.pdf)).
    creator_organization_name: Meta
    access: open
    num_parameters: 30000000000
    release_date: 2022-11-15
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: meta/llama-7b
    display_name: LLaMA (7B)
    description: LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.
    creator_organization_name: Meta
    access: open
    num_parameters: 7000000000
    release_date: 2023-02-24
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-13b
    display_name: LLaMA (13B)
    description: LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.
    creator_organization_name: Meta
    access: open
    num_parameters: 13000000000
    release_date: 2023-02-24
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-30b
    display_name: LLaMA (30B)
    description: LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.
    creator_organization_name: Meta
    access: open
    num_parameters: 30000000000
    release_date: 2023-02-24
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-65b
    display_name: LLaMA (65B)
    description: LLaMA is a collection of foundation language models ranging from 7B to 65B parameters.
    creator_organization_name: Meta
    access: open
    num_parameters: 65000000000
    release_date: 2023-02-24
    # TODO(#1828): Upgrade to FULL_FUNCTIONALITY_TEXT_MODEL_TAG
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-2-7b
    display_name: Llama 2 (7B)
    description: Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.
    creator_organization_name: Meta
    access: open
    num_parameters: 7000000000
    release_date: 2023-07-18
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-2-13b
    display_name: Llama 2 (13B)
    description: Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.
    creator_organization_name: Meta
    access: open
    num_parameters: 13000000000
    release_date: 2023-07-18
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-2-70b
    display_name: Llama 2 (70B)
    description: Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1.
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2023-07-18
    # TODO(#1828): Upgrade to FULL_FUNCTIONALITY_TEXT_MODEL_TAG
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-3-8b
    display_name: Llama 3 (8B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-04-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: meta/llama-3-8b-instruct-turbo
    display_name: Llama 3 Instruct Turbo (8B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Turbo is Together's implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-07-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3-8b-instruct-lite
    display_name: Llama 3 Instruct Lite (8B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Lite is Together's implementation, it leverages a number of optimizations including INT4 quantization, provides the most cost-efficient and scalable Llama 3 models available anywhere, while maintaining excellent quality relative to full precision reference implementations ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-07-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
    
  - name: meta/llama-3-70b
    display_name: Llama 3 (70B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-04-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]
    
  - name: meta/llama-3-70b-instruct-turbo
    display_name: Llama 3 Instruct Turbo (70B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Turbo is Together's implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-07-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
    
  - name: meta/llama-3-70b-instruct-lite
    display_name: Llama 3 Instruct Lite (70B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/) Lite is Together's implementation, it leverages a number of optimizations including INT4 quantization, provides the most cost-efficient and scalable Llama 3 models available anywhere, while maintaining excellent quality relative to full precision reference implementations ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-07-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.1-8b-instruct
    display_name: Llama 3.1 Instruct (8B)
    description: Llama 3.1 (8B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/))
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.1-70b-instruct
    display_name: Llama 3.1 Instruct (70B)
    description: Llama 3.1 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/))
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.1-405b-instruct
    display_name: Llama 3.1 Instruct (405B)
    description: Llama 3.1 (405B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/))
    creator_organization_name: Meta
    access: open
    num_parameters: 405000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.1-8b-instruct-turbo
    display_name: Llama 3.1 Instruct Turbo (8B)
    description: Llama 3.1 (8B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/), [blog](https://ai.meta.com/blog/meta-llama-3-1/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.1-70b-instruct-turbo
    display_name: Llama 3.1 Instruct Turbo (70B)
    description: Llama 3.1 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/), [blog](https://ai.meta.com/blog/meta-llama-3-1/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.1-405b-instruct-turbo
    display_name: Llama 3.1 Instruct Turbo (405B)
    description: Llama 3.1 (405B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/), [blog](https://ai.meta.com/blog/meta-llama-3-1/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 405000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.2-1b-instruct
    display_name: Llama 3.2 Instruct (1.23B)
    description: The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned text-only generative models in 1B and 3B sizes. ([blog](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/))
    creator_organization_name: Meta
    access: open
    num_parameters: 1230000000
    release_date: 2024-09-25
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.2-3b-instruct-turbo
    display_name: Llama 3.2 Instruct Turbo (3B)
    description: The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned text-only generative models in 1B and 3B sizes. ([blog](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 3210000000
    release_date: 2024-09-25
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.2-11b-vision-instruct-turbo
    display_name: Llama 3.2 Vision Instruct Turbo (11B)
    description: The Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. ([blog](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 10700000000
    release_date: 2024-09-25
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.2-90b-vision-instruct-turbo
    display_name: Llama 3.2 Vision Instruct Turbo (90B)
    description: The Llama 3.2 Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes. ([blog](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 88600000000
    release_date: 2024-09-25
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3.3-70b-instruct-turbo
    display_name: Llama 3.3 Instruct Turbo (70B)
    description: Llama 3.3 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)) Turbo is Together's implementation, providing a near negligible difference in quality from the reference implementation with faster performance and lower cost, currently using FP8 quantization. ([blog](https://www.together.ai/blog/llama-31-quality))
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-12-06
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
  
  - name: meta/llama-3.3-70b-instruct
    display_name: Llama 3.3 Instruct (70B)
    description: Llama 3.3 (70B) is part of the Llama 3 family of dense Transformer models that that natively support multilinguality, coding, reasoning, and tool usage. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/))
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-12-06
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-4-scout-17b-16e-instruct
    display_name: Llama 4 Scout (17Bx16E) Instruct
    description: Llama 4 Scout (17Bx16E) Instruct is part of the Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences using a mixture-of-experts architecture. ([blog](https://ai.meta.com/blog/llama-4-multimodal-intelligence/))
    creator_organization_name: Meta
    access: open
    num_parameters: 109000000000
    release_date: 2025-04-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-4-maverick-17b-128e-instruct-fp8
    display_name: Llama 4 Maverick (17Bx128E) Instruct FP8
    description: Llama 4 Maverick (17Bx128E) Instruct FP8 is part of the Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences using a mixture-of-experts architecture. ([blog](https://ai.meta.com/blog/llama-4-multimodal-intelligence/))
    creator_organization_name: Meta
    access: open
    num_parameters: 402000000000
    release_date: 2025-04-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3-8b-chat
    display_name: Llama 3 Instruct (8B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. It used SFT, rejection sampling, PPO and DPO for post-training. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-04-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-3-70b-chat
    display_name: Llama 3 Instruct (70B)
    description: Llama 3 is a family of language models that have been trained on more than 15 trillion tokens, and use Grouped-Query Attention (GQA) for improved inference scalability. It used SFT, rejection sampling, PPO and DPO for post-training. ([paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/)
    creator_organization_name: Meta
    access: open
    num_parameters: 70000000000
    release_date: 2024-04-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-guard-7b
    display_name: Llama Guard (7B)
    description: Llama-Guard is a 7B parameter Llama 2-based input-output safeguard model. It can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM it generates text in its output that indicates whether a given prompt or response is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories.
    creator_organization_name: Meta
    access: open
    num_parameters: 7000000000
    release_date: 2023-12-07
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-guard-2-8b
    display_name: Llama Guard 2 (8B)
    description: Llama Guard 2 is an 8B parameter Llama 3-based LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-04-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: meta/llama-guard-3-8b
    display_name: Llama Guard 3 (8B)
    description: Llama Guard 3 is an 8B parameter Llama 3.1-based LLM safeguard model. Similar to Llama Guard, it can be used for classifying content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.
    creator_organization_name: Meta
    access: open
    num_parameters: 8000000000
    release_date: 2024-07-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]


  # Microsoft/NVIDIA
  - name: microsoft/TNLGv2_530B
    display_name: TNLG v2 (530B)
    description: TNLG v2 (530B parameters) autoregressive language model trained on a filtered subset of the Pile and CommonCrawl ([paper](https://arxiv.org/pdf/2201.11990.pdf)).
    creator_organization_name: Microsoft/NVIDIA
    access: closed
    num_parameters: 530000000000
    release_date: 2022-01-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: microsoft/TNLGv2_7B
    display_name: TNLG v2 (6.7B)
    description: TNLG v2 (6.7B parameters) autoregressive language model trained on a filtered subset of the Pile and CommonCrawl ([paper](https://arxiv.org/pdf/2201.11990.pdf)).
    creator_organization_name: Microsoft/NVIDIA
    access: closed
    num_parameters: 6700000000
    release_date: 2022-01-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: microsoft/llava-1.5-7b-hf
    display_name: LLaVA 1.5 (7B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 7000000000
    release_date: 2023-10-05
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: microsoft/llava-1.5-13b-hf
    display_name: LLaVA 1.5 (13B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 13000000000
    release_date: 2023-10-05
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: uw-madison/llava-v1.6-vicuna-7b-hf
    display_name: LLaVA 1.6 (7B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 7000000000
    release_date: 2024-01-01
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: uw-madison/llava-v1.6-vicuna-13b-hf
    display_name: LLaVA 1.6 (13B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 13000000000
    release_date: 2024-01-01
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: uw-madison/llava-v1.6-mistral-7b-hf
    display_name: LLaVA 1.6 + Mistral (7B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 7000000000
    release_date: 2024-01-01
    tags: [ VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG ]

  - name: uw-madison/llava-v1.6-34b-hf
    display_name: LLaVA + Nous-Hermes-2-Yi-34B (34B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 34000000000
    release_date: 2024-01-01
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]
  
  - name: openflamingo/OpenFlamingo-9B-vitl-mpt7b
    display_name: OpenFlamingo (9B)
    description: OpenFlamingo is an open source implementation of DeepMind's Flamingo models. This 9B-parameter model uses a CLIP ViT-L/14 vision encoder and MPT-7B language model ([paper](https://arxiv.org/abs/2308.01390)).
    creator_organization_name: OpenFlamingo
    access: open
    num_parameters: 9000000000
    release_date: 2023-08-02
    tags: [VISION_LANGUAGE_MODEL_TAG, OPEN_FLAMINGO_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: microsoft/phi-2
    display_name: Phi-2
    description: Phi-2 is a Transformer with 2.7 billion parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value)
    creator_organization_name: Microsoft
    access: open
    num_parameters: 13000000000
    release_date: 2023-10-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: microsoft/phi-3-small-8k-instruct
    display_name: Phi-3 (7B)
    description: Phi-3-Small-8K-Instruct is a lightweight model trained with synthetic data and filtered publicly available website data with a focus on high-quality and reasoning dense properties. ([paper](https://arxiv.org/abs/2404.14219), [blog](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 7000000000
    release_date: 2024-05-21
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: microsoft/phi-3-medium-4k-instruct
    display_name: Phi-3 (14B)
    description: Phi-3-Medium-4K-Instruct is a lightweight model trained with synthetic data and filtered publicly available website data with a focus on high-quality and reasoning dense properties. ([paper](https://arxiv.org/abs/2404.14219), [blog](https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 14000000000
    release_date: 2024-05-21
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
  
  - name: microsoft/phi-3.5-mini-instruct
    display_name: Phi-3.5-mini-instruct (3.8B)
    description: Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites. ([paper](https://arxiv.org/abs/2404.14219), [blog](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 3800000000
    release_date: 2024-08-22
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: microsoft/phi-3.5-moe-instruct
    display_name: Phi-3.5 MoE
    description: Phi-3.5 MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. ([paper](https://arxiv.org/abs/2404.14219), [blog](https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280))
    creator_organization_name: Microsoft
    access: open
    num_parameters: 41900000000
    release_date: 2024-08-22
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # KAIST AI
  - name: kaistai/prometheus-vision-13b-v1.0-hf
    display_name: LLaVA + Vicuna-v1.5 (13B)
    description: LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. ([paper](https://arxiv.org/abs/2304.08485))
    creator_organization_name: KAIST AI
    access: open
    num_parameters: 13000000000
    release_date: 2024-01-01
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  # 01.AI
  - name: 01-ai/yi-6b
    display_name: Yi (6B)
    description: The Yi models are large language models trained from scratch by developers at 01.AI.
    creator_organization_name: 01.AI
    access: open
    num_parameters: 6000000000
    release_date: 2023-11-02
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: 01-ai/yi-34b
    display_name: Yi (34B)
    description: The Yi models are large language models trained from scratch by developers at 01.AI.
    creator_organization_name: 01.AI
    access: open
    num_parameters: 34000000000
    release_date: 2023-11-02
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: 01-ai/yi-6b-chat
    display_name: Yi Chat (6B)
    description: The Yi models are large language models trained from scratch by developers at 01.AI.
    creator_organization_name: 01.AI
    access: open
    num_parameters: 6000000000
    release_date: 2023-11-23
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: 01-ai/yi-34b-chat
    display_name: Yi Chat (34B)
    description: The Yi models are large language models trained from scratch by developers at 01.AI.
    creator_organization_name: 01.AI
    access: open
    num_parameters: 34000000000
    release_date: 2023-11-23
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: 01-ai/yi-large
    display_name: Yi Large
    description: The Yi models are large language models trained from scratch by developers at 01.AI. ([tweet](https://x.com/01AI_Yi/status/1789894091620458667))
    creator_organization_name: 01.AI
    access: limited
    release_date: 2024-05-12
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: 01-ai/yi-large-preview
    display_name: Yi Large (Preview)
    description: The Yi models are large language models trained from scratch by developers at 01.AI. ([tweet](https://x.com/01AI_Yi/status/1789894091620458667))
    creator_organization_name: 01.AI
    access: limited
    release_date: 2024-05-12
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # Allen Institute for AI
  # OLMo Blog: https://blog.allenai.org/olmo-open-language-model-87ccfc95f580
  - name: allenai/olmo-7b
    display_name: OLMo (7B)
    description: OLMo is a series of Open Language Models trained on the Dolma dataset.
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 7000000000
    release_date: 2024-02-01
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: allenai/olmo-7b-twin-2t
    display_name: OLMo (7B Twin 2T)
    description: OLMo is a series of Open Language Models trained on the Dolma dataset.
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 7000000000
    release_date: 2024-02-01
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: allenai/olmo-7b-instruct
    display_name: OLMo (7B Instruct)
    description: OLMo is a series of Open Language Models trained on the Dolma dataset. The instruct versions was trained on the Tulu SFT mixture and a cleaned version of the UltraFeedback dataset.
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 7000000000
    release_date: 2024-02-01
    # TODO: Add instruct tag.
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: allenai/olmo-1.7-7b
    display_name: OLMo 1.7 (7B)
    description: OLMo is a series of Open Language Models trained on the Dolma dataset. The instruct versions was trained on the Tulu SFT mixture and a cleaned version of the UltraFeedback dataset.
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-17
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: allenai/olmo-2-1124-7b-instruct
    display_name: OLMo 2 7B Instruct November 2024
    description: OLMo 2 is a family of 7B and 13B models trained on up to 5T tokens. ([blog](https://allenai.org/blog/olmo2))
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 7300000000
    release_date: 2024-11-26
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: allenai/olmo-2-1124-13b-instruct
    display_name: OLMo 2 13B Instruct November 2024
    description: OLMo 2 is a family of 7B and 13B models trained on up to 5T tokens. ([blog](https://allenai.org/blog/olmo2))
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 13700000000
    release_date: 2024-11-26
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: allenai/olmo-2-0325-32b-instruct
    display_name: OLMo 2 32B Instruct March 2025
    description: OLMo 2 32B Instruct March 2025 is trained up to 6T tokens and post-trained using Tulu 3.1. ([blog](https://allenai.org/blog/olmo2-32B))
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 32200000000
    release_date: 2025-03-13
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: allenai/olmoe-1b-7b-0125-instruct
    display_name: OLMoE 1B-7B Instruct January 2025
    description: OLMoE 1B-7B Instruct January 2025 is a fully open language model leveraging sparse Mixture-of-Experts (MoE). It has 7B parameters but uses only 1B per input token. It was pretrained on 5T tokens. ([blog](https://allenai.org/blog/olmoe-an-open-small-and-state-of-the-art-mixture-of-experts-model-c258432d0514), [paper](https://arxiv.org/abs/2409.02060))
    creator_organization_name: Allen Institute for AI
    access: open
    num_parameters: 32200000000
    release_date: 2025-03-13
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Mistral AI
  - name: mistralai/mistral-7b-v0.1
    display_name: Mistral v0.1 (7B)
    description: Mistral 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA) and Sliding-Window Attention (SWA). ([blog post](https://mistral.ai/news/announcing-mistral-7b/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 7300000000
    release_date: 2023-09-27
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: mistralai/mistral-7b-instruct-v0.1
    display_name: Mistral Instruct v0.1 (7B)
    description: Mistral v0.1 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA) and Sliding-Window Attention (SWA). The instruct version was fined-tuned using publicly available conversation datasets. ([blog post](https://mistral.ai/news/announcing-mistral-7b/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 7300000000
    release_date: 2023-09-27
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-7b-instruct-v0.2
    display_name: Mistral Instruct v0.2 (7B)
    description: Mistral v0.2 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA). Compared to v0.1, v0.2 has a 32k context window and no Sliding-Window Attention (SWA). ([blog post](https://mistral.ai/news/la-plateforme/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 7300000000
    release_date: 2024-03-23
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-7b-instruct-v0.3
    display_name: Mistral Instruct v0.3 (7B)
    description: Mistral v0.3 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA). Compared to v0.1, v0.2 has a 32k context window and no Sliding-Window Attention (SWA). ([blog post](https://mistral.ai/news/la-plateforme/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 7300000000
    release_date: 2024-05-22
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
  
  - name: mistralai/mistral-7b-instruct-v0.3-hf
    display_name: Mistral Instruct v0.3 (7B)
    description: Mistral v0.3 Instruct 7B is a 7.3B parameter transformer model that uses Grouped-Query Attention (GQA). Compared to v0.1, v0.2 has a 32k context window and no Sliding-Window Attention (SWA). ([blog post](https://mistral.ai/news/la-plateforme/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 7300000000
    release_date: 2024-05-22
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mixtral-8x7b-32kseqlen
    display_name: Mixtral (8x7B 32K seqlen)
    description: Mixtral is a mixture-of-experts model that has 46.7B total parameters but only uses 12.9B parameters per token. ([blog post](https://mistral.ai/news/mixtral-of-experts/), [tweet](https://twitter.com/MistralAI/status/1733150512395038967)).
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 46700000000
    release_date: 2023-12-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: mistralai/mixtral-8x7b-instruct-v0.1
    display_name: Mixtral Instruct (8x7B)
    description: Mixtral Instruct (8x7B) is a version of Mixtral (8x7B) that was optimized through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. ([blog post](https://mistral.ai/news/mixtral-of-experts/)).
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 46700000000
    # Blog post: https://mistral.ai/news/mixtral-of-experts/
    release_date: 2023-12-11
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mixtral-8x22b
    display_name: Mixtral (8x22B)
    description: Mistral AI's mixture-of-experts model that uses 39B active parameters out of 141B ([blog post](https://mistral.ai/news/mixtral-8x22b/)).
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 176000000000
    release_date: 2024-04-10
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: mistralai/mixtral-8x22b-instruct-v0.1
    display_name: Mixtral Instruct (8x22B)
    description: Mistral AI's mixture-of-experts model that uses 39B active parameters out of 141B ([blog post](https://mistral.ai/news/mixtral-8x22b/)).
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 176000000000
    release_date: 2024-04-10
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/bakLlava-v1-hf
    display_name: BakLLaVA v1 (7B)
    description: BakLLaVA v1 is a Mistral 7B base augmented with the LLaVA 1.5 architecture. ([blog](https://huggingface.co/llava-hf/bakLlava-v1-hf))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 7000000000
    release_date: 2023-10-16
    tags: [VISION_LANGUAGE_MODEL_TAG, LLAVA_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: mistralai/ministral-3b-2410
    display_name: Ministral 3B (2402)
    description: Ministral 3B (2402) is a model for on-device computing and at-the-edge use cases ([blog](https://mistral.ai/news/ministraux/)).
    creator_organization_name: Mistral AI
    access: limited
    release_date: 2024-10-16
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/ministral-8b-2410
    display_name: Ministral 8B (2402)
    description: Ministral 8B (2402) is a model for on-device computing and at-the-edge use cases a special interleaved sliding-window attention pattern for faster and memory-efficient inference ([blog](https://mistral.ai/news/ministraux/)).
    creator_organization_name: Mistral AI
    access: open
    release_date: 2024-10-16
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-small-2402
    display_name: Mistral Small (2402)
    description: Mistral Small is a multilingual model with a 32K tokens context window and function-calling capabilities. ([blog](https://mistral.ai/news/mistral-large/))
    creator_organization_name: Mistral AI
    access: limited
    release_date: 2023-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-small-2409
    display_name: Mistral Small (2409)
    description: Mistral Small is a multilingual model with a 32K tokens context window and function-calling capabilities. ([blog](https://mistral.ai/news/mistral-large/))
    creator_organization_name: Mistral AI
    access: limited
    release_date: 2024-09-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-small-2501
    display_name: Mistral Small 3 (2501)
    description: Mistral Small 3 (2501) is a pre-trained and instructed model catered to the '80%' of generative AI tasks—those that require robust language and instruction following performance, with very low latency. ([blog](https://mistral.ai/news/mistral-small-3/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 23600000000
    release_date: 2025-01-30
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-small-2503
    display_name: Mistral Small 3.1 (2503)
    description: Mistral Small 3.1 (2503) is a model with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. ([blog](https://mistral.ai/news/mistral-small-3-1))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 23600000000
    release_date: 2025-03-17
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-medium-2312
    display_name: Mistral Medium (2312)
    description: Mistral is a transformer model that uses Grouped-Query Attention (GQA) and Sliding-Window Attention (SWA).
    creator_organization_name: Mistral AI
    access: limited
    release_date: 2023-12-11
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-medium-2505
    display_name: Mistral Medium 3 (2505)
    description: Mistral Medium 3 (2505) is a language model that is intended to to deliver state-of-the-art performance at lower cost. ([blog](https://mistral.ai/news/mistral-medium-3))
    creator_organization_name: Mistral AI
    access: limited
    release_date: 2025-05-07
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-large-2402
    display_name: Mistral Large (2402)
    description: Mistral Large is a multilingual model with a 32K tokens context window and function-calling capabilities. ([blog](https://mistral.ai/news/mistral-large/))
    creator_organization_name: Mistral AI
    access: limited
    release_date: 2023-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-large-2407
    display_name: Mistral Large 2 (2407)
    description: Mistral Large 2 is a 123 billion parameter model that has a 128k context window and supports dozens of languages and 80+ coding languages. ([blog](https://mistral.ai/news/mistral-large-2407/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 123000000000
    release_date: 2023-07-24
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/mistral-large-2411
    display_name: Mistral Large (2411)
    description: Mistral Large (2411) is a 123B parameter model that has a 128k context window. ([blog](https://mistral.ai/news/pixtral-large/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 123000000000
    release_date: 2024-11-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/open-mistral-nemo-2407
    display_name: Mistral NeMo (2402)
    description: Mistral NeMo is a multilingual 12B model with a large context window of 128K tokens. ([blog](https://mistral.ai/news/mistral-nemo/))
    creator_organization_name: Mistral AI
    access: open
    release_date: 2024-07-18
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/pixtral-12b-2409
    display_name: Mistral Pixtral (2409)
    description: Mistral Pixtral 12B is the first multimodal Mistral model for image understanding. ([blog](https://mistral.ai/news/pixtral-12b/))
    creator_organization_name: Mistral AI
    access: open
    release_date: 2024-09-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: mistralai/pixtral-large-2411
    display_name: Mistral Pixtral Large (2411)
    description: Mistral Pixtral Large is a 124B open-weights multimodal model built on top of Mistral Large 2 (2407). ([blog](https://mistral.ai/news/pixtral-large/))
    creator_organization_name: Mistral AI
    access: open
    num_parameters: 124000000000
    release_date: 2024-11-18
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Moonshot AI
  - name: moonshotai/kimi-k2-instruct
    display_name: Kimi K2 Instruct
    description:  Kimi K2 Instruct is a mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters trained with the Muon optimizer on 15.5T tokens. ([blog](https://moonshotai.github.io/Kimi-K2/))
    creator_organization_name: Moonshot AI
    access: open
    num_parameters: 1029173256720
    release_date: 2024-07-14  # Blog post has no date, so use the date from this news article https://www.cnbc.com/2025/07/14/alibaba-backed-moonshot-releases-kimi-k2-ai-rivaling-chatgpt-claude.html
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # MosaicML
  - name: mosaicml/mpt-7b
    display_name: MPT (7B)
    description: MPT (7B) is a Transformer trained from scratch on 1T tokens of text and code.
    creator_organization_name: MosaicML
    access: open
    num_parameters: 6700000000
    release_date: 2023-05-05
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: mosaicml/mpt-7b-chat # NOT SUPPORTED
    display_name: MPT-Chat (7B)
    description: MPT-Chat (7B) is a chatbot-like model for dialogue generation. It is built by finetuning MPT (30B) , a Transformer trained from scratch on 1T tokens of text and code.
    creator_organization_name: MosaicML
    access: open
    num_parameters: 6700000000
    release_date: 2023-05-05
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: mosaicml/mpt-instruct-7b
    display_name: MPT-Instruct (7B)
    description: MPT-Instruct (7B) is a model for short-form instruction following. It is built by finetuning MPT (30B), a Transformer trained from scratch on 1T tokens of text and code.
    creator_organization_name: MosaicML
    access: open
    num_parameters: 6700000000
    release_date: 2023-05-05
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: mosaicml/mpt-30b
    display_name: MPT (30B)
    description: MPT (30B) is a Transformer trained from scratch on 1T tokens of text and code.
    creator_organization_name: MosaicML
    access: open
    num_parameters: 30000000000
    release_date: 2023-06-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: mosaicml/mpt-30b-chat # NOT SUPPORTED
    display_name: MPT-Chat (30B)
    description: MPT-Chat (30B) is a chatbot-like model for dialogue generation. It is built by finetuning MPT (30B), a Transformer trained from scratch on 1T tokens of text and code.
    creator_organization_name: MosaicML
    access: open
    num_parameters: 30000000000
    release_date: 2023-06-22
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: mosaicml/mpt-instruct-30b
    display_name: MPT-Instruct (30B)
    description: MPT-Instruct (30B) is a model for short-form instruction following. It is built by finetuning MPT (30B), a Transformer trained from scratch on 1T tokens of text and code.
    creator_organization_name: MosaicML
    access: open
    num_parameters: 30000000000
    release_date: 2023-06-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]


    
  # NECTEC
  - name: nectec/Pathumma-llm-text-1.0.0
    display_name: Pathumma-llm-text-1.0.0 (7B)
    description: Pathumma-llm-text-1.0.0 (7B) is a instruction model from  OpenThaiLLM-Prebuilt-7B ([blog](https://medium.com/nectec/pathummallm-v-1-0-0-release-6a098ddfe276))
    creator_organization_name: nectec
    access: open
    num_parameters: 7620000000
    release_date: 2024-10-28
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]
 
  - name: nectec/OpenThaiLLM-Prebuilt-7B
    display_name: OpenThaiLLM-Prebuilt-7B (7B)
    description: OpenThaiLLM-Prebuilt-7B (7B) is a pretrained Thai large language model with 7 billion parameters based on Qwen2.5-7B.
    creator_organization_name: nectec
    access: open
    num_parameters: 7620000000
    release_date: 2024-10-28
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]



  # Neurips
  - name: neurips/local
    display_name: Neurips Local
    description: Neurips Local
    creator_organization_name: Neurips
    access: open
    release_date: 2023-06-01
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]



  # NVIDIA
  - name: nvidia/megatron-gpt2
    display_name: Megatron GPT2
    description: GPT-2 implemented in Megatron-LM ([paper](https://arxiv.org/abs/1909.08053)).
    creator_organization_name: NVIDIA
    access: open
    release_date: 2019-09-17 # paper date
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, BUGGY_TEMP_0_TAG]

  - name: nvidia/nemotron-4-340b-instruct
    display_name: Nemotron-4 Instruct (340B)
    description: Nemotron-4 Instruct (340B) is an open weights model sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. 98% of the data used for model alignment was synthetically generated ([paper](https://arxiv.org/abs/2406.11704)).
    creator_organization_name: NVIDIA
    access: open
    release_date: 2024-06-17
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: nvidia/llama-3.1-nemotron-70b-instruct
    display_name: Llama 3.1 Nemotron Instruct (70B)
    description: Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. It was trained using RLHF (specifically, REINFORCE), Llama-3.1-Nemotron-70B-Reward and HelpSteer2-Preference prompts on a Llama-3.1-70B-Instruct model. ([paper](https://arxiv.org/abs/2410.01257))
    creator_organization_name: NVIDIA
    access: open
    num_parameters: 70000000000
    release_date: 2024-10-02
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]


  # OpenAI

  ## GPT 2 Models
  # Not served by OpenAI, instead served by HuggingFace.

  - name: openai/gpt2
    display_name: GPT-2 (1.5B)
    description: GPT-2 (1.5B parameters) is a transformer model trained on a large corpus of English text in a self-supervised fashion ([paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).
    creator_organization_name: OpenAI
    access: open
    num_parameters: 1500000000
    release_date: 2019-02-14
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]


  ## GPT 3 Models
  # The list of models can be found here: https://beta.openai.com/docs/engines/gpt-3

  - name: openai/davinci-002
    display_name: davinci-002
    description: Replacement for the GPT-3 curie and davinci base models.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-08-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: openai/babbage-002
    display_name: babbage-002
    description: Replacement for the GPT-3 ada and babbage base models.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-08-22
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # DEPRECATED: Announced on July 06 2023 that these models will be shut down on January 04 2024.

  - name: openai/davinci
    display_name: davinci (175B)
    description: Original GPT-3 (175B parameters) autoregressive language model ([paper](https://arxiv.org/pdf/2005.14165.pdf), [docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 175000000000
    release_date: 2020-05-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: openai/curie
    display_name: curie (6.7B)
    description: Original GPT-3 (6.7B parameters) autoregressive language model ([paper](https://arxiv.org/pdf/2005.14165.pdf), [docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 6700000000
    release_date: 2020-05-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]
    
  - name: openai/babbage
    display_name: babbage (1.3B)
    description: Original GPT-3 (1.3B parameters) autoregressive language model ([paper](https://arxiv.org/pdf/2005.14165.pdf), [docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 1300000000
    release_date: 2020-05-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]
    
  - name: openai/ada
    display_name: ada (350M)
    description: Original GPT-3 (350M parameters) autoregressive language model ([paper](https://arxiv.org/pdf/2005.14165.pdf), [docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 350000000
    release_date: 2020-05-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: openai/text-davinci-003
    display_name: GPT-3.5 (text-davinci-003)
    description: text-davinci-003 model that involves reinforcement learning (PPO) with reward models. Derived from text-davinci-002 ([docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 175000000000
    release_date: 2022-11-28
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/text-davinci-002
    display_name: GPT-3.5 (text-davinci-002)
    description: text-davinci-002 model that involves supervised fine-tuning on human-written demonstrations. Derived from code-davinci-002 ([docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 175000000000
    release_date: 2022-01-27
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: openai/text-davinci-001
    display_name: GPT-3.5 (text-davinci-001)
    description: text-davinci-001 model that involves supervised fine-tuning on human-written demonstrations ([docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 175000000000
    release_date: 2022-01-27
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: openai/text-curie-001
    display_name: text-curie-001
    description: text-curie-001 model that involves supervised fine-tuning on human-written demonstrations ([docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 6700000000
    release_date: 2022-01-27
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]
    
  - name: openai/text-babbage-001
    display_name: text-babbage-001
    description: text-babbage-001 model that involves supervised fine-tuning on human-written demonstrations ([docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 1300000000
    release_date: 2022-01-27
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]
    
  - name: openai/text-ada-001
    display_name: text-ada-001
    description: text-ada-001 model that involves supervised fine-tuning on human-written demonstrations ([docs](https://beta.openai.com/docs/model-index-for-researchers)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 350000000
    release_date: 2022-01-27
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]


  ## GPT 3.5 Turbo Models
  # ChatGPT: https://openai.com/blog/chatgpt
  
  - name: openai/gpt-3.5-turbo-instruct
    display_name: GPT-3.5 Turbo Instruct
    description: Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-09-18
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-3.5-turbo-0301
    display_name: GPT-3.5 Turbo (0301)
    description: Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-03-01.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-03-01
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-3.5-turbo-0613
    display_name: GPT-3.5 Turbo (0613)
    description: Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-06-13.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-06-13
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-3.5-turbo-1106
    display_name: GPT-3.5 Turbo (1106)
    description: Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-11-06.
    creator_organization_name: OpenAI
    access: limited
    # Actual release blog post was published on 2024-01-25:
    # https://openai.com/blog/new-embedding-models-and-api-updates
    release_date: 2024-01-25
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-3.5-turbo-0125
    display_name: GPT-3.5 Turbo (0125)
    description: Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2024-01-25.
    creator_organization_name: OpenAI
    access: limited
    # Release blog post was published on 2024-01-25:
    # https://openai.com/blog/new-embedding-models-and-api-updates
    # The actual release date is unclear - it was described as "next week".
    release_date: 2023-06-13
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-3.5-turbo-16k-0613
    display_name: gpt-3.5-turbo-16k-0613
    description: Sibling model of text-davinci-003 that is optimized for chat but works well for traditional completions tasks as well. Snapshot from 2023-06-13 with a longer context length of 16,384 tokens.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-06-13
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]


  ## GPT-4 and GPT-4 Turbo

  - name: openai/gpt-4-1106-preview
    display_name: GPT-4 Turbo (1106 preview)
    description: GPT-4 Turbo (preview) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Preview snapshot from 2023-11-06.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-11-06
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4-0314
    display_name: GPT-4 (0314)
    description: GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 from 2023-03-14.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-03-14
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4-32k-0314
    display_name: gpt-4-32k-0314
    description: GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 with a longer context length of 32,768 tokens from March 14th 2023.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-03-14
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4-0613
    display_name: GPT-4 (0613)
    description: GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 from 2023-06-13.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-06-13
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4-32k-0613
    display_name: gpt-4-32k-0613
    description: GPT-4 is a large multimodal model (currently only accepting text inputs and emitting text outputs) that is optimized for chat but works well for traditional completions tasks. Snapshot of gpt-4 with a longer context length of 32,768 tokens from 2023-06-13.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-06-13
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4-0125-preview
    display_name: GPT-4 Turbo (0125 preview)
    description: GPT-4 Turbo (preview) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Preview snapshot from 2023-01-25. This snapshot is intended to reduce cases of “laziness” where the model doesn’t complete a task.
    creator_organization_name: OpenAI
    access: limited
    # Actual release blog post was published on 2024-01-25:
    # https://openai.com/blog/new-embedding-models-and-api-updates
    release_date: 2024-01-25
    tags: [TEXT_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  ## GPT-4o

  - name: openai/gpt-4-turbo-2024-04-09
    display_name: GPT-4 Turbo (2024-04-09)
    description: GPT-4 Turbo (2024-04-09) is a large multimodal model that is optimized for chat but works well for traditional completions tasks. The model is cheaper and faster than the original GPT-4 model. Snapshot from 2024-04-09.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-04-09
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4o-2024-05-13
    display_name: GPT-4o (2024-05-13)
    description: GPT-4o (2024-05-13) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. ([blog](https://openai.com/index/hello-gpt-4o/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-04-09
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4o-2024-08-06
    display_name: GPT-4o (2024-08-06)
    description: GPT-4o (2024-08-06) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. ([blog](https://openai.com/index/introducing-structured-outputs-in-the-api/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-08-06
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4o-2024-11-20
    display_name: GPT-4o (2024-11-20)
    description: GPT-4o (2024-11-20) is a large multimodal model that accepts as input any combination of text, audio, and image and generates any combination of text, audio, and image outputs. ([blog](https://openai.com/index/introducing-structured-outputs-in-the-api/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-11-20
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4o-mini-2024-07-18
    display_name: GPT-4o mini (2024-07-18)
    description: GPT-4o mini (2024-07-18) is a multimodal model with a context window of 128K tokens and improved handling of non-English text. ([blog](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-07-18
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4.1-2025-04-14
    display_name: GPT-4.1 (2025-04-14)
    description: GPT-4.1 (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. ([blog](https://openai.com/index/gpt-4-1/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4.1-mini-2025-04-14
    display_name: GPT-4.1 mini (2025-04-14)
    description: GPT-4.1 mini (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. ([blog](https://openai.com/index/gpt-4-1/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4.1-nano-2025-04-14
    display_name: GPT-4.1 nano (2025-04-14)
    description: GPT-4.1 nano (2025-04-14) is a multimdodal model in the GPT-4.1 family, which outperforms the GPT-4o family, with major gains in coding and instruction following. They also have larger context windows of 1 million tokens and are able to better use that context with improved long-context comprehension. ([blog](https://openai.com/index/gpt-4-1/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-14
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/whisper-1_gpt-4o-2024-11-20
    display_name: Whisper-1 + GPT-4o (2024-11-20)
    description: Transcribes the text with Whisper-1 and then uses GPT-4o to generate a response.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-11-20
    tags: [AUDIO_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG]

  - name: openai/gpt-4o-transcribe_gpt-4o-2024-11-20
    display_name: GPT-4o Transcribe + GPT-4o (2024-11-20)
    description: Transcribes the text with GPT-4o Transcribe and then uses GPT-4o to generate a response.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-03-20
    tags: [AUDIO_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG]

  - name: openai/gpt-4o-mini-transcribe_gpt-4o-2024-11-20
    display_name: GPT-4o mini Transcribe + GPT-4o (2024-11-20)
    description: Transcribes the text with GPT-4o mini Transcribe and then uses GPT-4o to generate a response.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-03-20
    tags: [AUDIO_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG]

  - name: openai/gpt-4o-audio-preview-2024-10-01
    display_name: GPT-4o Audio (Preview 2024-10-01)
    description: GPT-4o Audio (Preview 2024-10-01) is a preview model that allows using use audio inputs to prompt the model ([documentation](https://platform.openai.com/docs/guides/audio)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-10-01
    tags: [AUDIO_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4o-audio-preview-2024-12-17
    display_name: GPT-4o Audio (Preview 2024-12-17)
    description: GPT-4o Audio (Preview 2024-12-17) is a preview model that allows using use audio inputs to prompt the model ([documentation](https://platform.openai.com/docs/guides/audio)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-12-17
    tags: [AUDIO_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/gpt-4o-mini-audio-preview-2024-12-17
    display_name: GPT-4o mini Audio (Preview 2024-12-17)
    description: GPT-4o mini Audio (Preview 2024-12-17) is a preview model that allows using use audio inputs to prompt the model ([documentation](https://platform.openai.com/docs/guides/audio)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-12-17
    tags: [AUDIO_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # GPT-4V

  - name: openai/gpt-4-vision-preview
    # According to https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4, this model has pointed gpt-4-1106-vision-preview.
    display_name: GPT-4V (1106 preview)
    description: GPT-4V is a large multimodal model that accepts both text and images and is optimized for chat ([model card](https://openai.com/research/gpt-4v-system-card)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-11-06
    tags: [VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: openai/gpt-4-1106-vision-preview
    display_name: GPT-4V (1106 preview)
    description: GPT-4V is a large multimodal model that accepts both text and images and is optimized for chat ([model card](https://openai.com/research/gpt-4v-system-card)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2023-11-06
    tags: [VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  ## GPT-4.5
  - name: openai/gpt-4.5-preview-2025-02-27
    display_name: GPT-4.5 (2025-02-27 preview)
    description: GPT-4.5 (2025-02-27 preview) is a large multimodal model that is designed to be more general-purpose than OpenAI's STEM-focused reasoning models. It was trained using new supervision techniques combined with traditional methods like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). ([blog](https://openai.com/index/introducing-gpt-4-5/), [system card](https://openai.com/index/gpt-4-5-system-card/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-02-27
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, OPENAI_CHATGPT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  ## o1 Models
  - name: openai/o1-pro-2025-03-19
    display_name: o1 pro (2025-03-19)
    description: o1 is a new large language model trained with reinforcement learning to perform complex reasoning. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-03-19
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-pro-2025-03-19-low-reasoning-effort
    display_name: o1 pro (2025-03-19, low reasoning effort)
    description: o1 is a new large language model trained with reinforcement learning to perform complex reasoning. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/)) The requests' reasoning effort parameter in is set to low.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-03-19
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-pro-2025-03-19-high-reasoning-effort
    display_name: o1 pro (2025-03-19, high reasoning effort)
    description: o1 is a new large language model trained with reinforcement learning to perform complex reasoning. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/)) The requests' reasoning effort parameter in is set to high.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-03-19
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-2024-12-17
    display_name: o1 (2024-12-17)
    description: o1 is a new large language model trained with reinforcement learning to perform complex reasoning. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-12-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-2024-12-17-low-reasoning-effort
    display_name: o1 (2024-12-17, low reasoning effort)
    description: o1 is a new large language model trained with reinforcement learning to perform complex reasoning. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/)) The requests' reasoning effort parameter in is set to low.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-12-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-2024-12-17-high-reasoning-effort
    display_name: o1 (2024-12-17, high reasoning effort)
    description: o1 is a new large language model trained with reinforcement learning to perform complex reasoning. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/)) The requests' reasoning effort parameter in is set to high.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-12-17
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-preview-2024-09-12
    display_name: o1-preview (2024-09-12)
    description: o1-preview is a language model trained with reinforcement learning to perform complex reasoning that can produce a long internal chain of thought before responding to the user. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/learning-to-reason-with-llms/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-09-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o1-mini-2024-09-12
    display_name: o1-mini (2024-09-12)
    description: o1-mini is a cost-effective reasoning model for applications that require reasoning without broad world knowledge. ([model card](https://openai.com/index/openai-o1-system-card/), [blog post](https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2024-09-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-mini-2025-01-31
    display_name: o3-mini (2025-01-31)
    description: o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. ([blog post](https://openai.com/index/openai-o3-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-01-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-mini-2025-01-31-low-reasoning-effort
    display_name: o3-mini (2025-01-31, low reasoning effort)
    description: o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. ([blog post](https://openai.com/index/openai-o3-mini/)) The requests' reasoning effort parameter in is set to low.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-01-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-mini-2025-01-31-high-reasoning-effort
    display_name: o3-mini (2025-01-31, high reasoning effort)
    description: o3-mini is a small reasoning model form OpenAI that aims to deliver STEM capabilities while maintaining the low cost and reduced latency of OpenAI o1-mini. ([blog post](https://openai.com/index/openai-o3-mini/)) The requests' reasoning effort parameter in is set to high.
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-01-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-2025-04-16
    display_name: o3 (2025-04-16)
    description: o3 is a reasoning model for math, science, coding, and visual reasoning tasks. ([blog post](https://openai.com/index/introducing-o3-and-o4-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-2025-04-16-low-reasoning-effort
    display_name: o3 (2025-04-16, low reasoning effort)
    description: o3 is a reasoning model for math, science, coding, and visual reasoning tasks. ([blog post](https://openai.com/index/introducing-o3-and-o4-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-2025-04-16-high-reasoning-effort
    display_name: o3 (2025-04-16, high reasoning effort)
    description: o3 is a reasoning model for math, science, coding, and visual reasoning tasks. ([blog post](https://openai.com/index/introducing-o3-and-o4-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o4-mini-2025-04-16
    display_name: o4-mini (2025-04-16)
    description: o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. ([blog post](https://openai.com/index/introducing-o3-and-o4-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o4-mini-2025-04-16-low-reasoning-effort
    display_name: o4-mini (2025-04-16, low reasoning effort)
    description: o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. ([blog post](https://openai.com/index/introducing-o3-and-o4-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o4-mini-2025-04-16-high-reasoning-effort
    display_name: o4-mini (2025-04-16, high reasoning effort)
    description: o4-mini is an o-series model optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. ([blog post](https://openai.com/index/introducing-o3-and-o4-mini/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openai/o3-pro-2025-06-10-high-reasoning-effort
    display_name: o3-pro (2025-06-10, high reasoning effort)
    description: o3-pro is an o-series model designed to think longer and provide the most reliable responses. ([blog post](https://help.openai.com/en/articles/9624314-model-release-notes))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-06-10
    tags: [TEXT_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  ## GPT-OSS
  - name: openai/gpt-oss-120b
    display_name: gpt-oss-120b
    description: gpt-oss-120b is a open-weight language model that was trained using a mix reinforcement learning and other techniques informed by OpenAI's internal models. It uses a mixture-of-experts architecture and activates 5.1B parameters per token. ([blog](https://openai.com/index/introducing-gpt-oss/))
    creator_organization_name: OpenAI
    access: limited
    release_date: 2025-08-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  ## Codex Models
  # DEPRECATED: Codex models have been shut down on March 23 2023.

  - name: openai/code-davinci-002
    display_name: code-davinci-002
    description: Codex-style model that is designed for pure code-completion tasks ([docs](https://beta.openai.com/docs/models/codex)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2021-07-01 # TODO: Find correct date (this is for v1)
    tags: [DEPRECATED_MODEL_TAG, CODE_MODEL_TAG]

  - name: openai/code-davinci-001
    display_name: code-davinci-001
    description: code-davinci-001 model
    creator_organization_name: OpenAI
    access: limited
    release_date: 2021-07-01 # Paper date
    tags: [DEPRECATED_MODEL_TAG, CODE_MODEL_TAG]

  - name: openai/code-cushman-001
    display_name: code-cushman-001 (12B)
    description: Codex-style model that is a stronger, multilingual version of the Codex (12B) model in the [Codex paper](https://arxiv.org/pdf/2107.03374.pdf).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 12000000000
    release_date: 2021-07-01 # Paper date
    tags: [DEPRECATED_MODEL_TAG, CODE_MODEL_TAG]


  ## Text Similarity Models
  # OpenAI similarity embedding models: https://beta.openai.com/docs/guides/embeddings
  # The number of parameters is guessed based on the number of parameters of the
  # corresponding GPT-3 model.
  # DEPRECATED: Announced on July 06 2023 that first generation embeddings models
  #  will be shut down on January 04 2024.

  - name: openai/text-similarity-davinci-001
    display_name: text-similarity-davinci-001
    description: Embedding model that is designed for text similarity tasks ([docs](https://openai.com/blog/introducing-text-and-code-embeddings)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 175000000000
    release_date: 2022-01-25 # Blog post date
    tags: [DEPRECATED_MODEL_TAG, TEXT_SIMILARITY_MODEL_TAG]

  - name: openai/text-similarity-curie-001
    display_name: text-similarity-curie-001
    description: Embedding model that is designed for text similarity tasks ([docs](https://openai.com/blog/introducing-text-and-code-embeddings)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 6700000000
    release_date: 2022-01-25 # Blog post date
    tags: [DEPRECATED_MODEL_TAG, TEXT_SIMILARITY_MODEL_TAG]

  - name: openai/text-similarity-babbage-001
    display_name: text-similarity-babbage-001
    description: Embedding model that is designed for text similarity tasks ([docs](https://openai.com/blog/introducing-text-and-code-embeddings)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 1300000000
    release_date: 2022-01-25 # Blog post date
    tags: [DEPRECATED_MODEL_TAG, TEXT_SIMILARITY_MODEL_TAG]

  - name: openai/text-similarity-ada-001
    display_name: text-similarity-ada-001
    description: Embedding model that is designed for text similarity tasks ([docs](https://openai.com/blog/introducing-text-and-code-embeddings)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 350000000
    release_date: 2022-01-25 # Blog post date
    tags: [DEPRECATED_MODEL_TAG, TEXT_SIMILARITY_MODEL_TAG]

  - name: openai/text-embedding-ada-002
    display_name: text-embedding-ada-002
    description: An improved embedding model that is designed for text similarity tasks ([docs](https://openai.com/blog/new-and-improved-embedding-model)).
    creator_organization_name: OpenAI
    access: limited
    release_date: 2022-12-15 # Blog post date
    tags: [TEXT_SIMILARITY_MODEL_TAG]

  # Text-to-image models
  - name: openai/dall-e-2
    display_name: DALL-E 2 (3.5B)
    description: DALL-E 2 is a encoder-decoder-based latent diffusion model trained on large-scale paired text-image datasets. The model is available via the OpenAI API ([paper](https://arxiv.org/abs/2204.06125)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 3500000000
    release_date: 2022-04-13
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: openai/dall-e-3
    display_name: DALL-E 3
    description: DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The default style, vivid, causes the model to lean towards generating hyper-real and dramatic images. The model is available via the OpenAI API ([paper](https://cdn.openai.com/papers/dall-e-3.pdf)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 0
    release_date: 2023-11-06
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: openai/dall-e-3-natural
    display_name: DALL-E 3 (natural style)
    description: DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The natural style causes the model to produce more natural, less hyper-real looking images. The model is available via the OpenAI API ([paper](https://cdn.openai.com/papers/dall-e-3.pdf)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 0
    release_date: 2023-11-06
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: openai/dall-e-3-hd
    display_name: DALL-E 3 HD
    description: DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The HD version creates images with finer details and greater consistency across the image, but generation is slower. The default style, vivid, causes the model to lean towards generating hyper-real and dramatic images. The model is available via the OpenAI API ([paper](https://cdn.openai.com/papers/dall-e-3.pdf)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 0
    release_date: 2023-11-06
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: openai/dall-e-3-hd-natural
    display_name: DALL-E 3 HD (natural style)
    description: DALL-E 3 is a text-to-image generation model built natively on ChatGPT, used to prompt engineer automatically. The HD version creates images with finer details and greater consistency across the image, but generation is slower. The natural style causes the model to produce more natural, less hyper-real looking images. The model is available via the OpenAI API ([paper](https://cdn.openai.com/papers/dall-e-3.pdf)).
    creator_organization_name: OpenAI
    access: limited
    num_parameters: 0
    release_date: 2023-11-06
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  # OpenThaiGPT
  - name: openthaigpt/openthaigpt-1.0.0-7b-chat
    display_name: OpenThaiGPT v1.0.0 (7B)
    description: OpenThaiGPT v1.0.0 (7B) is a Thai language chat model based on Llama 2 that has been specifically fine-tuned for Thai instructions and enhanced by incorporating over 10,000 of the most commonly used Thai words into the dictionary. ([blog post](https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than))
    creator_organization_name: OpenThaiGPT
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openthaigpt/openthaigpt-1.0.0-13b-chat
    display_name: OpenThaiGPT v1.0.0 (13B)
    description: OpenThaiGPT v1.0.0 (13B) is a Thai language chat model based on Llama 2 that has been specifically fine-tuned for Thai instructions and enhanced by incorporating over 10,000 of the most commonly used Thai words into the dictionary. ([blog post](https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than))
    creator_organization_name: OpenThaiGPT
    access: open
    num_parameters: 13000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: openthaigpt/openthaigpt-1.0.0-70b-chat
    display_name: OpenThaiGPT v1.0.0 (70B)
    description: OpenThaiGPT v1.0.0 (70B) is a Thai language chat model based on Llama 2 that has been specifically fine-tuned for Thai instructions and enhanced by incorporating over 10,000 of the most commonly used Thai words into the dictionary. ([blog post](https://openthaigpt.aieat.or.th/openthaigpt-1.0.0-less-than-8-apr-2024-greater-than))
    creator_organization_name: OpenThaiGPT
    access: open
    num_parameters: 70000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Qwen

  - name: qwen/qwen-7b
    display_name: Qwen
    description: 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: qwen/qwen1.5-7b
    display_name: Qwen1.5 (7B)
    description: 7B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: qwen/qwen1.5-14b
    display_name: Qwen1.5 (14B)
    description: 14B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: qwen/qwen1.5-32b
    display_name: Qwen1.5 (32B)
    description: 32B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. The 32B version also includes grouped query attention (GQA). ([blog](https://qwenlm.github.io/blog/qwen1.5-32b/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-04-02
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: qwen/qwen1.5-72b
    display_name: Qwen1.5 (72B)
    description: 72B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: qwen/qwen1.5-7b-chat
    display_name: Qwen1.5 Chat (7B)
    description: 7B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen1.5-14b-chat
    display_name: Qwen1.5 Chat (14B)
    description: 14B-parameter chat version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen1.5-32b-chat
    display_name: Qwen1.5 Chat (32B)
    description: 32B-parameter version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. The 32B version also includes grouped query attention (GQA). ([blog](https://qwenlm.github.io/blog/qwen1.5-32b/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-04-02
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen1.5-72b-chat
    display_name: Qwen1.5 Chat (72B)
    description: 72B-parameter chat version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. ([blog](https://qwenlm.github.io/blog/qwen1.5/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-02-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen1.5-110b-chat
    display_name: Qwen1.5 Chat (110B)
    description: 110B-parameter chat version of the large language model series, Qwen 1.5 (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen is a family of transformer models with SwiGLU activation, RoPE, and multi-head attention. The 110B version also includes grouped query attention (GQA). ([blog](https://qwenlm.github.io/blog/qwen1.5-110b/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-04-25
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen2-72b-instruct
    display_name: Qwen2 Instruct (72B)
    description: 72B-parameter chat version of the large language model series, Qwen2. Qwen2 uses Group Query Attention (GQA) and has extended context length support up to 128K tokens. ([blog](https://qwenlm.github.io/blog/qwen2/))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-06-07
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen2.5-7b-instruct-turbo
    display_name: Qwen2.5 Instruct Turbo (7B)
    description: Qwen2.5 Instruct Turbo (7B) was trained on 18 trillion tokens and supports 29 languages, and shows improvements over Qwen2 in knowledge, coding, mathematics, instruction following, generating long texts, and processing structure data. ([blog](https://qwenlm.github.io/blog/qwen2.5/)) Turbo is Together's cost-efficient implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-09-19
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen2.5-7b-instruct
    display_name: Qwen2.5 Instruct (7B)
    description: Qwen2.5 Instruct (7B) was trained on 18 trillion tokens and supports 29 languages, and shows improvements over Qwen2 in knowledge, coding, mathematics, instruction following, generating long texts, and processing structure data. ([blog](https://qwenlm.github.io/blog/qwen2.5/)) Turbo is Together's cost-efficient implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-09-19
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen2.5-72b-instruct-turbo
    display_name: Qwen2.5 Instruct Turbo (72B)
    description: Qwen2.5 Instruct Turbo (72B) was trained on 18 trillion tokens and supports 29 languages, and shows improvements over Qwen2 in knowledge, coding, mathematics, instruction following, generating long texts, and processing structure data. ([blog](https://qwenlm.github.io/blog/qwen2.5/)) Turbo is Together's cost-efficient implementation, providing fast FP8 performance while maintaining quality, closely matching FP16 reference models. ([blog](https://www.together.ai/blog/together-inference-engine-2))
    creator_organization_name: Qwen
    access: open
    release_date: 2024-09-19
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen3-235b-a22b-fp8-tput
    display_name: Qwen3 235B A22B FP8 Throughput
    description: Qwen3 235B A22B FP8 Throughput is a hybrid instruct and reasoning mixture-of-experts model ([blog](https://qwenlm.github.io/blog/qwen3/)).
    creator_organization_name: Qwen
    access: open
    release_date: 2025-04-29
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen3-235b-a22b-instruct-2507-fp8
    display_name: Qwen3 235B A22B Instruct 2507 FP8
    description: Qwen3 235B A22B Instruct 2507 FP8 is an updated version of the non-thinking mode of Qwen3 235B A22B FP8.
    creator_organization_name: Qwen
    access: open
    release_date: 2025-07-21  # https://x.com/Alibaba_Qwen/status/1947344511988076547
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwq-32b-preview
    display_name: QwQ (32B Preview)
    description: QwQ-32B-Preview is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. ([blog post](https://qwenlm.github.io/blog/qwq-32b-preview/)).
    creator_organization_name: Alibaba Cloud
    access: open
    num_parameters: 32800000000
    release_date: 2024-11-28
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: qwen/qwen-vl
    display_name: Qwen-VL
    description: Visual multimodal version of the Qwen large language model series ([paper](https://arxiv.org/abs/2308.12966)).
    creator_organization_name: Alibaba Cloud
    access: open
    release_date: 2023-08-24
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen-vl-chat
    display_name: Qwen-VL Chat
    description: Chat version of Qwen-VL ([paper](https://arxiv.org/abs/2308.12966)).
    creator_organization_name: Alibaba Cloud
    access: open
    release_date: 2023-08-24
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen2-vl-7b-instruct
    display_name: Qwen2-VL Instruct (7B)
    description: The second generation of Qwen2-VL models ([paper](https://arxiv.org/abs/2409.12191)).
    creator_organization_name: Alibaba Group
    access: open
    release_date: 2024-08-29
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen2-vl-72b-instruct
    display_name: Qwen2-VL Instruct (72B)
    description: The second generation of Qwen2-VL models ([paper](https://arxiv.org/abs/2409.12191)).
    creator_organization_name: Alibaba Group
    access: open
    release_date: 2024-08-29
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen2.5-vl-3b-instruct
    display_name: Qwen2.5-VL Instruct (3B)
    description: The second generation of Qwen2.5-VL models ([blog](https://qwenlm.github.io/blog/qwen2.5-vl/)).
    creator_organization_name: Alibaba Group
    access: open
    release_date: 2025-01-26
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen2.5-vl-7b-instruct
    display_name: Qwen2.5-VL Instruct (7B)
    description: The second generation of Qwen2.5-VL models ([blog](https://qwenlm.github.io/blog/qwen2.5-vl/)).
    creator_organization_name: Alibaba Group
    access: open
    release_date: 2025-01-26
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen2.5-vl-32b-instruct
    display_name: Qwen2.5-VL Instruct (32B)
    description: The second generation of Qwen2.5-VL models ([blog](https://qwenlm.github.io/blog/qwen2.5-vl/)).
    creator_organization_name: Alibaba Group
    access: open
    release_date: 2025-01-26
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen2.5-vl-72b-instruct
    display_name: Qwen2.5-VL Instruct (72B)
    description: The second generation of Qwen2.5-VL models ([blog](https://qwenlm.github.io/blog/qwen2.5-vl/)).
    creator_organization_name: Alibaba Group
    access: open
    release_date: 2025-01-26
    tags: [VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  - name: qwen/qwen-audio-chat
    display_name: Qwen-Audio Chat
    description: Auditory multimodal version of the Qwen large language model series ([paper](https://arxiv.org/abs/2311.07919)).
    creator_organization_name: Alibaba Cloud
    access: open
    release_date: 2023-11-14
    tags: [AUDIO_LANGUAGE_MODEL_TAG]

  - name: qwen/qwen2-audio-7b-instruct
    display_name: Qwen2-Audio Instruct (7B)
    description: The second version of auditory multimodal version of the Qwen large language model series ([paper](https://arxiv.org/abs/2407.10759)).
    creator_organization_name: Alibaba Cloud
    access: open
    release_date: 2024-07-15
    tags: [AUDIO_LANGUAGE_MODEL_TAG]

  - name: qwen/qwen2.5-omni-7b
    display_name: Qwen2.5-Omni (7B)
    description: The new flagship end-to-end multimodal model in the Qwen series that can process inputs including text, images, audio, and video ([paper](https://arxiv.org/abs/2503.20215)).
    creator_organization_name: Alibaba Cloud
    access: open
    release_date: 2025-03-27
    tags: [AUDIO_LANGUAGE_MODEL_TAG, VISION_LANGUAGE_MODEL_TAG, FULL_FUNCTIONALITY_VLM_TAG]

  # SAIL (Sea AI Lab)
  - name: sail/sailor-7b
    display_name: Sailor (7B)
    description: Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. ([paper](https://arxiv.org/abs/2404.03608))
    creator_organization_name: SAIL
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-04
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: sail/sailor-7b-chat
    display_name: Sailor Chat (7B)
    description: Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. ([paper](https://arxiv.org/abs/2404.03608))
    creator_organization_name: SAIL
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-04
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: sail/sailor-14b
    display_name: Sailor (14B)
    description: Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. ([paper](https://arxiv.org/abs/2404.03608))
    creator_organization_name: SAIL
    access: open
    num_parameters: 14000000000
    release_date: 2024-04-04
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: sail/sailor-14b-chat
    display_name: Sailor Chat (14B)
    description: Sailor is a suite of Open Language Models tailored for South-East Asia, focusing on languages such as Indonesian, Thai, Vietnamese, Malay, and Lao. These models were continually pre-trained from Qwen1.5. ([paper](https://arxiv.org/abs/2404.03608))
    creator_organization_name: SAIL
    access: open
    num_parameters: 14000000000
    release_date: 2024-04-04
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Salesforce
  - name: salesforce/codegen # NOT SUPPORTED
    display_name: CodeGen (16B)
    description: CodeGen (16B parameters) is an open dense code model trained for multi-turn program synthesis ([blog](https://arxiv.org/pdf/2203.13474.pdf)).
    creator_organization_name: Tsinghua
    access: open
    num_parameters: 16000000000
    release_date: 2022-03-25
    tags: [UNSUPPORTED_MODEL_TAG]

  # SambaNova
  - name: sambanova/sambalingo-thai-base
    display_name: SambaLingo-Thai-Base
    description: SambaLingo-Thai-Base is a pretrained bi-lingual Thai and English model that adapts Llama 2 (7B) to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset. ([paper](https://arxiv.org/abs/2404.05829))
    creator_organization_name: SambaLingo
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: sambanova/sambalingo-thai-chat
    display_name: SambaLingo-Thai-Chat
    description: SambaLingo-Thai-Chat is a chat model trained using direct preference optimization on SambaLingo-Thai-Base. SambaLingo-Thai-Base adapts Llama 2 (7B) to Thai by training on 38 billion tokens from the Thai split of the Cultura-X dataset. ([paper](https://arxiv.org/abs/2404.05829))
    creator_organization_name: SambaLingo
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: sambanova/sambalingo-thai-base-70b
    display_name: SambaLingo-Thai-Base-70B
    description: SambaLingo-Thai-Base-70B is a pretrained bi-lingual Thai and English model that adapts Llama 2 (70B) to Thai by training on 26 billion tokens from the Thai split of the Cultura-X dataset. ([paper](https://arxiv.org/abs/2404.05829))
    creator_organization_name: SambaLingo
    access: open
    num_parameters: 70000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: sambanova/sambalingo-thai-chat-70b
    display_name: SambaLingo-Thai-Chat-70B
    description: SambaLingo-Thai-Chat-70B is a chat model trained using direct preference optimization on SambaLingo-Thai-Base-70B. SambaLingo-Thai-Base-70B adapts Llama 2 (7B) to Thai by training on 26 billion tokens from the Thai split of the Cultura-X dataset. ([paper](https://arxiv.org/abs/2404.05829))
    creator_organization_name: SambaLingo
    access: open
    num_parameters: 70000000000
    release_date: 2024-04-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # SCB10X
  - name: scb10x/typhoon-7b
    display_name: Typhoon (7B)
    description: Typhoon (7B) is pretrained Thai large language model with 7 billion parameters based on Mistral 7B. ([paper](https://arxiv.org/abs/2312.13951))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 7000000000
    release_date: 2023-12-21
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: scb10x/typhoon-v1.5-8b
    display_name: Typhoon v1.5 (8B)
    description: Typhoon v1.5 (8B) is a pretrained Thai large language model with 8 billion parameters based on Llama 3 8B. ([blog](https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 8000000000
    release_date: 2024-05-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: scb10x/typhoon-v1.5-8b-instruct
    display_name: Typhoon v1.5 Instruct (8B)
    description: Typhoon v1.5 Instruct (8B) is a pretrained Thai large language model with 8 billion parameters based on Llama 3 8B. ([blog](https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 8000000000
    release_date: 2024-05-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: scb10x/typhoon-v1.5-72b
    display_name: Typhoon v1.5 (72B)
    description: Typhoon v1.5 (72B) is a pretrained Thai large language model with 72 billion parameters based on Qwen1.5-72B. ([blog](https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 72000000000
    release_date: 2024-05-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: scb10x/typhoon-v1.5-72b-instruct
    display_name: Typhoon v1.5 Instruct (72B)
    description: Typhoon v1.5 Instruct (72B) is a pretrained Thai large language model with 72 billion parameters based on Qwen1.5-72B. ([blog](https://blog.opentyphoon.ai/typhoon-1-5-release-a9364cb8e8d7))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 72000000000
    release_date: 2024-05-08
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: scb10x/llama-3-typhoon-v1.5x-8b-instruct
    display_name: Typhoon 1.5X instruct (8B)
    description: Llama-3-Typhoon-1.5X-8B-instruct is a 8 billion parameter instruct model designed for the Thai language based on Llama 3 Instruct. It utilizes the task-arithmetic model editing technique. ([blog](https://blog.opentyphoon.ai/typhoon-1-5x-our-experiment-designed-for-application-use-cases-7b85d9e9845c))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 8000000000
    release_date: 2024-05-29
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: scb10x/llama-3-typhoon-v1.5x-70b-instruct
    display_name: Typhoon 1.5X instruct (70B)
    description: Llama-3-Typhoon-1.5X-70B-instruct is a 70 billion parameter instruct model designed for the Thai language based on Llama 3 Instruct. It utilizes the task-arithmetic model editing technique. ([blog](https://blog.opentyphoon.ai/typhoon-1-5x-our-experiment-designed-for-application-use-cases-7b85d9e9845c))
    creator_organization_name: SCB10X
    access: open
    num_parameters: 70000000000
    release_date: 2024-05-29
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Alibaba DAMO Academy
  - name: damo/seallm-7b-v2
    display_name: SeaLLM v2 (7B)
    description: SeaLLM v2 is a multilingual LLM for Southeast Asian (SEA) languages trained from Mistral (7B). ([website](https://damo-nlp-sg.github.io/SeaLLMs/))
    creator_organization_name: Alibaba DAMO Academy
    access: open
    num_parameters: 7000000000
    release_date: 2024-02-02
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: damo/seallm-7b-v2.5
    display_name: SeaLLM v2.5 (7B)
    description: SeaLLM is a multilingual LLM for Southeast Asian (SEA) languages trained from Gemma (7B). ([website](https://damo-nlp-sg.github.io/SeaLLMs/))
    creator_organization_name: Alibaba DAMO Academy
    access: open
    num_parameters: 7000000000
    release_date: 2024-04-12
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # Snowflake
  - name: snowflake/snowflake-arctic-instruct
    display_name: Arctic Instruct
    description: Arctic combines a 10B dense transformer model with a residual 128x3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating.
    creator_organization_name: Snowflake
    access: open
    num_parameters: 482000000000
    release_date: 2024-04-24
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]


  # Stability AI
  - name: stabilityai/stablelm-base-alpha-3b
    display_name: StableLM-Base-Alpha (3B)
    description: StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.
    creator_organization_name: Stability AI
    access: open
    num_parameters: 3000000000
    release_date: 2023-04-20
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: stabilityai/stablelm-base-alpha-7b
    display_name: StableLM-Base-Alpha (7B)
    description: StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.
    creator_organization_name: Stability AI
    access: open
    num_parameters: 7000000000
    release_date: 2023-04-20
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  # Stanford
  - name: stanford/alpaca-7b
    display_name: Alpaca (7B)
    description: Alpaca 7B is a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations
    creator_organization_name: Stanford
    access: open
    num_parameters: 7000000000
    release_date: 2023-03-13
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]



  # TII UAE
  - name: tiiuae/falcon-7b
    display_name: Falcon (7B)
    description: Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora.
    creator_organization_name: TII UAE
    access: open
    num_parameters: 7000000000
    release_date: 2023-03-15
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: tiiuae/falcon-7b-instruct
    display_name: Falcon-Instruct (7B)
    description: Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets.
    creator_organization_name: TII UAE
    access: open
    num_parameters: 7000000000
    release_date: 2023-03-15
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: tiiuae/falcon-40b
    display_name: Falcon (40B)
    description: Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora.
    creator_organization_name: TII UAE
    access: open
    num_parameters: 40000000000
    release_date: 2023-05-25
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: tiiuae/falcon-40b-instruct
    display_name: Falcon-Instruct (40B)
    description: Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets.
    creator_organization_name: TII UAE
    access: open
    num_parameters: 40000000000
    release_date: 2023-05-25
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]



  # Together
  - name: together/gpt-jt-6b-v1
    display_name: GPT-JT (6B)
    description: GPT-JT (6B parameters) is a fork of GPT-J ([blog post](https://www.together.xyz/blog/releasing-v1-of-gpt-jt-powered-by-open-source-ai)).
    creator_organization_name: Together
    access: open
    num_parameters: 6700000000
    release_date: 2022-11-29
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: together/gpt-neoxt-chat-base-20b
    display_name: GPT-NeoXT-Chat-Base (20B)
    description: GPT-NeoXT-Chat-Base (20B) is fine-tuned from GPT-NeoX, serving as a base model for developing open-source chatbots.
    creator_organization_name: Together
    access: open
    num_parameters: 20000000000
    release_date: 2023-03-08
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, CHATML_MODEL_TAG]

  - name: together/redpajama-incite-base-3b-v1
    display_name: RedPajama-INCITE-Base-v1 (3B)
    description: RedPajama-INCITE-Base-v1 (3B parameters) is a 3 billion base model that aims to replicate the LLaMA recipe as closely as possible.
    creator_organization_name: Together
    access: open
    num_parameters: 3000000000
    release_date: 2023-05-05
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: together/redpajama-incite-instruct-3b-v1
    display_name: RedPajama-INCITE-Instruct-v1 (3B)
    description: RedPajama-INCITE-Instruct-v1 (3B parameters) is a model fine-tuned for few-shot applications on the data of GPT-JT. It is built from RedPajama-INCITE-Base-v1 (3B), a 3 billion base model that aims to replicate the LLaMA recipe as closely as possible.
    creator_organization_name: Together
    access: open
    num_parameters: 3000000000
    release_date: 2023-05-05
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: together/redpajama-incite-chat-3b-v1 # NOT SUPPORTED
    display_name: RedPajama-INCITE-Chat-v1 (3B)
    description: RedPajama-INCITE-Chat-v1 (3B parameters) is a model fine-tuned on OASST1 and Dolly2 to enhance chatting ability. It is built from RedPajama-INCITE-Base-v1 (3B), a 3 billion base model that aims to replicate the LLaMA recipe as closely as possible.
    creator_organization_name: Together
    access: open
    num_parameters: 3000000000
    release_date: 2023-05-05
    tags: [UNSUPPORTED_MODEL_TAG]

  - name: together/redpajama-incite-base-7b
    display_name: RedPajama-INCITE-Base (7B)
    description: RedPajama-INCITE-Base (7B parameters) is a 7 billion base model that aims to replicate the LLaMA recipe as closely as possible.
    creator_organization_name: Together
    access: open
    num_parameters: 7000000000
    release_date: 2023-05-05
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: together/redpajama-incite-instruct-7b
    display_name: RedPajama-INCITE-Instruct (7B)
    description: RedPajama-INCITE-Instruct (7B parameters) is a model fine-tuned for few-shot applications on the data of GPT-JT. It is built from RedPajama-INCITE-Base (7B), a 7 billion base model that aims to replicate the LLaMA recipe as closely as possible.
    creator_organization_name: Together
    access: open
    num_parameters: 7000000000
    release_date: 2023-05-05
    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]



  # Tsinghua

  - name: thudm/cogview2
    display_name: CogView2 (6B)
    description: CogView2 is a hierarchical transformer (6B-9B-9B parameters) for text-to-image generation that supports both English and Chinese input text ([paper](https://arxiv.org/abs/2105.13290))
    creator_organization_name: Tsinghua
    access: open
    num_parameters: 6000000000
    release_date: 2022-06-15
    tags: [TEXT_TO_IMAGE_MODEL_TAG]

  - name: tsinghua/glm
    display_name: GLM (130B)
    description: GLM (130B parameters) is an open bilingual (English & Chinese) bidirectional dense model that was trained using General Language Model (GLM) procedure ([paper](https://arxiv.org/pdf/2210.02414.pdf)).
    creator_organization_name: Tsinghua
    access: open
    num_parameters: 130000000000
    release_date: 2022-08-04
    # Inference with echo=True is not feasible -- in the prompt encoding phase, they use
    # bidirectional attention and do not perform predictions on them.
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, NO_NEWLINES_TAG]

  - name: tsinghua/codegeex # NOT SUPPORTED
    display_name: CodeGeeX (13B)
    description: CodeGeeX (13B parameters) is an open dense code model trained on more than 20 programming languages on a corpus of more than 850B tokens ([blog](http://keg.cs.tsinghua.edu.cn/codegeex/)).
    creator_organization_name: Tsinghua
    access: open
    num_parameters: 13000000000
    release_date: 2022-09-19
    tags: [UNSUPPORTED_MODEL_TAG]

  # Upstage
  - name: upstage/solar-pro-preview-instruct
    display_name: Solar Pro Preview (22B)
    description: Solar Pro Preview (22B) is open-weights model for single GPU inference that is a preview of the upcoming Solar Pro model ([blog](https://www.upstage.ai/products/solar-pro-preview)).
    creator_organization_name: Upstage
    access: open
    num_parameters: 22000000000
    release_date: 2024-09-11
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: upstage/solar-pro-241126
    display_name: Solar Pro
    display_name: Solar Pro
    description: Solar Pro is a LLM designed for instruction-following and processing structured formats like HTML and Markdown. It supports English, Korean, and Japanese and has domain expertise in Finance, Healthcare, and Legal. ([blog](https://www.upstage.ai/blog/press/solar-pro-aws)).
    creator_organization_name: Upstage
    access: limited
    num_parameters: 22000000000
    release_date: 2024-11-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  # Writer
  - name: writer/palmyra-base
    display_name: Palmyra Base (5B)
    description: Palmyra Base (5B)
    creator_organization_name: Writer
    access: limited
    num_parameters: 5000000000
    release_date: 2022-10-13
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-large
    display_name: Palmyra Large (20B)
    description: Palmyra Large (20B)
    creator_organization_name: Writer
    access: limited
    num_parameters: 20000000000
    release_date: 2022-12-23
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-instruct-30
    display_name: InstructPalmyra (30B)
    description: InstructPalmyra (30B parameters) is trained using reinforcement learning techniques based on feedback from humans.
    creator_organization_name: Writer
    access: limited
    num_parameters: 30000000000
    release_date: 2023-02-16
    # Does not support echo
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-e
    display_name: Palmyra E (30B)
    description: Palmyra E (30B)
    creator_organization_name: Writer
    access: limited
    num_parameters: 30000000000
    release_date: 2023-03-03
    # Does not support echo
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/silk-road
    display_name: Silk Road (35B)
    description: Silk Road (35B)
    creator_organization_name: Writer
    access: limited
    num_parameters: 35000000000
    release_date: 2023-04-13
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-x
    display_name: Palmyra X (43B)
    description: Palmyra-X (43B parameters) is trained to adhere to instructions using human feedback and utilizes a technique called multiquery attention. Furthermore, a new feature called 'self-instruct' has been introduced, which includes the implementation of an early stopping criteria specifically designed for minimal instruction tuning ([paper](https://dev.writer.com/docs/becoming-self-instruct-introducing-early-stopping-criteria-for-minimal-instruct-tuning)).
    creator_organization_name: Writer
    access: limited
    num_parameters: 43000000000
    release_date: 2023-06-11
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-x-v2
    display_name: Palmyra X V2 (33B)
    description: Palmyra-X V2 (33B parameters) is a Transformer-based model, which is trained on extremely large-scale pre-training data. The pre-training data more than 2 trillion tokens types are diverse and cover a wide range of areas, used FlashAttention-2.
    creator_organization_name: Writer
    access: limited
    num_parameters: 33000000000
    release_date: 2023-12-01
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-x-v3
    display_name: Palmyra X V3 (72B)
    description: Palmyra-X V3 (72B parameters) is a Transformer-based model, which is trained on extremely large-scale pre-training data. It is trained via unsupervised learning and DPO and use multiquery attention.
    creator_organization_name: Writer
    access: limited
    num_parameters: 72000000000
    release_date: 2023-12-01
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-x-32k
    display_name: Palmyra X-32K (33B)
    description: Palmyra-X-32K (33B parameters) is a Transformer-based model, which is trained on large-scale pre-training data. The pre-training data types are diverse and cover a wide range of areas. These data types are used in conjunction and the alignment mechanism to extend context window.
    creator_organization_name: Writer
    access: limited
    num_parameters: 33000000000
    release_date: 2023-12-01
    # Does not support echo
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: writer/palmyra-vision-003
    display_name: Palmyra Vision 003
    description:  Palmyra Vision 003 (internal only)
    creator_organization_name: Writer
    access: limited
    num_parameters: 5000000000
    release_date: 2024-05-24
    # Does not support echo
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_VLM_TAG]

  - name: writer/palmyra-x-004
    display_name: Palmyra-X-004
    description: Palmyra-X-004 language model with a large context window of up to 128,000 tokens that excels in processing and understanding complex tasks.
    creator_organization_name: Writer
    access: limited
    release_date: 2024-09-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: writer/palmyra-x5
    display_name: Palmyra X5
    description: Palmyra X5 is a language model for enterprise that uses a Mixture of Experts (MoE) architecture and a hybrid attention mechanism that blends linear and softmax attention. ([blog](https://writer.com/engineering/long-context-palmyra-x5/))
    creator_organization_name: Writer
    access: limited
    release_date: 2024-04-28
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: writer/palmyra-med-32k
    display_name: Palmyra-Med 32K (70B)
    description: Palmyra-Med 32K (70B) is a model finetuned from Palmyra-X-003 intended for medical applications.
    creator_organization_name: Writer
    access: open
    num_parameters: 70600000000
    release_date: 2024-07-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: writer/palmyra-med
    display_name: Palmyra Med
    description: Palmyra Med is a model intended for medical applications.
    creator_organization_name: Writer
    access: open
    release_date: 2024-07-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: writer/palmyra-fin-32k
    display_name: Palmyra-Fin 32K (70B)
    description: Palmyra-Fin 32K (70B) is a model finetuned from Palmyra-X-003 intended for financial applications.
    creator_organization_name: Writer
    access: open
    num_parameters: 70600000000
    release_date: 2024-07-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: writer/palmyra-fin
    display_name: Palmyra Fin
    description: Palmyra Fin is a financial LLM built using combining a well-curated set of financial training data with custom fine-tuning instruction data([blog](https://writer.com/blog/palmyra-med-fin-models/)).
    creator_organization_name: Writer
    access: limited
    release_date: 2024-07-31
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # xAI

  - name: xai/grok-3-beta
    display_name: Grok 3 Beta
    description: Grok 3 Beta is a model trained on xAI's Colossus supercluster with significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. ([blog](https://x.ai/news/grok-3))
    creator_organization_name: xAI
    access: limited
    release_date: 2025-04-03  # https://docs.x.ai/docs/release-notes#april-2025
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: xai/grok-3-mini-beta
    display_name: Grok 3 mini Beta
    description: Grok 3 mini Beta is a model trained on xAI's Colossus supercluster with significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. ([blog](https://x.ai/news/grok-3))
    creator_organization_name: xAI
    access: limited
    release_date: 2025-04-03  # https://docs.x.ai/docs/release-notes#april-2025
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: xai/grok-4-0709
    display_name: Grok 4 (0709)
    description: Grok 4 (0709) is a model that includes native tool use and real-time search integration. ([blog](https://x.ai/news/grok-4))
    creator_organization_name: xAI
    access: limited
    release_date: 2025-07-09
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  # Yandex
  - name: yandex/yalm
    display_name: YaLM (100B)
    description: YaLM (100B parameters) is an autoregressive language model trained on English and Russian text ([GitHub](https://github.com/yandex/YaLM-100B)).
    creator_organization_name: Yandex
    access: open
    num_parameters: 100000000000
    release_date: 2022-06-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG]

  # Reka
  - name: reka/reka-core
    display_name: Reka-Core
    description: Reka-Core
    creator_organization_name: Reka AI
    access: limited
    release_date: 2024-04-18
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: reka/reka-core-20240415
    display_name: Reka-Core-20240415
    description: Reka-Core-20240415
    creator_organization_name: Reka AI
    access: limited
    release_date: 2024-04-18
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]
  
  - name: reka/reka-core-20240501
    display_name: Reka-Core-20240501
    description: Reka-Core-20240501
    creator_organization_name: Reka AI
    access: limited
    release_date: 2024-05-01
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: reka/reka-flash
    display_name: Reka-Flash (21B)
    description: Reka-Flash (21B)
    creator_organization_name: Reka AI
    access: limited
    num_parameters: 21000000000
    release_date: 2024-04-18
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: reka/reka-flash-20240226
    display_name: Reka-Flash-20240226 (21B)
    description: Reka-Flash-20240226 (21B)
    creator_organization_name: Reka AI
    access: limited
    num_parameters: 21000000000
    release_date: 2024-04-18
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: reka/reka-edge
    display_name: Reka-Edge (7B)
    description: Reka-Edge (7B)
    creator_organization_name: Reka AI
    access: limited
    num_parameters: 7000000000
    release_date: 2024-04-18
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]

  - name: reka/reka-edge-20240208
    display_name: Reka-Edge-20240208 (7B)
    description: Reka-Edge-20240208 (7B)
    creator_organization_name: Reka AI
    access: limited
    num_parameters: 7000000000
    release_date: 2024-04-18
    tags: [VISION_LANGUAGE_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG]
  
# Diva Llama
  - name: stanford/diva-llama
    display_name: Diva Llama 3 (8B)
    description: Diva Llama 3 is an end-to-end Voice Assistant Model which can handle speech and text as inputs. It was trained using distillation loss. ([paper](https://arxiv.org/abs/2410.02678))
    creator_organization_name: Stanford
    access: open
    num_parameters: 8000000000
    release_date: 2024-10-03
    tags: [AUDIO_LANGUAGE_MODEL_TAG]


# LLaMA-Omni
  - name: ictnlp/llama-3.1-8b-omni
    display_name: LLaMA-Omni (8B)
    description: The audio-visual multimodal version of the LLaMA 3.1 model ([paper](https://arxiv.org/abs/2409.06666)).
    creator_organization_name: ICTNLP
    access: open
    num_parameters: 8000000000
    release_date: 2024-09-10
    tags: [AUDIO_LANGUAGE_MODEL_TAG]


# Maritaca AI
  - name: maritaca-ai/sabia-7b
    display_name: Sabia 7B
    description: Sabia 7B
    creator_organization_name: MARITACA-AI
    access: open
    num_parameters: 6740000000
    release_date: 2023-11-08
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: maritaca-ai/sabiazinho-3
    display_name: Sabiazinho 3
    description: Sabiazinho-3 is a decoder-only language model designed for Portuguese text generation and understanding tasks. It supports a long context window of up to 128,000 tokens and is offered via API with scalable rate limits. The model is trained on diverse Portuguese corpora with knowledge up to july 2023.
    creator_organization_name: Maritaca AI
    access: limited
    release_date: 2025-02-06
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: maritaca-ai/sabia-3
    display_name: Sabía 3
    description: Sabiá-3 is a decoder-only language model designed for Portuguese text generation and understanding tasks. It supports a long context window of up to 128,000 tokens and is offered via API with scalable rate limits. The model is trained on diverse Portuguese corpora with knowledge up to july 2023.
    creator_organization_name: Maritaca AI
    access: limited
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: maritaca-ai/sabia-3.1-2025-05-08
    display_name: Sabía 3.1
    description: Sabiá-3.1 is a decoder-only language model designed for Portuguese text generation and understanding tasks. It supports a long context window of up to 128,000 tokens and is offered via API with scalable rate limits. The model is trained on diverse Portuguese corpora with knowledge up to August 2024.
    creator_organization_name: Maritaca AI
    access: limited
    release_date: 2025-05-08
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG] 

  # Z.ai

  - name: zai-org/glm-4.5-air-fp8
    display_name: GLM-4.5-Air-FP8
    description: GLM-4.5-Air-FP8 is a hybrid reasoning model designed to unify reasoning, coding, and agentic capabilities into a single model. It has 106 billion total parameters and 12 billion active parameters. The thinking mode is enabled by default. ([blog](https://z.ai/blog/glm-4.5))
    creator_organization_name: Z.ai
    access: open
    num_parameters: 110000000000
    release_date: 2025-07-28
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]


# Granite - IBM
# https://www.ibm.com/granite
# https://github.com/ibm-granite/granite-3.0-language-models

  - name: ibm-granite/granite-3.0-2b-base
    display_name: Granite 3.0 base (2B)
    description: Granite-3.0-2B-Base is a decoder-only language model to support a variety of text-to-text generation tasks.
    creator_organization_name: IBM
    access: open
    num_parameters: 2530000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG]

  - name: ibm-granite/granite-3.0-2b-instruct
    display_name: Granite 3.0 Instruct (2B)
    description:  Granite-3.0-2B-Instruct is a 2B parameter model finetuned from Granite-3.0-2B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets. 
    creator_organization_name: IBM
    access: open
    num_parameters: 2630000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.0-8b-instruct
    display_name: Granite 3.0 instruct (8B)
    description:  Granite-3.0-8B-Instruct is a 8B parameter model finetuned from Granite-3.0-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.
    creator_organization_name: IBM
    access: open
    num_parameters: 8170000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.0-8b-base
    display_name: Granite 3.0 base (8B)
    description: Granite-3.0-8B-Base is a decoder-only language model to support a variety of text-to-text generation tasks.
    creator_organization_name: IBM 
    access: open
    num_parameters: 8170000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG]

  - name: ibm-granite/granite-3.0-3b-a800m-instruct
    display_name: Granite 3.0 A800M instruct (3B)
    description: Granite-3.0-3B-A800M-Instruct is a 3B parameter model finetuned from Granite-3.0-3B-A800M-Base-4K using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.
    creator_organization_name: IBM
    access: open
    num_parameters: 3370000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.0-3b-a800m-base
    display_name: Granite 3.0 A800M base (3B)
    description: Granite-3.0-3B-A800M-Base is a decoder-only language model to support a variety of text-to-text generation tasks.
    creator_organization_name: IBM
    access: open
    num_parameters: 3370000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG]

  - name: ibm-granite/granite-3.0-1b-a400m-instruct
    display_name: Granite 3.0 A400M instruct (1B)
    description: Granite-3.0-1B-A400M-Instruct is an 1B parameter model finetuned from Granite-3.0-1B-A400M-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets.
    creator_organization_name: IBM
    access: open
    num_parameters: 1330000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.0-1b-a400m-base
    display_name: Granite 3.0 A400M base (1B)
    description: Granite-3.0-1B-A400M-Base is a decoder-only language model to support a variety of text-to-text generation tasks. It is trained from scratch following a two-stage training strategy.
    creator_organization_name: IBM
    access: open
    num_parameters: 1380000000
    release: 2024-10-21
    tags: [TEXT_MODEL_TAG]
   
  - name: ibm-granite/granite-3.1-8b-base
    display_name: Granite 3.1 - 8B - Base
    description: Granite-3.1-8B-Base extends the context length of Granite-3.0-8B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.
    creator_organization_name: IBM-GRANITE
    access: open
    num_parameters: 8170000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG]

  - name: ibm-granite/granite-3.1-8b-instruct
    display_name: Granite 3.1 - 8B - Instruct
    description: Granite-3.1-8B-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.
    creator_organization_name: IBM
    access: open
    num_parameters: 8170000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.1-2b-instruct
    display_name: Granite 3.1 - 2B - Instruct
    description: Granite-3.1-2B-Instruct is a 2B parameter long-context instruct model finetuned from Granite-3.1-2B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.
    creator_organization_name: IBM
    access: open
    num_parameters: 2530000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.1-2b-base
    display_name: Granite 3.1 - 2B - Base
    description: Granite-3.1-2B-Base extends the context length of Granite-3.0-2B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.
    creator_organization_name: IBM-GRANITE
    access: open
    num_parameters: 2530000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG]

  - name: ibm-granite/granite-3.1-3b-a800m-instruct
    display_name: Granite 3.1 - 3B - A800M - Instruct
    description: Granite-3.1-3B-A800M-Instruct is a 3B parameter long-context instruct model finetuned from Granite-3.1-3B-A800M-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.
    creator_organization_name: IBM-GRANITE
    access: open
    num_parameters: 3300000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.1-3b-a800m-base
    display_name: Granite 3.1 - 3B - A800M - Base
    description: Granite-3.1-3B-A800M-Base extends the context length of Granite-3.0-3B-A800M-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.
    creator_organization_name: IBM-GRANITE
    access: open
    num_parameters: 3300000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG]

  - name: ibm-granite/granite-3.1-1b-a400m-instruct
    display_name: Granite 3.1 - 1B - A400M - Instruct
    description: Granite-3.1-1B-A400M-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-1B-A400M-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.
    creator_organization_name: IBM-GRANITE
    access: open
    num_parameters: 1330000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm-granite/granite-3.1-1b-a400m-base
    display_name: Granite 3.1 - 1B - A400M - Base
    description: Granite-3.1-1B-A400M-Base extends the context length of Granite-3.0-1B-A400M-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K.
    creator_organization_name: IBM-GRANITE
    access: open
    num_parameters: 1330000000
    release_date: 2024-12-18
    tags: [TEXT_MODEL_TAG]

  - name: ibm/granite-13b-instruct-v2
    display_name: Granite 13b instruct v2
    description: Granite Base (13B) Instruct V2.0 is a large decoder-only transformer model.The following features were used in the design of the model Decoder-only model
    creator_organization_name: IBM
    access: limited
    num_parameters: 13000000000
    release: 2023-11-30
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]

  - name: ibm/granite-20b-code-instruct-8k
    display_name: Granite 20b code instruct (8K)
    description: Granite-20B-Code-Base-8K is a decoder-only code model designed for code generative tasks (e.g., code generation, code explanation, code fixing, etc.). It is trained from scratch with a two-phase training strategy. In phase 1, our model is trained on 3 trillion tokens sourced from 116 programming languages, ensuring a comprehensive understanding of programming languages and syntax. In phase 2, our model is trained on 500 billion tokens with a carefully designed mixture of high-quality data from code and natural language domains to improve the models’ ability to reason and follow instructions.
    creator_organization_name: IBM
    access: limited
    num_parameters: 20000000000
    release: 2024-18-4
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]

  - name: ibm/granite-34b-code-instruct
    display_name: Granite 34b code instruct
    description: Granite Base (34B) Code Instruct is a 34B parameter model fine tuned from Granite-34B-Code-Base on a combination of permissively licensed instruction data to enhance instruction following capabilities including logical reasoning and problem-solving skills.
    creator_organization_name: IBM
    access: open
    num_parameters: 34000000000
    release: 2024-6-5
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]


  - name: ibm/granite-3b-code-instruct
    display_name: Granite 3b code instruct
    description: Granite-3B-Code-Instruct-128K is a 3B parameter long-context instruct model fine tuned from Granite-3B-Code-Base-128K on a combination of permissively licensed data used in training the original Granite code instruct models, in addition to synthetically generated code instruction datasets tailored for solving long context problems. By exposing the model to both short and long context data, we aim to enhance its long-context capability without sacrificing code generation performance at short input context.
    creator_organization_name: IBM
    access: open
    num_parameters: 3000000000
    release: 2024-6-18
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]

  - name: ibm/granite-8b-code-instruct
    display_name: Granite 8b code instruct
    description: Granite-8B-Code-Instruct-128K is a 8B parameter long-context instruct model fine tuned from Granite-8B-Code-Base-128K on a combination of permissively licensed data used in training the original Granite code instruct models, in addition to synthetically generated code instruction datasets tailored for solving long context problems. By exposing the model to both short and long context data, we aim to enhance its long-context capability without sacrificing code generation performance at short input context.
    creator_organization_name: IBM
    access: open
    num_parameters: 8000000000
    release: 2024-6-18
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]

  - name: ibm/granite-3.1-8b-instruct
    display_name: Granite 3.1 - 8B - Instruct
    description: Granite-3.1-8B-Instruct is a 8B parameter long-context instruct model finetuned from Granite-3.1-8B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.
    creator_organization_name: IBM
    access: open
    num_parameters: 8170000000
    release_date: 2024-12-18
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]

  - name: ibm/granite-3.1-2b-instruct
    display_name: Granite 3.1 - 2B - Instruct
    description: Granite-3.1-2B-Instruct is a 2B parameter long-context instruct model finetuned from Granite-3.1-2B-Base using a combination of open source instruction datasets with permissive license and internally collected synthetic datasets tailored for solving long context problems.
    creator_organization_name: IBM
    access: open
    num_parameters: 2530000000
    release_date: 2024-12-18
    tags: [ TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG ]

  - name: ibm/granite-3.3-8b-instruct
    display_name: IBM Granite 3.3 8B Instruct
    description: IBM Granite 3.3 8B Instruct is an 8-billion parameter 128K context length language model fine-tuned for improved reasoning and instruction-following capabilities. ([model card](https://huggingface.co/ibm-granite/granite-3.3-8b-instruct))
    creator_organization_name: IBM
    access: open
    num_parameters: 8170000000
    release_date: 2025-04-16
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ibm/granite-3.3-8b-instruct-with-guardian
    display_name: IBM Granite 3.3 8B Instruct (with guardian)
    description: IBM Granite 3.3 8B Instruct is an 8-billion parameter 128K context length language model fine-tuned for improved reasoning and instruction-following capabilities. All prompts were first evaluated for risk by [IBM Granite Guardian 3.2 5B](https://www.ibm.com/granite/docs/models/guardian/) and prompts that were deemed risky (with a risk threshold of 0.8) received the response "I'm very sorry, but I can't assist with that.". ([model card](https://huggingface.co/ibm-granite/granite-3.3-8b-instruct))
    creator_organization_name: IBM
    access: open
    num_parameters: 8170000000
    release_date: 2025-04-16
    # Unfortunately this setup is not easily reproducible, so we mark it with DEPRECATED_MODEL_TAG
    tags: [DEPRECATED_MODEL_TAG, TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, ABLATION_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/ura-llama-2.1-8b
    display_name: URA-Llama 2.1 (8B)
    description: URA-Llama 2.1 (8B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 8000000000
    release_date: 2024-08-04
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/ura-llama-2-8b
    display_name: URA-Llama 2 (8B)
    description: URA-Llama 2 (8B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 8000000000
    release_date: 2024-08-04
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/ura-llama-7b
    display_name: URA-Llama 7B (7B)
    description: URA-Llama 7B (7B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 7000000000
    release_date: 2023-10-10
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/ura-llama-13b
    display_name: URA-Llama 13B (13B)
    description: URA-Llama 13B (13B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 13000000000
    release_date: 2023-10-10
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/ura-llama-70b
    display_name: URA-Llama 70B (70B)
    description: URA-Llama 70B (70B) is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 70000000000
    release_date: 2023-10-10
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/GemSUra-7B
    display_name: GemSUra 7B
    description: GemSUra 7B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 7000000000
    release_date: 2024-03-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/GemSUra-2B
    display_name: GemSUra 2B
    description: GemSUra 2B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: URA
    access: open
    num_parameters: 2000000000
    release_date: 2024-03-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: ura-hcmut/MixSUra
    display_name: MixSUra
    description: MixSUra is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text. It is a mixture of experts model with 8 active experts.
    creator_organization_name: URA
    access: open
    num_parameters: 46700000000
    release_date: 2024-03-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/vinallama-7b-chat
    display_name: VinaLLaMa
    description: VinaLLaMa is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 7000000000
    release_date: 2024-03-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/vinallama-2.7b-chat
    display_name: VinaLLaMa 2.7B
    description: VinaLLaMa 2.7B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 2700000000
    release_date: 2024-03-12
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/vietcuna-7b-v3
    display_name: VietCuna 7B (v3)
    description: VietCuna 7B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 7000000000
    release_date: 2023-08-07
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/vietcuna-3b-v2
    display_name: VietCuna 3B (v2)
    description: VietCuna 3B is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 3000000000
    release_date: 2023-07-15
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/Quyen-v0.1
    display_name: Quyen (v0.1)
    description: Quyen is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 4000000000
    release_date: 2024-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/Quyen-Plus-v0.1
    display_name: Quyen Plus (v0.1)
    description: Quyen Plus is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 7000000000
    release_date: 2024-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/Quyen-Pro-v0.1
    display_name: Quyen Pro (v0.1)
    description: Quyen Pro is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 14000000000
    release_date: 2024-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/Quyen-Pro-Max-v0.1
    display_name: Quyen Pro Max (v0.1)
    description: Quyen Pro Max is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 72000000000
    release_date: 2024-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/Quyen-Mini-v0.1
    display_name: Quyen Mini (v0.1)
    description: Quyen Mini is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 1800000000
    release_date: 2024-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vilm/Quyen-SE-v0.1
    display_name: Quyen SE (v0.1)
    description: Quyen SE is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: ViLM
    access: open
    num_parameters: 500000000
    release_date: 2024-02-26
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: Viet-Mistral/Vistral-7B-Chat
    display_name: Vistral 7B Chat
    description: Vistral 7B Chat is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: Viet-Mistral
    access: open
    num_parameters: 7000000000
    release_date: 2024-02-28
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vinai/PhoGPT-7B5-Instruct
    display_name: PhoGPT 7B5 Instruct
    description: PhoGPT 7B5 Instruct is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: VinAI
    access: open
    num_parameters: 7500000000
    release_date: 2024-02-19
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: vinai/PhoGPT-4B-Chat
    display_name: PhoGPT 4B Chat
    description: PhoGPT 4B Chat is a model trained on a large corpus of Vietnamese text data, including books, articles, and websites. It is designed to understand and generate Vietnamese text.
    creator_organization_name: VinAI
    access: open
    num_parameters: 4000000000
    release_date: 2024-04-02
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: CEIA-UFG/Gemma-3-Gaia-PT-BR-4b-it
    display_name: Gemma-3 Gaia PT-BR 4b Instruct
    description: Gemma-3 Gaia PT-BR 4b Instruct is a model trained by CEIA-UFG for understanding and generating Brazilian Portuguese text.
    creator_organization_name: CEIA-UFG
    access: open
    num_parameters: 4000000000
    release_date: 2025-06-01
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: recogna-nlp/bode-13b-alpaca-pt-br-no-peft
    display_name: Bode 13B Alpaca PT-BR
    description: Bode is a language model (LLM) for Portuguese, based on LLaMA 2 and fine-tuned with the Alpaca dataset translated into Portuguese. Suitable for instruction, text generation, translation and tasks in Portuguese.
    creator_organization_name: Recogna NLP
    access: open
    num_parameters: 13000000000
    release_date: 2024-01-05
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: 22h/cabrita_7b_pt_850000
    display_name: Cabrita PT-BR 7B
    description: Cabrita is an OpenLLaMA-based model, continuously trained in Portuguese (mC4-pt subset) for 850000 steps with efficient tokenization adapted to the language.
    creator_organization_name: 22h
    access: open
    num_parameters: 7000000000
    release_date: 2023-08-23
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: PORTULAN/gervasio-7b-portuguese-ptbr-decoder
    display_name: Gervásio PT-BR/PT-PT 7B Decoder
    description: Gervásio PT* is a 7B parameter decoder model, adapted from LLaMA27B, trained for both Brazilian and European Portuguese. Fine-tuned with translated data from benchmarks such as GLUE and SuperGLUE.
    creator_organization_name: PORTULAN (University of Lisbon NLX)
    access: open
    num_parameters: 6740000000
    release_date: 2024-02-29
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: TucanoBR/Tucano-2b4
    display_name: Tucano PT-BR 2b4
    description: Tucano is a series of decoder models based on LLaMA2, natively pre-trained in Portuguese using the GigaVerbo dataset (200B tokens), with the 2B model trained for 1.96M steps over 845h (515B tokens, 4 epochs).
    creator_organization_name: TucanoBR (University of Bonn)
    access: open
    num_parameters: 2444618240
    release_date: 2024-12-11
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

  - name: nicholasKluge/TeenyTinyLlama-460m
    display_name: TeenyTinyLlama 460M PT-BR
    description: TeenyTinyLlama-460m is a lightweight and efficient model based on LLaMA2, trained exclusively on Brazilian Portuguese. It uses RoPE embeddings and SwiGLU activations, with a refined SentencePiece tokenizer and a low-resource optimized architecture.
    creator_organization_name: Nicholas Kluge.
    access: open
    num_parameters: 460000000
    release_date: 2024-01-30
    tags: [TEXT_MODEL_TAG, LIMITED_FUNCTIONALITY_TEXT_MODEL_TAG, INSTRUCTION_FOLLOWING_MODEL_TAG]

