# `RunSpec`s for benchmarking performance on legal reasoning in English.

entries: [
  # LegalBench
  {description: "legalbench:model=text_code,subset=abercrombie,follow_format_instructions=instruct", priority: 2}
  {description: "legalbench:model=text_code,subset=corporate_lobbying,follow_format_instructions=instruct", priority: 2}
  {description: "legalbench:model=text_code,subset=international_citizenship_questions,follow_format_instructions=instruct", priority: 2}
  {description: "legalbench:model=text_code,subset=function_of_decision_section,follow_format_instructions=instruct", priority: 2}
  {description: "legalbench:model=text_code,subset=proa,follow_format_instructions=instruct", priority: 2}

  # LSAT QA
  {description: "lsat_qa:model=text_code,task=grouping", priority: 3}
  {description: "lsat_qa:model=text_code,task=ordering", priority: 3}
  {description: "lsat_qa:model=text_code,task=assignment", priority: 3}
  {description: "lsat_qa:model=text_code,task=miscellaneous", priority: 3}

  # LegalSupport: Excluded because part of LegalBench (casehold)
  # LexGLUE: excluded because tasks are too difficult
  # LEXTREME: excluded because tasks are multilingual
  # Summarization: Maybe add later
  #{description: "billsum_legal_summarization:model=text", priority: 3},
  #{description: "multilexsum_legal_summarization:model=text", priority: 3},
  #{description: "eurlexsum_legal_summarization:model=text", priority: 3},
]


