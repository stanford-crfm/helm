entries: [
    # MedQA: USMLE QA multiple choice questions with 4-5 choices
    {description: "med_qa:eval_split=test,model=text,max_train_instances=0", priority: 1}

    # MedMCQA: AIIMS/NEET QA multiple choice questions with 4 choices
    {description: "med_mcqa:model=text,max_train_instances=0", priority: 1}

    # PubMedQA: biomedical literature Q + Context + A yes/no/maybe + long answer questions
    {description: "pubmed_qa:eval_split=test,model=text,max_train_instances=0", priority: 1}

    # MMLU: exam questions QA multiple choice with 4 choices
    # NOTE: this is the subset used in the MultiMedQA dataset in the MedPaLM works
    {description: "mmlu:eval_split=test,model=text,subject=anatomy,max_train_instances=0", priority: 1}
    {description: "mmlu:eval_split=test,model=text,subject=clinical_knowledge,max_train_instances=0", priority: 1}
    {description: "mmlu:eval_split=test,model=text,subject=college_medicine,max_train_instances=0", priority: 1}
    {description: "mmlu:eval_split=test,model=text,subject=medical_genetics,max_train_instances=0", priority: 1}
    {description: "mmlu:eval_split=test,model=text,subject=professional_medicine,max_train_instances=0", priority: 1}
    {description: "mmlu:eval_split=test,model=text,subject=college_biology,max_train_instances=0", priority: 1}

    # LiveQA: consumer health questions with librarian-generated reference answers. QA long answer
    {description: "live_qa:model=text", priority: 1}

    # MedicationQA: consumer medicaiton questions with reference answers. QA long answer
    {description: "medication_qa:model=text", priority: 1}
]