#####################################################################################
# ChatGPT RunSpecs
#
# RunSpecs to evaluate ChatGPT (https://openai.com/blog/chatgpt) with.
# This file has P1/P2s RunSpecs from src/helm/benchmark/presentation/run_specs.conf.
# We excluded some RunSpecs because certain prompts that work for other language
# models are incompatible with ChatGPT. We can grow this list as we figure out the
# right way to prompt ChatGPT (e.g., making instructions more detailed).
# We also excluded RunSpecs that requires log probs (e.g., language modeling, separate
# strategy for multiple choice QA, etc.).
#####################################################################################
entries: [
  ##### Generic #####

  ##### Question Answering #####
  ## Reading comprehension

  # ChatGPT tends to include an explanation with the answer.
  # We would have to rely on the "prefix_exact_match" metric.
  # {description: "boolq:model=chat_run,data_augmentation=canonical", priority: 1}

  # {description: "narrative_qa:model=chat_run,data_augmentation=canonical", priority: 2}
  # {description: "quac:model=chat_run,data_augmentation=canonical", priority: 1}

  ## Reading comprehension and closedbook QA variants

  # Causes errors. Do we need to decrease the size of the context window?
  # {description: "natural_qa:model=chat_run,mode=openbook_longans,data_augmentation=canonical", priority: 1}
  # {description: "natural_qa:model=chat_run,mode=closedbook,data_augmentation=canonical", priority: 1}

  ## Closed-book QA with multiple choice
  {description: "truthful_qa:model=chat_run,task=mc_single,data_augmentation=canonical", priority: 1}

  # For MMLU, we sampled the following 10 subjects, which cover diverse topics across humanities, social sciences and STEM.
  {description: "mmlu:model=chat_run,subject=abstract_algebra,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=chat_run,subject=college_chemistry,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=chat_run,subject=computer_security,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=chat_run,subject=econometrics,data_augmentation=canonical", priority: 2}
  {description: "mmlu:model=chat_run,subject=us_foreign_policy,data_augmentation=canonical", priority: 2}

  ##### Summarization #####
  # Scenarios: XSUM, CNN/DM

  # In-context learning doesn't work. Summarizes wrong example. Try zero-shot.
  # {description: "summarization_cnndm:model=chat_run,temperature=0.3,device=cpu", priority: 1}
  # {description: "summarization_xsum_sampled:model=chat_run,temperature=0.3,device=cpu", priority: 1}


  ##### Sentiment Analysis #####
  # Scenarios: IMDB

  # ChatGPT tends to include the output prefix in the answer.
  # {description: "imdb:model=chat_run,data_augmentation=canonical", priority: 1}


  ##### (Miscellaneous) Text Classification #####
  # Scenarios: RAFT

  # Excluded some of RAFT subsets where ChatGPT tends to include the output prefix in the answer.
  {description: "raft:subset=ade_corpus_v2,model=chat_run,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=banking_77,model=chat_run,data_augmentation=canonical", priority: 2}
  # {description: "raft:subset=neurips_impact_statement_risks,model=chat_run,data_augmentation=canonical", priority: 2}
  # {description: "raft:subset=one_stop_english,model=chat_run,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=overruling,model=chat_run,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=semiconductor_org_types,model=chat_run,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=tweet_eval_hate,model=chat_run,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=twitter_complaints,model=chat_run,data_augmentation=canonical", priority: 2}
  {description: "raft:subset=systematic_review_inclusion,model=chat_run,data_augmentation=canonical", priority: 2}
  # {description: "raft:subset=tai_safety_research,model=chat_run,data_augmentation=canonical", priority: 2}
  # {description: "raft:subset=terms_of_service,model=chat_run,data_augmentation=canonical", priority: 2}


  ##### Toxicity Detection #####
  # Scenarios: CivilComments

  # ChatGPT sometimes includes an explanation with the answer or responds "It is difficult to determine".
  # We would have to rely on the "prefix_exact_match" metric.
  # {description: "civil_comments:model=chat_run,demographic=all,data_augmentation=canonical", priority: 1}
  # {description: "civil_comments:model=chat_run,demographic=male,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=female,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=LGBTQ,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=christian,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=muslim,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=other_religions,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=black,data_augmentation=canonical", priority: 2}
  # {description: "civil_comments:model=chat_run,demographic=white,data_augmentation=canonical", priority: 2}


  ##### Component Skills and Risks #####

  ##### Knowledge #####

  # ChatGPT tends to respond
  # "I'm sorry, I don't know the answer to your question. I am a large language model trained by OpenAI and my
  # knowledge is based on the text that I have been trained on, which has a fixed cutoff date. I am not able to
  # browse the internet or access any new information that may have become available since my training."
  # to many of the questions. However it still seems to do pretty well on certain subjects
  # like `author` and `plantiff`.
  # Skip for now since it's more expensive to run (num_outputs=5)
  # {description: "wikifact:model=chat_run,k=5,subject=plaintiff", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=place_of_birth", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=medical_condition_treated", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=instance_of", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=part_of", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=currency", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=position_held", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=author", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=discoverer_or_inventor", priority: 2}
  # {description: "wikifact:model=chat_run,k=5,subject=symptoms_and_signs", priority: 2}


  ##### Reasoning #####

  # Got responses like "I'm sorry, but I cannot solve the problem as you have not provided
  # enough information. You have only provided the rules and target for each part of
  # the problem, but not the actual values for X, Y, and Z. Without this information"
  # or "It looks like the final rule set is missing from the last example.
  # Can you please provide the full rule set and target for the final example?"
  {description: "synthetic_reasoning:model=chat_run,mode=pattern_match", priority: 2}
  # {description: "synthetic_reasoning:model=chat_run,mode=variable_substitution", priority: 2}
  # {description: "synthetic_reasoning:model=chat_run,mode=induction", priority: 2}

  # ChatGPT tends to responds "Nothing can be determined about X".
  {description: "synthetic_reasoning_natural:model=chat_run,difficulty=easy", priority: 2}
  # {description: "synthetic_reasoning_natural:model=chat_run,difficulty=hard", priority: 2}

  # ChatGPT tends to respond "it is not possible answer" or answer in a complete sentence.
  {description: "babi_qa:model=chat_run,task=all", priority: 2}
  # {description: "babi_qa:model=chat_run,task=3", priority: 2}
  # {description: "babi_qa:model=chat_run,task=15", priority: 2}
  # {description: "babi_qa:model=chat_run,task=19", priority: 2}

  # ChatGPT tends to include the output prefix in the answer.
  # {description: "dyck_language:model=chat_run,num_parenthesis_pairs=3", priority: 2}

  ## Real

  # ChatGPT sometimes responds that it can't do complex reasoning problems.
  {description: "math:model=chat_run,subject=number_theory,level=1,use_official_examples=True", priority: 2}
  {description: "math:model=chat_run,subject=intermediate_algebra,level=1,use_official_examples=True", priority: 2}
  # {description: "math:model=chat_run,subject=algebra,level=1,use_official_examples=True", priority: 2}
  # {description: "math:model=chat_run,subject=prealgebra,level=1,use_official_examples=True", priority: 2}
  # {description: "math:model=chat_run,subject=geometry,level=1,use_official_examples=True", priority: 2}
  # {description: "math:model=chat_run,subject=counting_and_probability,level=1,use_official_examples=True", priority: 2}
  # {description: "math:model=chat_run,subject=precalculus,level=1,use_official_examples=True", priority: 2}

  # With chain-of-thought prompting:
  {description: "math:model=chat_run,subject=number_theory,level=1,use_chain_of_thought=True", priority: 2}
  {description: "math:model=chat_run,subject=intermediate_algebra,level=1,use_chain_of_thought=True", priority: 2}
  # {description: "math:model=chat_run,subject=algebra,level=1,use_chain_of_thought=True", priority: 2}
  # {description: "math:model=chat_run,subject=prealgebra,level=1,use_chain_of_thought=True", priority: 2}
  # {description: "math:model=chat_run,subject=geometry,level=1,use_chain_of_thought=True", priority: 2}
  # {description: "math:model=chat_run,subject=counting_and_probability,level=1,use_chain_of_thought=True", priority: 2}
  # {description: "math:model=chat_run,subject=precalculus,level=1,use_chain_of_thought=True", priority: 2}

  {description: "gsm:model=chat_run", priority: 2}

  # Legal reasoning
  {description: "legal_support:model=chat_run", priority: 2}

  # ChatGPT doesn't always give a letter answer
  {description: "lsat_qa:model=chat_run,task=all", priority: 2}

  # Data processing

  # ChatGPT tends to include an explanation with the answer or just answers "It is impossible to determine".ChatGPT
  # We would have to rely on the "prefix_exact_match" metric.
  # {description: "entity_matching:model=chat_run,dataset=Beer", priority: 2}
  # {description: "entity_matching:model=chat_run,dataset=Abt_Buy", priority: 2}
  # {description: "entity_matching:model=chat_run,dataset=Dirty_iTunes_Amazon", priority: 2}

  {description: "entity_data_imputation:model=chat_run,dataset=Buy", priority: 2}
  {description: "entity_data_imputation:model=chat_run,dataset=Restaurant", priority: 2}


  ##### Harms #####

  ## Copyright

  # Randomly sampled instances from the original BooksCorpus.
  # We expect data here to be less repeated in the pretraining corpus. This approximates the average case.
  {description: "copyright:model=chat_run,datatag=n_books_1000-extractions_per_book_1-prefix_length_125", priority: 1}
  # We expect data here to be repeated more in the pretraining corpus. This approximates the worst case.
  {description: "copyright:model=chat_run,datatag=popular_books-prefix_length_125.json", priority: 1}


  ## Disinformation

  # Skip for now since it's more expensive to run (num_outputs=5)
  # {description: "disinformation:model=chat_run,capability=reiteration,topic=climate", priority: 1}
  # {description: "disinformation:model=chat_run,capability=reiteration,topic=covid", priority: 1}
  # {description: "disinformation:model=chat_run,capability=wedging", priority: 1}

  ## Bias

  {description: "bbq:model=chat_run,subject=all", priority: 2}

  ## Toxicity

  # For some prompts, ChatGPT responds "It is not appropriate to spread false or misleading
  # information about individuals, especially in a derogatory or defamation".
  # This is because ChatGPT using the Moderation API to warn or block unsafe content.
  {description: "real_toxicity_prompts:model=chat_run", priority: 2}

  {description: "bold:model=chat_run,subject=all", priority: 2}


  ##### Efficiency #####

  # We're assuming ChatGPT uses the GPT-2 tokenizer.
  # Run later when OpenAI makes ChatGPT available through API access
  # {description: "synthetic_efficiency:model=chat_run,tokenizer=default,num_prompt_tokens=default_sweep,num_output_tokens=default_sweep", priority: 1}
  # {description: "synthetic_efficiency:model=code,tokenizer=default,num_prompt_tokens=default_sweep,num_output_tokens=default_sweep", priority: 1}

  ##### Robustness #####

  ## Contrast sets (these are separate runs since we will only consider Instances that have contrast sets
  # {description: "boolq:model=chat_run,only_contrast=True,data_augmentation=contrast_sets", priority: 2, groups: ["robustness_contrast_sets"]}
  # {description: "imdb:model=chat_run,only_contrast=True,data_augmentation=contrast_sets", priority: 2, groups: ["robustness_contrast_sets"]}
]
