# Conf file for VHELM: Holistic Evaluation of Vision-Language Models (VLMs)
entries: [

    ################################################# Main experiments #################################################

    ####################################################################################################################
    # Perception: Is the output semantically correct, given the text and image inputs?
    ####################################################################################################################

    # Questions about natural images
    {description: "vqa:model=vlm", priority: 1,  groups: ["vqa_base"]}
    {description: "viz_wiz:model=vlm", priority: 1}

    # BLINK
    {description: "blink:category=Art_Style,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Counting,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Object_Localization,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Relative_Depth,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Relative_Reflectance,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Semantic_Correspondence,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Spatial_Relation,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Visual_Correspondence,model=vlm", priority: 1, groups: ["blink_perception"]}
    {description: "blink:category=Visual_Similarity,model=vlm", priority: 1, groups: ["blink_perception"]}

    # MM-STAR
    {description: "mm_star:category=coarse_perception,model=vlm", priority: 1, groups: ["mm_star_perception"]}
    {description: "mm_star:category=fine-grained_perception,model=vlm", priority: 1, groups: ["mm_star_perception"]}

    # Image captioning
    {description: "flickr30k:model=vlm,num_respondents=1", priority: 1}

    ####################################################################################################################
    # Reasoning: Does the model understand objects, counts, and spatial and temporal relations?
    #            Can the model reason about both the text (e.g., negation, word order, etc.) and image (e.g., visual
    #            understanding or detection), i.e., visio-linguistic compositional reasoning?
    ####################################################################################################################

    # Real-world visual reasoning
    {description: "gqa:model=vlm", priority: 1}

    # MathVista
    {description: "math_vista:grade=elementary_school,question_type=multi_choice,model=vlm", priority: 1}
    {description: "math_vista:grade=elementary_school,question_type=free_form,model=vlm", priority: 1}

    {description: "math_vista:grade=high_school,question_type=multi_choice,model=vlm", priority: 1}
    {description: "math_vista:grade=high_school,question_type=free_form,model=vlm", priority: 1}

    {description: "math_vista:grade=college,question_type=multi_choice,model=vlm", priority: 1}
    {description: "math_vista:grade=college,question_type=free_form,model=vlm", priority: 1}

    {description: "math_vista:grade=daily_life,question_type=multi_choice,model=vlm", priority: 1}
    {description: "math_vista:grade=daily_life,question_type=free_form,model=vlm", priority: 1}

    # Seed bench
    {description: "seed_bench:subject=visual-reasoning,model=vlm", priority: 1}
    {description: "seed_bench:subject=instance-interaction,model=vlm", priority: 1}

    # Mementos
    {description: "mementos:subject=dailylife,num_respondents=1,model=vlm", priority: 1}

    # BLINK
    {description: "blink:category=IQ_Test,model=vlm", priority: 1, groups: ["blink_reasoning"]}
    {description: "blink:category=Jigsaw,model=vlm", priority: 1, groups: ["blink_reasoning"]}
    {description: "blink:category=Multi-view_Reasoning,model=vlm", priority: 1, groups: ["blink_reasoning"]}

    # MM-STAR
    {description: "mm_star:category=instance_reasoning,model=vlm", priority: 1, groups: ["mm_star_reasoning"]}
    {description: "mm_star:category=logical_reasoning,model=vlm", priority: 1, groups: ["mm_star_reasoning"]}
    {description: "mm_star:category=math,model=vlm", priority: 1, groups: ["mm_star_reasoning"]}

    ####################################################################################################################
    # Knowledge: Does the model have knowledge about the world or specific domains?
    ####################################################################################################################

    # A-OKVQA tests for general world knowledge
    {description: "a_okvqa:model=vlm", priority: 1, groups: ["a_okvqa_base"]}

    # RealWorldQA is a benchmark designed for real-world understanding
    {description: "real_world_qa:model=vlm", priority: 1}

    # MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI
    {description: "mmmu:subject=Accounting,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Agriculture,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Architecture_and_Engineering,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Art,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Art_Theory,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Basic_Medical_Science,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Biology,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Chemistry,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Clinical_Medicine,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Computer_Science,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Design,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Diagnostics_and_Laboratory_Medicine,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Economics,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Electronics,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Energy_and_Power,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Finance,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Geography,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=History,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Literature,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Manage,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Marketing,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Materials,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Math,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Mechanical_Engineering,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Music,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Pharmacy,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Physics,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Psychology,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Public_Health,question_type=multiple-choice,model=vlm", priority: 1}
    {description: "mmmu:subject=Sociology,question_type=multiple-choice,model=vlm", priority: 1}

    # MME (fine-grained tasks)
    {description: "mme:subject=posters,model=vlm", priority: 1}
    {description: "mme:subject=celebrity,model=vlm", priority: 1}
    {description: "mme:subject=artwork,model=vlm", priority: 1}
    {description: "mme:subject=landmark,model=vlm", priority: 1}

    # Vibe-Eval
    {description: "vibe_eval:subject=difficulty-normal,model=vlm,num_respondents=1", priority: 1}
    {description: "vibe_eval:subject=difficulty-hard,model=vlm,num_respondents=1", priority: 1}

    # BLINK
    {description: "blink:category=Functional_Correspondence,model=vlm", priority: 1, groups: ["blink_knowledge"]}
    {description: "blink:category=Forensic_Detection,model=vlm", priority: 1, groups: ["blink_knowledge"]}

    # MM-STAR
    {description: "mm_star:category=science_technology,model=vlm", priority: 1, groups: ["mm_star_knowledge"]}

    ####################################################################################################################
    # Bias: Are the generations biased in demographic representation (e.g., gender, skin tone)?
    ####################################################################################################################

    {description: "pairs:model=vlm,subset=occupations,person=black_man", priority: 1}
    {description: "pairs:model=vlm,subset=occupations,person=black_woman", priority: 1}
    {description: "pairs:model=vlm,subset=occupations,person=white_man", priority: 1}
    {description: "pairs:model=vlm,subset=occupations,person=white_woman", priority: 1}

    {description: "pairs:model=vlm,subset=potential_crime,person=black_man", priority: 1}
    {description: "pairs:model=vlm,subset=potential_crime,person=black_woman", priority: 1}
    {description: "pairs:model=vlm,subset=potential_crime,person=white_man", priority: 1}
    {description: "pairs:model=vlm,subset=potential_crime,person=white_woman", priority: 1}

    {description: "pairs:model=vlm,subset=status,person=black_man", priority: 1}
    {description: "pairs:model=vlm,subset=status,person=black_woman", priority: 1}
    {description: "pairs:model=vlm,subset=status,person=white_man", priority: 1}
    {description: "pairs:model=vlm,subset=status,person=white_woman", priority: 1}

    ####################################################################################################################
    # Fairness: Does the model exhibit performance disparities across social groups (e.g., gender, dialect)?
    ####################################################################################################################

    {description: "vqa:model=vlm,data_augmentation=dialect_deterministic", priority: 1, groups: ["vqa_dialect"]}
    {description: "a_okvqa:model=vlm,data_augmentation=dialect_deterministic", priority: 1, groups: ["a_okvqa_dialect"]}

    # Crossmodal-3600 dataset also can measure geographic bias and robustness.
    # Geographic bias refers to the tendency to favor or prioritize information, perspectives, resources,
    # or experiences from certain geographic locations over others
    {description: "crossmodal_3600:model=vlm,location=english,language=english,num_respondents=1", priority: 1}
    {description: "crossmodal_3600:model=vlm,location=spanish,language=english,num_respondents=1", priority: 1}
    {description: "crossmodal_3600:model=vlm,location=chinese,language=english,num_respondents=1", priority: 1}
    {description: "crossmodal_3600:model=vlm,location=hindi,language=english,num_respondents=1", priority: 1}

    {description: "crossmodal_3600:model=vlm,location=cusco_quechua,language=english,num_respondents=1", priority: 1}
    {description: "crossmodal_3600:model=vlm,location=maori,language=english,num_respondents=1", priority: 1}
    {description: "crossmodal_3600:model=vlm,location=swahili,language=english,num_respondents=1", priority: 1}
    {description: "crossmodal_3600:model=vlm,location=telugu,language=english,num_respondents=1", priority: 1}

    # FairFace
    # There are three supported attributes: race, gender and age
    # Race
    {description: "fair_face:model=vlm,attribute=race,subgroup=East_Asian", priority: 1}
    {description: "fair_face:model=vlm,attribute=race,subgroup=Indian", priority: 1}
    {description: "fair_face:model=vlm,attribute=race,subgroup=Black", priority: 1}
    {description: "fair_face:model=vlm,attribute=race,subgroup=White", priority: 1}
    {description: "fair_face:model=vlm,attribute=race,subgroup=Middle_Eastern", priority: 1}
    {description: "fair_face:model=vlm,attribute=race,subgroup=Latino_Hispanic", priority: 1}
    {description: "fair_face:model=vlm,attribute=race,subgroup=Southeast_Asian", priority: 1}

    # Gender
    {description: "fair_face:model=vlm,attribute=gender,subgroup=Male", priority: 1}
    {description: "fair_face:model=vlm,attribute=gender,subgroup=Female", priority: 1}

    # Age
    {description: "fair_face:model=vlm,attribute=age,subgroup=0-2_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=3-9_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=10-19_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=20-29_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=30-39_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=40-49_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=50-59_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=60-69_years", priority: 1}
    {description: "fair_face:model=vlm,attribute=age,subgroup=Over_70_years", priority: 1}

    ####################################################################################################################
    # Toxicity: Does the model generate toxic or inappropriate content? Can the model identify toxic
    #           or inappropriate content?
    ####################################################################################################################

    # Identifying toxic memes
    {description: "hateful_memes:model=vlm", priority: 1}

    # Does the model generate toxic content given toxicity-inducing images + captions
    {description: "mm_safety_bench:subset=illegal_activity,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=hate_speech,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=malware_generation,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=physical_harm,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=economic_harm,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=fraud,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=sex,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=political_lobbying,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=privacy_violence,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=legal_opinion,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=financial_advice,model=vlm", priority: 1}
    {description: "mm_safety_bench:subset=health_consultation,model=vlm", priority: 1}
    # Has some examples related to bias
    {description: "mm_safety_bench:subset=government_decision,model=vlm", priority: 1}

    ####################################################################################################################
    # Robustness: Is the model robust to invariant input (text/image) perturbations?
    ####################################################################################################################

    {description: "unicorn:subject=OODCV-VQA,model=vlm", priority: 1}
    {description: "unicorn:subject=Sketchy-VQA,model=vlm", priority: 1}

    {description: "bingo:subject=Region,model=vlm,num_respondents=1", priority: 1}
    {description: "bingo:subject=OCR,model=vlm,num_respondents=1", priority: 1}

    {description: "bingo:subject=Factual,model=vlm,num_respondents=1", priority: 1}
    {description: "bingo:subject=T2I,model=vlm,num_respondents=1", priority: 1}
    {description: "bingo:subject=I2I,model=vlm,num_respondents=1", priority: 1}

    {description: "pope:model=vlm", priority: 1}

    ####################################################################################################################
    # Multilinguality: Are languages other than English supported?
    ####################################################################################################################

    # Exams-V
    {description: "exams_v:language=Chinese,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_chinese"]}
    {description: "exams_v:language=Chinese,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_chinese"]}
    {description: "exams_v:language=Chinese,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_chinese"]}

    {description: "exams_v:language=Croation,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_croation"]}
    {description: "exams_v:language=Croation,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_croation"]}
    {description: "exams_v:language=Croation,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_croation"]}

    {description: "exams_v:language=Italian,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_italian"]}
    {description: "exams_v:language=Italian,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_italian"]}
    {description: "exams_v:language=Italian,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_italian"]}

    {description: "exams_v:language=Hungarian,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_hungarian"]}
    {description: "exams_v:language=Hungarian,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_hungarian"]}
    {description: "exams_v:language=Hungarian,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_hungarian"]}

    {description: "exams_v:language=Arabic,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_arabic"]}
    {description: "exams_v:language=Arabic,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_arabic"]}
    {description: "exams_v:language=Arabic,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_arabic"]}

    {description: "exams_v:language=Serbian,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_serbian"]}
    {description: "exams_v:language=Serbian,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_serbian"]}
    {description: "exams_v:language=Serbian,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_serbian"]}

    {description: "exams_v:language=Bulgarian,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_bulgarian"]}
    {description: "exams_v:language=Bulgarian,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_bulgarian"]}
    {description: "exams_v:language=Bulgarian,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_bulgarian"]}

    {description: "exams_v:language=English,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_english"]}
    {description: "exams_v:language=English,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_english"]}
    {description: "exams_v:language=English,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_english"]}

    {description: "exams_v:language=German,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_german"]}
    {description: "exams_v:language=German,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_german"]}
    {description: "exams_v:language=German,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_german"]}

    {description: "exams_v:language=French,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_french"]}
    {description: "exams_v:language=French,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_french"]}
    {description: "exams_v:language=French,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_french"]}

    {description: "exams_v:language=Spanish,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_spanish"]}
    {description: "exams_v:language=Spanish,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_spanish"]}
    {description: "exams_v:language=Spanish,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_spanish"]}

    {description: "exams_v:language=Polish,subject_grouped=Natural_Science,model=vlm", priority: 1, groups: ["exams_v_polish"]}
    {description: "exams_v:language=Polish,subject_grouped=Social_Sciences,model=vlm", priority: 1, groups: ["exams_v_polish"]}
    {description: "exams_v:language=Polish,subject_grouped=Other,model=vlm", priority: 1, groups: ["exams_v_polish"]}


    {description: "a_okvqa:model=vlm,data_augmentation=chinese", priority: 1, groups: ["a_okvqa_chinese"]}
    {description: "a_okvqa:model=vlm,data_augmentation=hindi", priority: 1, groups: ["a_okvqa_hindi"]}
    {description: "a_okvqa:model=vlm,data_augmentation=spanish", priority: 1, groups: ["a_okvqa_spanish"]}
    {description: "a_okvqa:model=vlm,data_augmentation=swahili", priority: 1, groups: ["a_okvqa_swahili"]}

]