---
############################################################
perturbations: []
############################################################
metrics:
  - name: num_references
    display_name: '# ref'
    description: Number of references.
  - name: num_train_trials
    display_name: '# trials'
    description: Number of trials, where in each trial we choose an independent, random set of training instances.
  - name: estimated_num_tokens_cost
    display_name: 'cost'
    description: An estimate of the number of tokens (including prompt and output completions) needed to perform the request.
  - name: num_prompt_tokens
    display_name: '# prompt tokens'
    description: Number of tokens in the prompt.
  - name: num_prompt_characters
    display_name: '# prompt chars'
    description: Number of characters in the prompt.
  - name: num_completion_tokens
    display_name: '# completion tokens'
    description: Actual number of completion tokens (over all completions).
  - name: num_output_tokens
    display_name: '# output tokens'
    description: Actual number of output tokens.
  - name: max_num_output_tokens
    display_name: 'Max output tokens'
    description: Maximum number of output tokens (overestimate since we might stop earlier due to stop sequences).
  - name: num_requests
    display_name: '# requests'
    description: Number of distinct API requests.
  - name: num_instances
    display_name: '# eval'
    description: Number of evaluation instances.
  - name: num_train_instances
    display_name: '# train'
    description: Number of training instances (e.g., in-context examples).
  - name: prompt_truncated
    display_name: truncated
    description: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).
  - name: finish_reason_length
    display_name: finish b/c length
    description: Fraction of instances where the the output was terminated because of the max tokens limit.
  - name: finish_reason_stop
    display_name: finish b/c stop
    description: Fraction of instances where the the output was terminated because of the stop sequences.
  - name: finish_reason_endoftext
    display_name: finish b/c endoftext
    description: Fraction of instances where the the output was terminated because the end of text token was generated.
  - name: finish_reason_unknown
    display_name: finish b/c unknown
    description: Fraction of instances where the the output was terminated for unknown reasons.
  - name: num_completions
    display_name: '# completions'
    description: Number of completions.
  - name: predicted_index
    display_name: Predicted index
    description: Integer index of the reference (0, 1, ...) that was predicted by the model (for multiple-choice).

  # Accuracy metrics:
  - name: exact_match
    display_name: Exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference exactly.
    lower_is_better: false
  - name: quasi_exact_match
    display_name: Quasi-exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference up to light processing.
    lower_is_better: false
  - name: quasi_leave_articles_exact_match
    display_name: Quasi-exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference up to light processing.
    lower_is_better: false
  - name: prefix_exact_match
    display_name: Prefix exact match
    short_display_name: PEM
    description: Fraction of instances that the predicted output matches the prefix of a correct reference exactly.
    lower_is_better: false
  - name: quasi_prefix_exact_match
    # TODO: should call this prefix_quasi_exact_match
    display_name: Prefix quasi-exact match
    short_display_name: PEM
    description: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.
    lower_is_better: false

  - name: rouge_1
    display_name: ROUGE-1
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 1-gram overlap.
    lower_is_better: false
  - name: rouge_2
    display_name: ROUGE-2
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.
    lower_is_better: false
  - name: rouge_l
    display_name: ROUGE-L
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on longest common subsequence overlap.
    lower_is_better: false
  - name: bleu_1
    display_name: BLEU-1
    description: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 1-gram overlap.
    lower_is_better: false
  - name: bleu_4
    display_name: BLEU-4
    description: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.
    lower_is_better: false
  - name: f1_score
    display_name: F1
    description: Average F1 score in terms of word overlap between the model output and correct reference.
    lower_is_better: false
  - name: cider
    display_name: CIDEr
    description: Evaluates the quality of generated caption by measuring the weighted similarity of n-grams between the captions and a set of human-written reference captions, emphasizing informativeness and consensus.
    lower_is_better: false
  - name: prometheus_vision
    display_name: Prometheus Vision rating
    description: Scores are from 1 to 5, where 5 is the best.
    lower_is_better: false

  # RoboRewardBench metrics
  - name: abs_error
    display_name: Absolute Error
    lower_is_better: true
    description: Absolute difference between the predicted progress reward and the ground-truth human reward.
  - name: squared_error
    display_name: Squared Error
    lower_is_better: true
    description: Squared difference (L2 loss) between the predicted progress reward and the ground-truth human reward.

############################################################
metric_groups:
  - name: accuracy
    display_name: Accuracy
    aggregation_strategies:
      - mean
    metrics:
      - name: ${main_name}
        split: ${main_split}

  - name: general_information
    display_name: General information
    hide_win_rates: true
    metrics:
    - name: num_instances
      split: ${main_split}
    - name: num_train_instances
      split: ${main_split}
    - name: prompt_truncated
      split: ${main_split}
    - name: num_prompt_tokens
      split: ${main_split}
    - name: num_output_tokens
      split: ${main_split}


############################################################
run_groups:
  - name: core_scenarios
    display_name: RoboReward
    description: "The full leaderboard for **RoboReward** ([paper](https://arxiv.org/abs/2601.00675)). **RoboRewardBench** is our *human-verified* evaluation suite: a clean test split of 2,831 real-robot rollouts spanning 14 robot embodiments, diverse scenes/tasks, and a mix of egocentric and exocentric viewpoints. Each model is evaluated as an episodic reward model: given a task instruction and the full rollout video, it outputs a discrete end-of-episode progress score in {1,2,3,4,5} under a fixed rubric (1=no success, 5=perfect completion). The main metric is **MAE (Mean Absolute Error; lower is better)** between the predicted score and the human-verified label. The overall ranking is determined by overall group-wise MAE computed by averaging MAE across the different subsets so no single subset dominates."
    category: All scenarios
    subgroups:
      - robo_arena
      - austin_sirius_dataset_converted_externally_to_rlds
      - berkeley_autolab_ur5
      - berkeley_fanuc_manipulation
      - berkeley_mvp_converted_externally_to_rlds
      - berkeley_rpt_converted_externally_to_rlds
      - bridge
      - cmu_play_fusion
      - dlr_edan_shared_control_converted_externally_to_rlds
      - droid
      - fractal20220817_data
      - iamlab_cmu_pickup_insert_converted_externally_to_rlds
      - jaco_play
      - kaist_nonprehensile_converted_externally_to_rlds
      - roboturk
      - stanford_hydra_dataset_converted_externally_to_rlds
      - taco_play
      - tokyo_u_lsmo_converted_externally_to_rlds
      - ucsd_kitchen_dataset_converted_externally_to_rlds
      - ucsd_pick_and_place_dataset_converted_externally_to_rlds
      - utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds
      - utokyo_xarm_bimanual_converted_externally_to_rlds
      - viola

  - name: austin_buds_dataset_converted_externally_to_rlds
    display_name: Austin BUDS
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: austin_sirius_dataset_converted_externally_to_rlds
    display_name: Austin Sirius
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: berkeley_autolab_ur5
    display_name: Berkeley Autolab UR5
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: berkeley_cable_routing
    display_name: Berkeley Cable Routing
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: berkeley_fanuc_manipulation
    display_name: Berkeley Fanuc Manipulation
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: berkeley_mvp_converted_externally_to_rlds
    display_name: Berkeley MVP
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: berkeley_rpt_converted_externally_to_rlds
    display_name: Berkeley RPT
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: bridge
    display_name: Berkeley Bridge
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: cmu_play_fusion
    display_name: CMU Play Fusion
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: dlr_edan_shared_control_converted_externally_to_rlds
    display_name: DLR Wheelchair Shared Control
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: dlr_sara_grid_clamp_converted_externally_to_rlds
    display_name: DLR Sara Grid Clamp
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: droid
    display_name: DROID
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: fractal20220817_data
    display_name: RT-1 Robot Action
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: iamlab_cmu_pickup_insert_converted_externally_to_rlds
    display_name: CMU Franka Pick-Insert
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: imperialcollege_sawyer_wrist_cam
    display_name: Imperial Wrist Cam
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: jaco_play
    display_name: USC Jaco Play
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: kaist_nonprehensile_converted_externally_to_rlds
    display_name: KAIST Nonprehensile Objects
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: nyu_door_opening_surprising_effectiveness
    display_name: NYU VINN
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: nyu_rot_dataset_converted_externally_to_rlds
    display_name: NYU ROT
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: robo_arena
    display_name: RoboArena
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: roboturk
    display_name: Roboturk
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: stanford_hydra_dataset_converted_externally_to_rlds
    display_name: Stanford HYDRA
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: taco_play
    display_name: Freiburg Franka Play
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: tokyo_u_lsmo_converted_externally_to_rlds
    display_name: LSMO
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: toto
    display_name: TOTO
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: ucsd_kitchen_dataset_converted_externally_to_rlds
    display_name: UCSD Kitchen
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: ucsd_pick_and_place_dataset_converted_externally_to_rlds
    display_name: UCSD Pick Place
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: utaustin_mutex
    display_name: Austin Mutex
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: utokyo_pr2_opening_fridge_converted_externally_to_rlds
    display_name: Tokyo PR2 Fridge Opening
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds
    display_name: Tokyo PR2 Tabletop Manipulation
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: utokyo_xarm_bimanual_converted_externally_to_rlds
    display_name: UTokyo xArm Bimanual
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: utokyo_xarm_pick_and_place_converted_externally_to_rlds
    display_name: UTokyo xArm PickPlace
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English

  - name: viola
    display_name: Austin VIOLA
    description: To evaluate how well VLMs can judge rollouts of robot actions in videos.
    metric_groups:
      - accuracy
    environment:
      main_name: abs_error
      main_split: test
    taxonomy:
      task: judging
      what: Real-world robot videos
      who: Human experts
      when: "2025"
      language: English
