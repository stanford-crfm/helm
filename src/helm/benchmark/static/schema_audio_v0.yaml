---
metrics:
  # Infrastructure metrics:
  - name: num_perplexity_tokens
    display_name: '# tokens'
    description: Average number of tokens in the predicted output (for language modeling, the input too).
  - name: num_bytes
    display_name: '# bytes'
    description: Average number of bytes in the predicted output (for language modeling, the input too).

  - name: num_references
    display_name: '# ref'
    description: Number of references.
  - name: num_train_trials
    display_name: '# trials'
    description: Number of trials, where in each trial we choose an independent, random set of training instances.
  - name: estimated_num_tokens_cost
    display_name: 'cost'
    description: An estimate of the number of tokens (including prompt and output completions) needed to perform the request.
  - name: num_prompt_tokens
    display_name: '# prompt tokens'
    description: Number of tokens in the prompt.
  - name: num_prompt_characters
    display_name: '# prompt chars'
    description: Number of characters in the prompt.
  - name: num_completion_tokens
    display_name: '# completion tokens'
    description: Actual number of completion tokens (over all completions).
  - name: num_output_tokens
    display_name: '# output tokens'
    description: Actual number of output tokens.
  - name: max_num_output_tokens
    display_name: 'Max output tokens'
    description: Maximum number of output tokens (overestimate since we might stop earlier due to stop sequences).
  - name: num_requests
    display_name: '# requests'
    description: Number of distinct API requests.
  - name: num_instances
    display_name: '# eval'
    description: Number of evaluation instances.
  - name: num_train_instances
    display_name: '# train'
    description: Number of training instances (e.g., in-context examples).
  - name: prompt_truncated
    display_name: truncated
    description: Fraction of instances where the prompt itself was truncated (implies that there were no in-context examples).
  - name: finish_reason_length
    display_name: finish b/c length
    description: Fraction of instances where the the output was terminated because of the max tokens limit.
  - name: finish_reason_stop
    display_name: finish b/c stop
    description: Fraction of instances where the the output was terminated because of the stop sequences.
  - name: finish_reason_endoftext
    display_name: finish b/c endoftext
    description: Fraction of instances where the the output was terminated because the end of text token was generated.
  - name: finish_reason_unknown
    display_name: finish b/c unknown
    description: Fraction of instances where the the output was terminated for unknown reasons.
  - name: num_completions
    display_name: '# completions'
    description: Number of completions.
  - name: predicted_index
    display_name: Predicted index
    description: Integer index of the reference (0, 1, ...) that was predicted by the model (for multiple-choice).

  # Accuracy metrics:
  - name: exact_match
    display_name: Exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference exactly.
    lower_is_better: false
  - name: quasi_exact_match
    display_name: Quasi-exact match
    short_display_name: EM
    description: Fraction of instances that the predicted output matches a correct reference up to light processing.
    lower_is_better: false
  - name: prefix_exact_match
    display_name: Prefix exact match
    short_display_name: PEM
    description: Fraction of instances that the predicted output matches the prefix of a correct reference exactly.
    lower_is_better: false
  - name: quasi_prefix_exact_match
    # TODO: should call this prefix_quasi_exact_match
    display_name: Prefix quasi-exact match
    short_display_name: PEM
    description: Fraction of instances that the predicted output matches the prefix of a correct reference up to light processing.
    lower_is_better: false

  - name: rouge_1
    display_name: ROUGE-1
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 1-gram overlap.
    lower_is_better: false
  - name: rouge_2
    display_name: ROUGE-2
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on 2-gram overlap.
    lower_is_better: false
  - name: rouge_l
    display_name: ROUGE-L
    description: Average ROUGE score [(Lin, 2004)](https://aclanthology.org/W04-1013/) based on longest common subsequence overlap.
    lower_is_better: false
  - name: bleu_1
    display_name: BLEU-1
    description: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 1-gram overlap.
    lower_is_better: false
  - name: bleu_4
    display_name: BLEU-4
    description: Average BLEU score [(Papineni et al., 2002)](https://aclanthology.org/P02-1040/) based on 4-gram overlap.
    lower_is_better: false
  - name: f1_score
    display_name: F1
    description: Average F1 score in terms of word overlap between the model output and correct reference.
    lower_is_better: false
  - name: cider
    display_name: CIDEr
    description: Evaluates the quality of generated caption by measuring the weighted similarity of n-grams between the captions and a set of human-written reference captions, emphasizing informativeness and consensus.
    lower_is_better: false

  # Machine Translation metrics
  - name: bleu
    display_name: BLEU
    short_display_name: BLEU
    description: BLEU score based on [Post, (2018)](https://aclanthology.org/W18-6319/).
    lower_is_better: false

  # Speech Recognition metrics
  - name: wer_score
    display_name: Word Error Rate
    short_display_name: WER
    description: Word error rate between model predictions and ground truth answers for ASR tasks.
    lower_is_better: true

  - name: mer_score
    display_name: Match Error Rate
    short_display_name: MER
    description: Word match error rate between model predictions and ground truth answers.
    lower_is_better: true

  - name: wip_score
    display_name: Word Information Preservation
    short_display_name: WIP
    description: Word information preservation (WIP) for evaluating the preserved information of ASR.
    lower_is_better: false

  - name: cer_score
    display_name: Character Error Rate
    short_display_name: CER
    description: Character error rate (CER) for evaluating the accuracy of ASR.
    lower_is_better: true

  - name: chinese_wer_score
    display_name: Chinese Word Error Rate
    short_display_name: Chinese WER
    description: Chinese word error rate between model predictions and ground truth answers for ASR tasks.
    lower_is_better: true

  - name: chinese_mer_score
    display_name: Chinese Match Error Rate
    short_display_name: Chinese MER
    description: Chinese word match error rate between model predictions and ground truth answers.
    lower_is_better: true

  - name: chinese_wip_score
    display_name: Chinese Word Information Preservation
    short_display_name: Chinese WIP
    description: Chinese word information preservation (WIP) for evaluating the preserved information of ASR.
    lower_is_better: false

  - name: chinese_cer_score
    display_name: Chinese Character Error Rate
    short_display_name: Chinese CER
    description: Chinese character error rate (CER) for evaluating the accuracy of Chiese ASR.
    lower_is_better: true

############################################################
perturbations: []

############################################################
metric_groups:
  - name: accuracy
    display_name: Accuracy
    metrics:
      - name: ${main_name}
        split: ${main_split}

  - name: efficiency
    display_name: Efficiency
    metrics:
    - name: inference_runtime
      split: ${main_split}

  - name: general_information
    display_name: General information
    hide_win_rates: true
    metrics:
    - name: num_instances
      split: ${main_split}
    - name: num_train_instances
      split: ${main_split}
    - name: prompt_truncated
      split: ${main_split}
    - name: num_prompt_tokens
      split: ${main_split}
    - name: num_output_tokens
      split: ${main_split}

############################################################

run_groups:
  - name: audio_scenarios
    display_name: Audio Scenarios
    description: Audio Scenarios
    category: All scenarios
    subgroups:
      - meld_audio
      - vocal_sound
      - fleurs
      - common_voice_15

  - name: vocal_sound
    display_name: VocalSound
    description: >
      VocalSound dataset consisting of over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat 
      clearing, sneezes, and sniffs from 3,365 unique subjects. 
      
      Different from previous datasets, the VocalSound dataset contains meta information such as speaker 
      age, gender, native language, country, and health condition ([Gong et al, 2022](https://arxiv.org/abs/2205.03433)).
    metric_groups:
      - accuracy
      - general_information
    environment:
      main_name: quasi_exact_match
      main_split: test
    taxonomy:
      task: audio classification
      what: audio samples of behaviours ("cough", "laughter", "sigh", "sneeze", "sniff", "throatclearing")
      who: real speakers
      when: "2022"
      language: English

  - name: fleurs
    display_name: FLEURS
    description: >
      FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 
      benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of 
      speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), 
      Translation and Retrieval.

      The dataset contains the audio, transcriptions, and language in 102 different languages, which are divided into
      7 language groups: Western European, Eastern European, Central Asia Middle North African, Sub Saharan African, 
      South Asian, South East Asian, Chinese Japanase Korean ([Conneau et al, 2022](https://arxiv.org/abs/2205.12446)).
    metric_groups:
      - accuracy
      - general_information
    environment:
      main_name: wer_score
      main_split: test
    taxonomy:
      task: audio classification
      what: audio, transcripts, and language names in 102 languages
      who: real speakers
      when: "2022"
      language: 102 languages

  - name: common_voice_15
    display_name: Common Voice 15
    description: >
      The most recent release of CommonVoice15 (Ardila et al, 2019) includes 114 languages. Over 50,000 
      individuals have participated so far, resulting in 2,500 hours of collected audio. This is the largest 
      audio corpus in the public domain for speech recognition, both in terms of number of hours and number 
      of languages. The task is to recognize the speech from the audio sample.

      The dataset contains the audio, transcriptions, location, speaker's gender, age, and accent 
      ([Ardila et al, 2020](https://arxiv.org/abs/1912.06670)).
    metric_groups:
      - accuracy
      - general_information
    environment:
      main_name: wer_score
      main_split: test
    taxonomy:
      task: audio recognition
      what: audio, transcripts, location, speaker's information in 114 languages
      who: real speakers
      when: "2019"
      language: 114 languages

  - name: meld_audio
    display_name: Multimodal EmotionLines Dataset (MELD) Audio
    description: >
      Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset.
      MELD has more than 1400 dialogues and 13000 utterances from Friends TV series. Multiple speakers participated
      in the dialogues. Each utterance in a dialogue has been labeled by any of these seven emotions -
      Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear.
    metric_groups:
      - accuracy
      - general_information
    environment:
      main_name: quasi_exact_match
      main_split: test
    taxonomy:
      task: audio classification
      what: Classify audio by emotion
      who: Friends TV series
      when: "2018"
      language: English
